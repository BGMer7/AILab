å­¦ä¹  **tokenizationï¼ˆåˆ†è¯ï¼‰** å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ç³»ç»Ÿå±•å¼€ï¼Œæ¶µç›–å…¶ **åŸç†ã€ç®—æ³•ã€å®ç°ã€ä½¿ç”¨ã€è°ƒä¼˜** ç­‰çŸ¥è¯†ç»´åº¦ã€‚ä»¥ä¸‹ä¸ºè¯¦ç»†åˆ†ç±»ï¼š

------

## Concepts

### Token

- ä»€ä¹ˆæ˜¯ tokenï¼Ÿï¼ˆå­è¯ / å•è¯ / å­—ç¬¦ï¼‰
- ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨å­—ç¬¦æˆ–å•è¯è®­ç»ƒæ¨¡å‹ï¼Ÿ
- token å’Œ embedding çš„å…³ç³»

#### ä»€ä¹ˆæ˜¯ tokenï¼Ÿ

**Token** æ˜¯å¤§è¯­è¨€æ¨¡å‹ä¸­ç”¨äºè¡¨ç¤ºæ–‡æœ¬çš„æœ€å°å•ä½ä¹‹ä¸€ã€‚å®ƒä¸æ˜¯å›ºå®šçš„â€œä¸€ä¸ªå­—â€æˆ–â€œä¸€ä¸ªè¯â€ï¼Œè€Œæ˜¯**æ¨¡å‹åˆ†è¯å™¨ï¼ˆtokenizerï¼‰åˆ‡åˆ†æ–‡æœ¬åå¾—åˆ°çš„åŸºæœ¬ç‰‡æ®µ**ã€‚

> ç®€å•æ¥è¯´ï¼š**token æ˜¯æ¨¡å‹ç†è§£æ–‡æœ¬çš„â€œæ‹¼å›¾å—â€**ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªå­—æ¯ã€ä¸€ä¸ªæ±‰å­—ã€ä¸€ä¸ªè¯ï¼Œæˆ–è€…ä¸€ä¸ªè¯çš„ä¸€éƒ¨åˆ†ï¼ˆå­è¯ï¼‰ã€‚

------

**ç¤ºä¾‹è¯´æ˜**

| åŸå§‹æ–‡æœ¬           | Tokenizer ç±»å‹     | Tokensï¼ˆç¤ºä¾‹ï¼‰                           |
| ------------------ | ------------------ | ---------------------------------------- |
| `ChatGPT is cool.` | è‹±æ–‡ BPE           | ["Chat", "G", "PT", " is", " cool", "."] |
| `ä»Šå¤©å¤©æ°”ä¸é”™`     | ä¸­æ–‡ SentencePiece | ["ä»Šå¤©", "å¤©æ°”", "ä¸é”™"]                 |
| `unbelievable`     | BPE                | ["un", "believ", "able"]                 |

è¿™äº›åˆ‡åˆ†çš„å•ä½å°±æ˜¯ tokensã€‚ä¸åŒçš„ tokenizer åˆ‡åˆ†ç»“æœä¸åŒã€‚

**Token ä¸æ˜¯â€œè¯â€æˆ–â€œå­—ç¬¦â€**

| å•ä½ç±»å‹     | ç‰¹ç‚¹                             |
| ------------ | -------------------------------- |
| å­—ç¬¦ï¼ˆcharï¼‰ | "C"ã€"h"ã€"a" ç­‰å•ä¸ªå­—ç¬¦         |
| è¯ï¼ˆwordï¼‰   | "ChatGPT"ã€"cool" ç­‰å®Œæ•´å•è¯     |
| âœ… **Token**  | çµæ´»çš„å•ä½ï¼Œå¯ä»¥æ˜¯å­è¯ã€å­—ç¬¦æˆ–è¯ |

æ¯ä¸ª token å¯¹åº”ï¼š

- ä¸€ä¸ªå­—ç¬¦ä¸²ç‰‡æ®µï¼ˆå¦‚ "Chat"ï¼‰
- ä¸€ä¸ªæ•´æ•° idï¼ˆå¦‚ 15496ï¼‰
- ä¸€ä¸ªå‘é‡ï¼ˆé€šè¿‡ embedding æ˜ å°„å‡ºæ¥ï¼‰

------

**ä¸ºä»€ä¹ˆä½¿ç”¨ token è€Œä¸æ˜¯å­—ç¬¦æˆ–è¯ï¼Ÿ**

- **æ¯”è¯æ›´æ³›åŒ–**ï¼šèƒ½æ‹†è¯å¤„ç†æ–°è¯ï¼ˆæ¯”å¦‚ â€œre-inventedâ€ï¼‰
- **æ¯”å­—ç¬¦æ›´é«˜æ•ˆ**ï¼šå¥å­ä¸ä¼šå¤ªé•¿ï¼Œè®­ç»ƒæ”¶æ•›æ›´å¿«
- **é€šç”¨æ€§å¼º**ï¼šé€‚ç”¨äºä¸­è‹±æ··åˆã€å¤šè¯­è¨€ã€æ‹¼å†™å˜åŒ–

**ä¸­æ–‡ä¸­çš„ token æ˜¯ä»€ä¹ˆï¼Ÿ**

ä¸­æ–‡æ²¡æœ‰ç©ºæ ¼ï¼Œtoken é€šå¸¸æŒ‰ **å­—** æˆ– **è¯è¯­** åˆ‡ï¼š

- å­—ç²’åº¦ tokenï¼š["ä»Š", "å¤©", "å¤©", "æ°”", "ä¸", "é”™"]
- è¯ç²’åº¦ tokenï¼ˆå¦‚ä½¿ç”¨ SentencePieceï¼‰ï¼š["ä»Šå¤©", "å¤©æ°”", "ä¸é”™"]

æ€»ç»“ä¸€å¥è¯ï¼š

> **Token æ˜¯æ¨¡å‹ç†è§£æ–‡æœ¬çš„æœ€å°è¾“å…¥å•ä½ï¼Œæ—¢ä¸æ˜¯å­—ç¬¦ï¼Œä¹Ÿä¸æ˜¯è¯ï¼Œè€Œæ˜¯ tokenizer è®¾è®¡åå¾—åˆ°çš„â€œå­è¯å•ä½â€**

#### ä¸ºä½•ä¸èƒ½ç›´æ¥ç”¨å­—ç¬¦çº§æˆ–å•è¯çº§è®­ç»ƒï¼Ÿ

##### å­—ç¬¦çº§æ¨¡å‹ï¼ˆCharacter-levelï¼‰çš„é—®é¢˜

| é—®é¢˜             | è¯´æ˜                                                         |
| ---------------- | ------------------------------------------------------------ |
| è¾“å…¥å¤ªé•¿         | ä¸€ä¸ªå¥å­å¯èƒ½åŒ…å«å‡ åä¸Šç™¾ä¸ªå­—ç¬¦ï¼Œå¯¼è‡´ token åºåˆ—å¾ˆé•¿ï¼ŒTransformer è®¡ç®—å¤æ‚åº¦æ˜¯ O(nÂ²)ï¼Œå¼€é”€å·¨å¤§ |
| è¯­è¨€ç»“æ„éš¾æ•æ‰   | å¾ˆéš¾ä»å­—ç¬¦ä¸­å­¦ä¹ è¯­ä¹‰ã€è¯æ€§ç­‰è¯­è¨€è§„å¾‹ï¼ˆæ¯”å¦‚ "information" å’Œ "data" å…±äº«çš„æ„ä¹‰æ¨¡å‹å¾ˆå¼±ï¼‰ |
| æ”¶æ•›æ…¢           | å› ä¸ºæ¨¡å‹ä»ä½çº§å•ä½å­¦ä¹ è¯­è¨€ï¼Œå­¦ä¹ è·¯å¾„å¾ˆé•¿ï¼Œè®­ç»ƒæ”¶æ•›ææ…¢       |
| ä¸åˆ©äºé¢„è®­ç»ƒè¿ç§» | å­—ç¬¦å±‚å¤ªä½ï¼Œéš¾ä»¥æ•æ‰é€šç”¨çŸ¥è¯†æŠ½è±¡èƒ½åŠ›                         |

âœ… ä¼˜ç‚¹æ˜¯ï¼š**å­—ç¬¦é›†å¾ˆå°ï¼Œé€‚åº”æ€§å¼ºï¼Œå¯å¤„ç†æ‹¼å†™é”™è¯¯ã€æ–°è¯**
 ä½†ç¼ºç‚¹æ›´è‡´å‘½ï¼š**å¤ªæ…¢ï¼Œè¡¨è¾¾èƒ½åŠ›å·®**

------

##### å•è¯çº§æ¨¡å‹ï¼ˆWord-levelï¼‰çš„é—®é¢˜

| é—®é¢˜                         | è¯´æ˜                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| è¯è¡¨å¤ªå¤§                     | è‹±æ–‡å¸¸è§è¯æœ‰ä¸Šç™¾ä¸‡ï¼Œä¸­æ–‡æ›´ç”šï¼Œå¯¼è‡´ embedding å±‚åºå¤§ï¼Œå å†…å­˜ä¸”éš¾è®­ç»ƒ |
| OOVï¼ˆOut-Of-Vocabularyï¼‰é—®é¢˜ | è®­ç»ƒæ—¶æ²¡è§è¿‡çš„è¯ï¼ˆå¦‚â€œChatGPTâ€ï¼‰ï¼Œæ— æ³•ç¼–ç ï¼Œæ¨¡å‹å¤±æ•ˆ          |
| ä¸æ”¯æŒæ–°è¯ç»„åˆ               | æ— æ³•æ³›åŒ–ç»„åˆè¯ï¼Œå¦‚ "re-unzip-able" è¿™ç§ç»„åˆå¼æ„è¯            |
| å¤šè¯­è¨€æ¨¡å‹éš¾ç»Ÿä¸€             | ä¸­æ–‡â€œæ‰“å·¥äººâ€ã€è‹±æ–‡â€œhustlerâ€ã€æ—¥æ–‡â€œã‚µãƒ©ãƒªãƒ¼ãƒãƒ³â€è¯è¡¨åˆ†è£‚ï¼Œç®¡ç†å›°éš¾ |

âœ… ä¼˜ç‚¹æ˜¯ï¼š**è¯­ä¹‰å•ä½ç›´æ¥æ˜ç¡®**
 ä½†ç¼ºç‚¹æ˜¯ï¼š**ä¸å…·å¤‡æ³›åŒ–èƒ½åŠ›ï¼Œè¯è¡¨ä¸ç¨³å®š**

------

##### å­è¯çº§ï¼ˆSubword-levelï¼‰æ¨¡å‹çš„ä¼˜åŠ¿ï¼ˆå½“å‰ä¸»æµï¼‰

å­è¯æ–¹æ³•ï¼ˆå¦‚ BPEã€WordPieceã€Unigramï¼‰ç»“åˆäº†å­—ç¬¦çº§å’Œè¯çº§çš„ä¼˜ç‚¹ï¼š

| ç‰¹ç‚¹          | è¯´æ˜                                                         |
| ------------- | ------------------------------------------------------------ |
| é«˜æ•ˆæ³›åŒ–      | è¯è¡¨è¾ƒå°ï¼ˆ3ä¸‡~5ä¸‡ï¼‰ï¼Œèƒ½è‡ªåŠ¨ç»„åˆæ–°è¯ï¼Œå¦‚ "unknowable" â†’ "un" + "know" + "able" |
| æ²¡æœ‰ OOV é—®é¢˜ | ä»»ä½•è¾“å…¥éƒ½èƒ½è¢«æ‹†åˆ†æˆå­è¯ï¼ˆå­—ç¬¦æˆ–å·²çŸ¥ç‰‡æ®µï¼‰                   |
| è¯­ä¹‰åˆç†      | å­è¯æ¯”å­—ç¬¦æ›´è¯­ä¹‰åŒ–ï¼Œæ¯”è¯æ›´çµæ´»                               |
| é€‚åº”å¤šè¯­è¨€    | å¯ä»¥è®­ç»ƒå¤šè¯­è¨€å…±äº«çš„è¯è¡¨ï¼ˆå¦‚ä¸­æ–‡æ‹¼éŸ³+è‹±æ–‡+æ•°å­—ï¼‰             |



| ç²’åº¦       | ä¼˜ç‚¹                     | ç¼ºç‚¹                   | ä¸»æµåº”ç”¨æ¨¡å‹                              |
| ---------- | ------------------------ | ---------------------- | ----------------------------------------- |
| å­—ç¬¦çº§     | æ— OOVï¼Œè¯è¡¨å°ï¼Œé€‚åº”å¼º    | åºåˆ—é•¿ã€è¯­ä¹‰å¼±ã€æ”¶æ•›æ…¢ | å°‘æ•°ç§‘ç ”å®éªŒ                              |
| å•è¯çº§     | è¯­ä¹‰æ˜ç¡®ï¼Œå¥æ³•ç»“æ„æ¸…æ™°   | è¯è¡¨å¤§ï¼ŒOOV é—®é¢˜ä¸¥é‡   | å¤è€æ¨¡å‹ï¼ˆå¦‚ early RNNï¼‰                  |
| **å­è¯çº§** | âœ… æŠ˜ä¸­æ–¹æ¡ˆï¼Œç»“åˆä¸¤è€…ä¼˜ç‚¹ | ç¼–ç å¤æ‚åº¦ç•¥é«˜         | GPT/BERT/T5/LLaMA/Qwen ç­‰å‡ ä¹æ‰€æœ‰ç°ä»£æ¨¡å‹ |

------



#### Token å’Œ Embedding

å¾ˆå…³é”®çš„ä¸€ç‚¹æ˜¯ï¼š**token id æœ¬èº« \*ä¸åŒ…å«\* è¯­ä¹‰ä¿¡æ¯**ï¼ŒçœŸæ­£çš„è¯­ä¹‰æ˜¯é€šè¿‡ **embedding å±‚** èµ‹äºˆçš„ã€‚

------

##### token id 

token id æ˜¯ tokenizer æŠŠæ–‡æœ¬è½¬æˆçš„ä¸€ä¸²æ•´æ•°ç¼–å·ï¼Œä¾‹å¦‚ï¼š

```text
è¾“å…¥æ–‡æœ¬ï¼š "Hello world!"
Tokenizer è¾“å‡ºï¼š
  tokens: ["Hello", " world", "!"]
  ids:    [15496, 2159, 0]
```

è¿™äº› `15496`ã€`2159` ç­‰ **åªæ˜¯è¯è¡¨ä¸­çš„ç´¢å¼•**ï¼Œæœ¬è´¨æ˜¯æ•´æ•°ï¼Œä¸å«ä»»ä½•è¯­ä¹‰ã€‚ä¸åŒæ¨¡å‹å¯¹åŒä¸€ä¸ª token çš„ id ç¼–å·å¯èƒ½å®Œå…¨ä¸åŒã€‚

------

##### è¯­ä¹‰æ¥è‡ªå“ªå„¿ï¼Ÿ

**Embedding å±‚**ä¼šå°†æ¯ä¸ª token id æ˜ å°„ä¸ºä¸€ä¸ªé«˜ç»´å‘é‡ï¼Œå¦‚ï¼š

```text
token id 15496 â†’ [0.23, -0.78, ..., 0.31] ï¼ˆå‡è®¾æ˜¯ 4096 ç»´ï¼‰
```

è¿™ä¸ªå‘é‡ç»è¿‡è®­ç»ƒï¼Œæ•æ‰äº†è¯çš„ä¸Šä¸‹æ–‡å«ä¹‰ã€è¯­ä¹‰å…³ç³»ã€‚ä¾‹å¦‚ï¼š

- "dog" å’Œ "puppy" çš„ embedding ä¼šåœ¨è¯­ä¹‰ç©ºé—´ä¸­é è¿‘ï¼›
- "bank" åœ¨é‡‘èè¯­å¢ƒå’Œæ²³æµè¯­å¢ƒä¸­ä¼šæœ‰ä¸åŒå‘é‡è¡¨ç¤ºï¼ˆå¦‚æœæ¨¡å‹è¶³å¤Ÿå¤§æˆ–ä½¿ç”¨äº† contextual embeddingï¼‰ã€‚

æ‰€ä»¥ï¼š

> âœ… **token id æ˜¯â€œæ ‡å·â€ï¼Œembedding å‘é‡æ‰æ˜¯â€œè¯­ä¹‰â€**

------

##### è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³ç³»

è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤§è‡´æµç¨‹æ˜¯ï¼š

```text
æ–‡æœ¬ â†’ tokenizer â†’ token ids â†’ embedding lookup â†’ transformer â†’ é¢„æµ‹ä¸‹ä¸€ä¸ª token
```

è¿™æ„å‘³ç€ï¼š

- æ¨¡å‹å­¦çš„æ˜¯ **embedding å‘é‡çš„ç»„åˆä¸å˜åŒ–**ï¼›
- embedding å±‚æ˜¯æœ€å…ˆå­¦ä¼šâ€œè¯ä¹‰â€çš„ç»„ä»¶ï¼ˆåœ¨é¢„è®­ç»ƒä¸­ä¸æ–­è°ƒæ•´ï¼‰ï¼›
- è®­ç»ƒè¿‡ç¨‹ä¼šæŠŠè¯­ä¹‰å‹è¿› embedding ä¸­ï¼Œè€Œä¸æ˜¯ token id ä¸­ã€‚

##### Cases

| Token   | Token ID | Embedding å‘é‡ç¤ºæ„       |
| ------- | -------- | ------------------------ |
| "dog"   | 12345    | [0.15, -0.22, ..., 0.01] |
| "puppy" | 67890    | [0.14, -0.25, ..., 0.02] |
| "car"   | 54321    | [-0.33, 0.12, ..., 0.70] |

è™½ç„¶å®ƒä»¬çš„ ID å®Œå…¨ä¸åŒï¼Œä½† embedding å‘é‡å¯ä»¥å­¦åˆ° **"dog" â‰ˆ "puppy"** è€Œä¸â‰ˆ "car"ã€‚

## âœ… æ€»ç»“

| é—®é¢˜                         | å›ç­”                                            |
| ---------------------------- | ----------------------------------------------- |
| token id å«è¯­ä¹‰å—ï¼Ÿ          | âŒ ä¸å«ï¼Œå®ƒåªæ˜¯è¯è¡¨é‡Œçš„ç¼–å·                      |
| è¯­ä¹‰ä»å“ªé‡Œæ¥ï¼Ÿ               | âœ… æ¥è‡ª embedding å±‚çš„å‘é‡å­¦ä¹                    |
| embedding æ˜¯å¦‚ä½•è·å¾—è¯­ä¹‰çš„ï¼Ÿ | é€šè¿‡å¤§è§„æ¨¡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯/å¡«ç©ºä»»åŠ¡ç­‰ï¼‰ |



### Tokenization

- æ§åˆ¶è¯è¡¨å¤§å°
- æé«˜ç¨€æœ‰è¯çš„è¦†ç›–ç‡
- ä¿ç•™è¯­è¨€ç»“æ„çš„è¡¨è¾¾èƒ½åŠ›

------

## Algorithm

### Character-level

- æ¯ä¸ªå­—ç¬¦æ˜¯ä¸€ä¸ª tokenï¼ˆæç«¯ç»†ç²’åº¦ï¼‰

### Word-level

- åŸºäºç©ºæ ¼æˆ–è§„åˆ™çš„è¯çº§åˆ†è¯
- ä¸é€‚åˆä¸­æ–‡æˆ–æ–°è¯å¤„ç†ï¼Œè¯è¡¨åºå¤§

### Subword-level

- **BPEï¼ˆByte Pair Encodingï¼‰**
- **WordPiece**ï¼ˆBERT ä½¿ç”¨ï¼‰
- **Unigram Language Model**ï¼ˆSentencePiece é»˜è®¤ï¼‰
- **SentencePieceï¼ˆæ”¯æŒ Unicodeã€é€‚åˆä¸­æ–‡ï¼‰**
- **Tiktokenï¼ˆOpenAI ç”¨äº GPT æ¨¡å‹ï¼‰**

### Byte-level Tokenization

- GPT-2 å¼€å§‹ä½¿ç”¨ï¼ˆä¾‹å¦‚æ‰€æœ‰å­—ç¬¦éƒ½æ˜ å°„æˆ UTF-8 byteï¼‰

------

## Training

- è¯­æ–™å‡†å¤‡ï¼ˆå¤§è§„æ¨¡æ–‡æœ¬ã€æ¸…æ´—ï¼‰
- è®¾ç½®å‚æ•°ï¼švocab sizeã€coverageã€ç‰¹ä¾‹ tokenï¼ˆã€ ç­‰ï¼‰
- è®­ç»ƒå·¥å…·ï¼š
  - `sentencepiece`ï¼ˆGoogleï¼Œä¸»æµï¼‰
  - `tokenizers`ï¼ˆHugging Faceï¼ŒRust å®ç°ï¼Œé«˜æ€§èƒ½ï¼‰
  - OpenAI `tiktoken`ï¼ˆåªæ”¯æŒåŠ è½½ï¼Œä¸æ”¯æŒè®­ç»ƒï¼‰

### ç¤ºä¾‹å‘½ä»¤ï¼ˆSentencePieceï¼‰ï¼š

```bash
spm_train \
  --input=data.txt \
  --model_prefix=tokenizer \
  --vocab_size=32000 \
  --model_type=bpe
```

------

## Usage

### ä¸ LLM æ¨¡å‹å¯¹æ¥

- è¾“å…¥æ–‡æœ¬ â†’ token ids â†’ æ¨¡å‹ embedding å±‚
- output ids â†’ è§£ç ä¸ºæ–‡æœ¬ï¼ˆdecodeï¼‰

### å…¸å‹åº“ä½¿ç”¨

- Hugging Face `AutoTokenizer`
- `sentencepiece`
- `tokenizers`ï¼ˆæ›´å¿«ï¼‰
- `tiktoken`ï¼ˆOpenAI GPTï¼‰

------

## Evaluation & Tuning

### tokenæ•ˆç‡åˆ†æ

- å¹³å‡æ¯è¯ token æ•°ï¼ˆä¸­æ–‡æ¯”è‹±æ–‡é«˜ï¼‰
- token é•¿åº¦å¯¹æ¨ç†æ—¶é—´å½±å“å¤§

### vocabulary è®¾è®¡ç­–ç•¥

- vocab å¤§å°çš„é€‰æ‹©ï¼ˆå°â†’æ›´å¤š OOVï¼Œå¤§â†’embedding å‚æ•°å†—ä½™ï¼‰
- å¤šè¯­è¨€æ”¯æŒæ—¶éœ€åšç‰¹æ®Šå¤„ç†ï¼ˆå¦‚ä¸­è‹±æ–‡å…±ç”¨è¯è¡¨ï¼‰

------

## Case Studies

| æ¨¡å‹    | Tokenizer                 | è¯´æ˜                                  |
| ------- | ------------------------- | ------------------------------------- |
| BERT    | WordPiece                 | åŸºäº WordPiece                        |
| GPT-2/3 | Byte-level BPE (Tiktoken) | byte-level BPEï¼Œæ”¯æŒæ‰€æœ‰ Unicode å­—ç¬¦ |
| T5      | SentencePiece             | ä½¿ç”¨ Unigram                          |
| Qwen    | SentencePiece             | ä¸­æ–‡æ”¯æŒä¼˜åŒ–                          |
| LLaMA   | BPE (custom tokenizer)    | åŸºäº BPEï¼Œè‡ªå®šä¹‰ vocab                |

------

## Reference

- ğŸ“˜ [The Illustrated BPE](https://huggingface.co/learn/nlp-course/chapter6/6)
- ğŸ“— SentencePiece GitHub: https://github.com/google/sentencepiece
- ğŸ“˜ Hugging Face Tokenizers: https://github.com/huggingface/tokenizers
- ğŸ“— Tiktoken (OpenAI): https://github.com/openai/tiktoken

------

## æ€»ç»“ï¼šå­¦ä¹  Tokenization çš„å»ºè®®è·¯å¾„

1. ç†è§£ Token å’Œ embedding çš„åŸºæœ¬å…³ç³»
2. å­¦ä¹ ä¸»æµå­è¯åˆ†è¯ç®—æ³•ï¼ˆBPE / Unigramï¼‰
3. å­¦ä¼šç”¨ SentencePiece è®­ç»ƒè‡ªå·±çš„ tokenizer
4. å­¦ä¼šå¦‚ä½•å¯¹æ¥ tokenizer åˆ° LLM æ¨¡å‹ä¸­
5. åˆ†æä¸åŒæ¨¡å‹çš„ tokenizer å·®å¼‚
6. æŒæ¡ tokenizer çš„è¯„ä¼°ä¸ä¼˜åŒ–æŠ€å·§

