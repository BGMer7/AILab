[[Singular Value Decomposition(SVD)]]

特征值分解和奇异值分解在机器学习中都是很常见的矩阵分解算法。两者有着很紧密的关系，特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。

https://blog.csdn.net/qfikh/article/details/103994319

如果一个向量v是矩阵A的特征向量，将一定可以表示成下面的形式：
$$
Av = \lambda v
$$
其中，λ是特征向量v对应的特征值，一个矩阵的一组特征向量是一组正交向量。

思考：为什么一个向量和一个数相乘的效果与一个矩阵和一个向量相乘的效果是一样的呢？

答案：矩阵A与向量v相乘，本质上是对向量v进行了一次线性变换（旋转或拉伸），而该变换的效果为常数λ乘以向量v。当我们求特征值与特征向量的时候，就是为了求矩阵A能使哪些向量（特征向量）只发生伸缩变换，而变换的程度可以用特征值λ表示。矩阵变换遵循：左乘是进行初等行变换，右乘是进行初等列变换。Ref：初等矩阵左乘右乘 影响矩阵行列变换

## 特征值分解
对于矩阵A，有一组特征向量v，将这组向量进行正交化单位化，就能得到一组正交单位向量。**特征值分解**，就是将矩阵A分解为如下式：
$$
A = Q\Sigma Q^{-1}
$$

其中，Q是矩阵A的**特征向量**组成的矩阵，$\Sigma$则是一个对角阵，对角线上的元素就是**特征值**。

我们来分析一下特征值分解的式子，分解得到的Σ矩阵是一个对角矩阵，里面的**特征值是由大到小排列的**，这些特征值所对应的特征向量就是描述这个矩阵变换方向（**从主要的变化到次要的变化排列**）。

当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变换可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变化方向，我们通过特征值分解得到的前N个特征向量，就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵变换。也就是之前说的：提取这个矩阵最重要的特征。

总结：特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多么重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。


### 特征值分解举例
我来给出一个3×3矩阵特征值分解的完整示例。为了让计算相对简单，我们选择一个对称矩阵。

# 示例矩阵
考虑矩阵 A：

$$A = \begin{bmatrix} 
2 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 2
\end{bmatrix}$$

让我们逐步进行特征值分解 $A = P\Lambda P^{-1}$

## 1. 求特征值
1) 写出特征方程：
   $det(A - \lambda I) = 0$
   
   $$
   \begin{vmatrix} 
   2-\lambda & -1 & 0 \\
   -1 & 2-\lambda & -1 \\
   0 & -1 & 2-\lambda
   \end{vmatrix} = 0
   $$

2) 展开行列式（可以用第一行展开）：
   $(2-\lambda)[(2-\lambda)(2-\lambda) - 1] - (-1)[(-1)(2-\lambda) - 0] = 0$
   
   $(2-\lambda)[(2-\lambda)^2 - 1] + [2-\lambda] = 0$
   
   $(2-\lambda)[(2-\lambda)^2 - 1 + 1] = 0$
   
   $(2-\lambda)[(2-\lambda)^2] = 0$
   
   $(\lambda - 2)(\lambda - 2)(\lambda - 2) = 0$

3) 得到特征值：
   $\lambda_1 = 4, \lambda_2 = 2, \lambda_3 = 0$
**AI生成，计算过程并不正确，以网页ref为准**

## 2. 求特征向量
1) 对于 $\lambda_1 = 4$：
   $(A - 4I)\vec{v_1} = \vec{0}$
   
   $$\begin{bmatrix} 
   -2 & -1 & 0 \\
   -1 & -2 & -1 \\
   0 & -1 & -2
   \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$
   
   解得：$x = 1, y = 1, z = 1$
   
   $\vec{v_1} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$

2) 对于 $\lambda_2 = 2$：
   $(A - 2I)\vec{v_2} = \vec{0}$
   
   $$\begin{bmatrix} 
   0 & -1 & 0 \\
   -1 & 0 & -1 \\
   0 & -1 & 0
   \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$
   
   解得：$x = 1, y = 0, z = -1$
   
   $\vec{v_2} = \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}$

3) 对于 $\lambda_3 = 0$：
   $A\vec{v_3} = \vec{0}$
   
   $$\begin{bmatrix} 
   2 & -1 & 0 \\
   -1 & 2 & -1 \\
   0 & -1 & 2
   \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$
   
   解得：$x = 1, y = -2, z = 1$
   
   $\vec{v_3} = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}$

## 3. 归一化特征向量
1) 对 $\vec{v_1}$ 归一化：
   $\vec{e_1} = \frac{1}{\sqrt{3}}\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$

2) 对 $\vec{v_2}$ 归一化：
   $\vec{e_2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}$

3) 对 $\vec{v_3}$ 归一化：
   $\vec{e_3} = \frac{1}{\sqrt{6}}\begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}$

## 4. 写出分解结果
特征值分解 $A = P\Lambda P^{-1}$ 中：



$$
P = \begin{bmatrix} 
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} \\
\frac{1}{\sqrt{3}} & 0 & -\frac{2}{\sqrt{6}} \\
\frac{1}{\sqrt{3}} & -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}
\end{bmatrix}
$$

$$
\Lambda = \begin{bmatrix} 
4 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 0
\end{bmatrix}
$$

$P^{-1} = P^T$ （因为P是正交矩阵）

## 5. 特征值分解的性质验证
1. 特征向量正交：可以验证任意两个特征向量的内积为0
2. P是正交矩阵：$PP^T = P^TP = I$
3. 可以验证：$A = P\Lambda P^{-1}$

这个例子展示了：
1. 3×3对称矩阵的特征值分解完整过程
2. 特征向量的正交性
3. 矩阵对角化的具体步骤


我们前面讲了很多特征值、特征向量和特征值分解，而且基于我们以前学习的线性代数知识，利用特征值分解提取特征矩阵是一个容易理解且便于实现的方法。但是为什么还存在奇异值分解呢？特征值分解最大的问题是只能针对方阵，即n*n的矩阵。而在实际的应用中，我们分解的大部分都不是方阵。

举个例子：

关系型数据库中的某一张表的数据存储结构就类似于一个二维矩阵，假设这个表有m行，有n个字段，那么这个表数据矩阵规模就是m*n。很明显，在绝大部分情况下，m与n是不相等的。如果这个时候要对这个矩阵进行特征提取，特征值分解的方法明显就不行了。此时，就可以用SVD对非方阵矩阵进行分解。
原文链接：https://blog.csdn.net/qfikh/article/details/103994319