[[Math-Foundation/Probability-Statistics/Bellman Equation]]

# 马尔可夫奖励函数

马尔可夫奖励函数定义了在每个状态（或状态-动作对）下，系统所能获得的奖励值。奖励函数是强化学习和决策理论的核心，用于描述系统的目标。以下是不同类型的马尔可夫奖励函数：



## 基于状态的奖励函数$R(s)$

这种奖励函数仅与当前状态$s$相关，不依赖动作或时间。例如，在一个迷宫问题中，奖励可能仅表示某个状态是否是终点。

公式形式：

$$
R(s) : S \to \mathbb{R}
$$

**特点**：
- 简单直观。
- 适用于那些奖励与状态直接相关的问题。



## 基于状态-动作对的奖励函数$R(s, a)$

这种奖励函数与状态$s$和动作$a$都相关，表示在某个状态执行特定动作时的奖励值。例如，在机器人导航中，不同的动作可能消耗不同的能量，因此奖励可能依赖动作。

公式形式：

$$
R(s, a) : S \times A \to \mathbb{R}
$$

**特点**：
- 更灵活，能够区分不同动作的影响。
- 用于描述基于策略的系统行为。



## 基于状态-动作-后续状态的奖励函数$R(s, a, s')$

这种奖励函数不仅考虑当前状态和动作，还涉及转移后的下一状态$s'$。它可以用于建模动态环境中的复杂奖励关系。例如，在交通流量管理中，奖励可能依赖于进入某个状态后的交通条件。

公式形式：

$$
R(s, a, s') : S \times A \times S \to \mathbb{R}
$$

**特点**：
- 更高的描述能力，可以表达复杂的奖励结构。
- 常用于动态变化的环境。



## 时间依赖的奖励函数$R(s, t)$

这种奖励函数不仅与状态$s$有关，还依赖时间步$t$。例如，在资源管理问题中，资源的价值可能随时间变化。

公式形式：

$$
R(s, t) : S \times \mathbb{T} \to \mathbb{R}
$$

**特点**：
- 适用于时变环境。
- 可用于建模季节性或时间敏感的任务。



## 多目标奖励函数

在一些问题中，可能需要同时优化多个目标，例如时间、成本和效率。多目标奖励函数可以表示为不同子奖励的加权组合：

$$
R(s) = \sum_{i=1}^{k} w_i R_i(s)
$$

其中，$w_i$是每个目标的权重，$R_i(s)$是各个子目标的奖励函数。

**特点**：
- 用于多目标优化问题。
- 需要对权重进行合理调整。



## Discount折扣系数

关于Return的计算为什么需要 γ 折扣系数。David给出了下面几条的解释：

- 数学表达的方便，这也是最重要的
- 避免陷入无限循环
- 远期利益具有一定的不确定性
- 在金融学上，立即的回报相对于延迟的汇报能够获得更多的利益
- 符合人类更看重眼前利益的性格



## 总结

不同形式的马尔可夫奖励函数适用于不同类型的问题场景：

| 类型                                      | 特点                                       | 应用场景                 |
| ----------------------------------------- | ------------------------------------------ | ------------------------ |
| 基于状态的奖励$R(s)$                      | 简单直观                                   | 静态决策问题             |
| 基于状态-动作的奖励$R(s, a)$              | 更灵活，区分动作的影响                     | 动态策略优化             |
| 基于状态-动作-后续状态的奖励$R(s, a, s')$ | 高度描述能力，建模复杂关系                 | 动态环境建模             |
| 时间依赖奖励$R(s, t)$                     | 随时间变化，适应时变场景                   | 季节性任务、时间敏感问题 |
| 多目标奖励                                | 同时优化多个目标，需要权衡子目标之间的关系 | 复杂的多目标决策问题     |

根据具体问题的特性选择合适的奖励函数形式，是强化学习和马尔可夫决策模型设计的关键步骤。



### 奖励函数和折扣因子是否作为深度学习训练的参数？

奖励函数和折扣因子通常**不会直接作为深度学习模型的可训练参数**，但它们在强化学习中通过以下方式间接影响训练过程：

---

#### 奖励函数的作用
1. **非可训练参数**： 
   奖励函数是预先定义的，反映了问题的目标和约束条件。它直接影响智能体的行为和学习方向，但通常不是深度学习模型的可训练部分。

2. **作为输入信号**： 
   奖励是环境给出的反馈，用于计算损失函数（如优势函数或 Q 值），从而指导深度神经网络更新参数。

---

#### 折扣因子的作用
1. **固定超参数**： 
   折扣因子 $\gamma$ 通常是强化学习中的超参数，用来平衡即时奖励和长期收益。它的值由人工选择（如$\gamma$ = 0.99 \) 表示更关注长期收益）。

2. **非训练部分**： 
   折扣因子不是神经网络的一部分，它影响目标值的计算，例如：
   $$
   G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots
   $$

3. **实验调优**： 
   虽然折扣因子不是可训练的，但可以通过实验调整其值，观察对学习效果的影响。

---

#### 可训练的情况
在某些扩展方法中，奖励函数或折扣因子可以成为可训练对象：
1. **奖励函数学习**：  
   - 在逆强化学习（Inverse Reinforcement Learning, IRL）中，奖励函数由模型学习。  
   - 奖励可能通过神经网络建模，例如 \( R(s, a, s') \) 是一个可训练的函数。

2. **动态折扣因子**：  
   - 在特定任务中，折扣因子可能动态调整以适应环境变化。  
   - 例如，通过元学习优化 \( $\gamma$ \)，使其适应不同时间尺度的任务。

---

#### 总结
奖励函数和折扣因子大多数情况下是预定义的参数或超参数，而不是深度学习模型的可训练部分。它们的作用是指导训练过程，而非直接通过梯度下降进行优化。特定扩展方法（如 IRL）可能让它们成为可训练对象，但这属于特殊应用场景。



## Return 和 Discount 的介绍

在强化学习和马尔可夫决策过程中，**Return** 和 **Discount** 是衡量和计算累积奖励的重要概念，用于描述智能体在某个状态下的长期收益。



### Return（回报）

**回报**是指从某个时间步 $t$开始，智能体所能获得的所有未来奖励的总和。其定义为：

$$
G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots = \sum_{k=0}^\infty R_{t+k+1}
$$

其中：
- $G_t$：在时间步 $t$开始的回报（Return）。
- $R_{t+k+1}$：在时间步 $t+k+1$获得的即时奖励。

**特点**：
- 回报可以直接描述智能体在未来的累积收益。
- **在没有折扣的情况下，回报可能趋于无穷大，尤其是在无限时间步的任务中，比如说当一个任务成为了循环**。



### Discount（折扣因子）

**折扣因子**用于对未来奖励进行衰减，表示未来奖励的价值相对于当前奖励的权重。其值通常是一个介于 0 和 1 之间的常数，记为 $\gamma$：

- $\gamma = 0$：智能体只关注当前奖励（完全短视）。
- $\gamma = 1$：未来奖励与当前奖励同等重要（完全长视）。

引入折扣因子的回报定义为：

$$
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$
其中：
- $\gamma^k$：对第 $k$步奖励的折扣权重。
- 折扣因子可以控制未来奖励的影响范围，确保累积回报有限。



## Return 和 Discount 的作用

1. **平衡当前和未来奖励**：
   - 折扣因子可以权衡当前奖励和未来奖励的权重，避免智能体过度关注短期或长期收益。

2. **确保数学可解性**：
   - 在无限时间步的任务中，折扣因子 $\gamma < 1$能够保证累积回报 $G_t$收敛。

3. **建模现实问题**：
   - 在许多实际问题中，未来的收益通常会随着时间的推移而减小，例如金钱的时间价值。



## 回报与折扣的示例

假设某个强化学习问题中，智能体的奖励序列如下：

$$
R_1 = 10, \, R_2 = 20, \, R_3 = 30, \dots
$$

- **未折扣回报（$\gamma = 1$）**：
  $$
  G_0 = 10 + 20 + 30 + \dots = \infty
  $$

- **折扣回报（$\gamma = 0.9$）**：
  $$
  G_0 = 10 + 0.9 \cdot 20 + 0.9^2 \cdot 30 + \dots
  $$

在引入折扣因子后，累积回报将变为有限值。



## 总结

- **Return（回报）**是智能体从某个时间步开始获得的累积奖励，用于评估策略的好坏。
- **Discount（折扣因子）**用于平衡当前和未来的奖励影响，同时确保数学收敛性。
- 两者结合，能够更准确地描述强化学习中的长期收益，适用于各种场景建模。