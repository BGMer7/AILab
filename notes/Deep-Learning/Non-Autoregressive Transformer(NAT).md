[[Auto Regressive(AR)]]
[[Transformer]]
[[Autoregressive Transformer(AT)]]

非自回归转换器（Non-Autoregressive Transformer, NAT）解码器是一种特殊类型的解码器，设计用来在序列生成任务中加速输出的生成过程。

在深度学习中，**自回归 Transformer (AT)** 和 **非自回归 Transformer (NAT)** 是两种用于序列生成任务的重要模型类型。它们的主要区别在于生成过程是否逐步依赖于之前的输出。

### **非自回归 Transformer (NAT)**
非自回归 Transformer 试图克服自回归模型生成速度慢的问题，通过一次性生成整个序列来提高效率。

#### **特点**
1. **并行生成**：输出序列中的所有 token 可以同时生成。
2. **生成公式**：
   $$
   P(Y) = \prod_{t=1}^T P(y_t | X)
   $$
   输出序列中的每个 token 的生成直接依赖于输入序列 $X$，而不依赖其他输出 token。
3. **模型结构**：编码器与自回归 Transformer 相同，解码器通过特殊设计实现并行生成，通常需要额外的长度预测模块。
4. **优点**：
   - **生成速度快**：由于输出序列是并行生成的。
   - 更适合实时生成任务。
5. **缺点**：
   - **生成质量较低**：缺少上下文依赖建模，难以捕捉复杂的序列依赖关系。
   - 对齐问题：需要对输入与输出之间的对齐进行特殊建模（如使用动态规划或插值）。

#### **典型模型**
- **Non-Autoregressive Transformer (NAT)**（Gu 等，2018）
- **Masked Language Models (MLM)** 的生成形式可以视为一种 NAT。

#### **应用场景**
- 机器翻译（如大规模翻译任务）
- 快速生成任务（如语音合成、实时推荐系统）

### AT 与 NAT 的对比

|特性|自回归 Transformer (AT)|非自回归 Transformer (NAT)|
|---|---|---|
|**生成方式**|逐步生成，每一步依赖前一步|并行生成，一次生成整个序列|
|**生成速度**|较慢，适合离线任务|快速，适合实时任务|
|**生成质量**|高，适合复杂序列依赖的任务|较低，适合对序列依赖较弱的任务|
|**模型复杂性**|结构相对简单|需要额外模块处理对齐与长度预测|
|**典型问题**|错误传播、效率低|序列依赖不足、质量下降|

### **改进 NAT 的策略**

为了提高 NAT 的生成质量，研究者提出了许多增强方法：
1. **长度预测**：预测输出序列长度（Gu 等，2018）。
2. **迭代生成**：逐步细化生成的序列（如 Levenshtein Transformer）。
3. **对齐建模**：通过注意力机制或动态规划，增强输入与输出之间的对齐（如 CTC 损失）。
4. **噪声建模**：引入 Masked Language Model 风格的噪声建模来生成目标序列。



### **实际应用中的选择**

- **AT** 通常用于需要高质量生成的场景，如高精度机器翻译、自然语言生成。
- **NAT** 更适合对生成速度要求高的任务，如在线翻译、实时语音生成、语音合成等。

总结来说，AT 强调质量但牺牲速度，NAT 注重效率但可能面临质量损失。通过结合两者的优势（如半自回归模型或迭代式 NAT），可以更好地平衡生成质量与速度。