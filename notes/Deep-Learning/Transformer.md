[[Self-Attention]]


# Component

## Feed-Forward Network (FFN)

或者称为 FFNN (Feed-forward Neural Network)



在Transformer模型中，每个编码器和解码器层都包括两个主要部分：多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed Forward Neural Network，FFN）。FFN通常由两个线性变换和一个激活函数组成，细节如下：

- **输入**：来自自注意力机制的输出。
- **线性变换**：首先应用一个线性变换（全连接层），将输入映射到一个高维空间。
- **激活函数**：接着使用非线性激活函数，通常是ReLU（Rectified Linear Unit）。
- **再次线性变换**：最后，再次通过一个线性变换将数据映射回原来的维度。



FFN的本质就是一个两层的MLP。

这个MLP的简单数学表示是：
$$
FFN(x) = f(x \cdot W_1^T) \cdot W_2
$$
在这两层MLP中，第一层将输入的向量升维，第二层将向量重新降维，这样可以学习到更加抽象的特征。

### 主要作用

FFN的主要作用是在输入的特征上进行非线性变换，从而增加模型的表达能力。自注意力机制主要负责捕捉输入序列中各个元素之间的关系（上下文信息），而FFN通过引入非线性特性，使得模型能够更好地学习复杂的表示。

在自然语言处理中，语言的复杂性往往不仅仅依赖于词与词之间的关系，还涉及上下文的多样性和丰富性。FFN通过激活函数引入了非线性，可以帮助模型更好地处理这些复杂的关系。此外，FFN也为每个位置的表示引入了独立的学习能力，使得模型可以处理不同位置的模式。

### FFN 和 Transformer

在Transformer模型中，FFN位于多头自注意力机制之后。每个编码器和解码器层都包含一个FFN，用于对自注意力机制的输出进行进一步的处理。这种设计使得模型能够在捕获全局依赖关系的同时，对局部特征进行更精细的处理。

## Multi-Layer Perceptron (MLP)

MLP（Multi-Layer Perceptron）和全连接层（Fully Connected Layer，也称为 Dense Layer）之间有密切的关系，但它们并不完全相同。MLP 是一种神经网络架构，而全连接层是构成 MLP 的基本组件之一。以下是它们之间的关系和区别：

### MLP

1. **定义**：
   - MLP 是一种前馈神经网络（Feed-Forward Neural Network），由多个层组成，每层包含多个神经元（节点）。MLP 至少包含三层：输入层、一个或多个隐藏层和输出层。
   - MLP 的关键特点是每层的神经元与下一层的神经元完全连接，即每个神经元都与下一层的所有神经元相连。
2. **结构**：
   - **输入层**：接收输入数据。
   - **隐藏层**：对输入数据进行非线性变换，提取特征。
   - **输出层**：生成最终的输出结果。
3. **激活函数**：
   - MLP 中的每个神经元通常使用非线性激活函数（如 ReLU、Sigmoid 或 Tanh）来引入非线性特性，使网络能够学习复杂的模式。
4. **用途**：
   - MLP 是一种通用的神经网络架构，可以用于各种任务，包括分类、回归和特征提取等。

### Dense Layer

1. **定义**：

   - 全连接层是神经网络中的一种层类型，其中每个神经元与前一层的所有神经元相连。全连接层通常用于 MLP 中，也可以用于其他神经网络架构（如 CNN 和 RNN）中。

2. **结构**：

   - 全连接层由一组权重（参数）和偏置组成。每个神经元的输出是前一层所有神经元输出的加权和加上一个偏置，然后通过激活函数进行非线性变换。

3. **数学表示**：

   - 假设输入向量为 $x∈Rn$，权重矩阵为 $W∈Rm×n$，偏置向量为 $b∈Rm$，则全连接层的输出为：

     $y=f(xW+b)$

     其中，$f$是激活函数。

4. **用途**：

   - 全连接层通常用于神经网络的最后几层，将特征映射到输出空间。在 MLP 中，全连接层是构成网络的主要组件。

### 关系和区别

- **关系**：
  - MLP 是一种神经网络架构，由多个全连接层组成。
  - 全连接层是 MLP 的基本组件，用于实现层与层之间的连接和数据变换。
- **区别**：
  - **MLP** 是一种网络架构，包含多个层（输入层、隐藏层和输出层），每层之间通过全连接层连接。
  - **全连接层** 是一种层类型，可以用于 MLP 中，也可以用于其他神经网络架构中。全连接层负责实现层与层之间的连接和数据变换。

MLP 是一种由多个全连接层组成的神经网络架构，而全连接层是构成 MLP 的基本组件。

全连接层负责实现层与层之间的连接和数据变换，而 MLP 则通过多个全连接层的堆叠来实现复杂的特征提取和模式学习。