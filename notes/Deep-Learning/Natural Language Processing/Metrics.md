# NLP Metrics

## Classification

NLP领域最常见、应用最广泛的一类任务。

- **核心目标：** 给定一段文本（可以是单个词、一个句子或整篇文章），模型需要从一个预定义的类别集合中，为该文本分配一个最合适的标签。这是一个“多对一”的映射。

### Text Classification

**文本分类 (Text Classification):** 这是一个总称，泛指所有将文本归类的任务。

- **例子：** 将新闻文章自动分类到 “体育”、“财经”、“科技”、“娱乐” 等频道。

准确度(Accuracy) 评估预测正确的比例，精确率(Precision) 评估预测正例的查准率，召回率(Recall) 评估真实正例的查全率。如果是多分类，则每个类别各自求P、R最终求平均值。

#### Accuracy

-**详细定义:** 准确率是指在所有样本中，模型预测正确的样本（包括正确预测为正类和正确预测为负类）所占的比例。这是最直观、最容易理解的指标。

**使用场景:** 准确率适用于**数据类别分布均衡**的场景。如果不同类别的样本数量大致相同，准确率可以很好地反映模型的整体性能。然而，在类别不均衡的数据集上，这个指标会产生严重的误导性（例如，在99%都是负类的样本中，模型将所有样本都预测为负类，准确率高达99%，但模型其实毫无价值）。
$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

#### Precision

**详细定义:** 精确率，又称**查准率**，它关注的是**所有被模型预测为正类的样本中，有多少是真正的正类**。这个指标衡量的是模型预测结果的“纯度”，即预测为正的结果有多么可信。

**使用场景:** 当**假正例 (FP) 的代价非常高**时，我们会优先考虑精确率。换句话说，我们希望模型的预测非常“准”，宁可漏掉一些，也不能错判。

- **例子：** 垃圾邮件检测。将一封重要的工作邮件（负类）错误地识别为垃圾邮件（正类）的后果很严重，因此我们要求高精确率。

$$
\text{Precision} = \frac{TP}{TP + FP}
$$



#### Recall

**详细定义:** 召回率，又称查全率，它关注的是**所有真实为正类的样本中，有多少被模型成功地预测了出来**。这个指标衡量的是模型发现正类样本的“完整性”或能力。

**使用场景:** 当**假负例 (FN) 的代价非常高**时，我们会优先考虑召᱐回率。也就是说，我们希望模型能把我们关心的类别尽可能多地找出来，宁可误判一些，也不能漏掉。

- **例子：** 疾病诊断或金融欺诈检测。漏掉一个真正的病人或一笔欺诈交易（将其误判为正常）的后果是灾难性的，因此我们要求高召回率。

$$
\text{Recall} = \frac{TP}{TP + FN}
$$



#### F1-Score

**详细定义:** F1-Score是精确率和召回率的**调和平均数 (Harmonic Mean)**。它是一个综合性指标，旨在同时兼顾模型的精确率和召回率，作为两者的平衡。只有当精确率和召回率都比较高时，F1-Score才会高。

**使用场景:**

- 在**数据类别不均衡**的情况下，F1-Score是一个比准确率更可靠的指标。
- 当你希望在**精确率和召回率之间寻求一个平衡**时，F1-Score是一个非常重要的参考。它是机器学习竞赛和学术研究中最常用的评估指标之一。

$$
\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}
$$



#### Weighted(-Average) F1-Score

这是在评估多分类模型时最常用的指标之一，尤其是在**类别不均衡**的数据集上。

- **核心思想：** 在多分类任务中（例如，将新闻分为体育、财经、科技、娱乐四个类别），我们会先针对**每一个类别**单独计算其F1-Score（例如，计算“体育”类的F1，再计算“财经”类的F1，以此类推）。

  但是，如何将这些单独的F1分数合并成一个总分呢？加权平均F1分数的核心思想是：**根据每个类别样本数量的多少，来赋予其F1分数不同的权重。** 样本多的类别，其F1分数在总分中的占比就大；样本少的类别，其F1分数在总分中的占比就小。

- **计算步骤：**

  1. **独立计算：** 对每一个类别 i，都计算出它自己的F1分数（F1i）。
  2. **计算权重：** 计算每一个类别 i 的样本数量在总样本数量中的占比，这个占比就是它的权重（wi）。
  3. **加权求和：** 将每个类别的F1分数与其对应的权重相乘，然后将所有结果相加，得到最终的加权平均F1分数。

- **使用场景：** 当你认为**样本量大的类别更重要**，模型在这些主要类别上的表现更能反映其整体价值时，就应该使用加权平均F1分数。它能够很好地反映模型在整体数据分布上的性能，避免了模型因在极少数样本的类别上表现差而导致总分过低的情况。

计算公式：
$$
F1_{\text{weighted}} = \sum_{i=1}^{L} w_i \times F1_i
$$
其中： * $L$ 是类别的总数。 * $F1_i$ 是第 $i$ 个类别的 F1-Score。 * $w_i$ 是第 $i$ 个类别的权重，其计算方法为：    
$$
w_i = \frac{\text{属于类别 } i \text{ 的真实样本数量}}{\text{总样本数量}}
$$
这个公式的核心思想是：将每个类别单独的F1分数，乘以该类别样本在总样本中所占的比例（即权重），然后将所有类别的结果相加。

**为什么这个指标很重要？** 它的主要价值在于**处理类别不均衡 (Imbalanced Classes) 的数据集**。

- 在上面的例子中，“猫”是主要类别，“鸟”是稀有类别。Weighted-F1 在计算总分时，会让模型在“猫”这个大类上的表现占据更大的比重。
- 这样做的好处是，最终得分能够更好地反映模型在**整体数据分布**上的表现。如果一个模型只是在样本极少的类别上表现不好，但在样本占绝大多数的类别上表现优异，Weighted-F1分数依然会比较高。

**与 Macro-F1 (宏平均F1) 的关键区别：**

- **Weighted-F1 (加权平均):** **考虑**每个类别的样本数量。样本多的类别，话语权更重。
- **Macro-F1 (宏平均):** **不考虑**每个类别的样本数量。它平等地对待所有类别，无论大小，简单地将所有类别的F1分数求算术平均。如果你特别关心模型在稀有类别上的性能，Macro-F1是更好的选择。

### sentiment classification

**情感分析 (Sentiment Analysis):** 这是文本分类的一个热门子领域。主要用于判断文本所表达的情感倾向是正面的、负面的还是中性的。

- **例子：** 分析一条电影评论 “这部电影的特效很棒，但剧情太拖沓了。” 模型可能将其判断为“中性”或同时抽取出正面（特效）和负面（剧情）两种情感。



### subject classification

**主题分类 (Topic Classification):** 也称主题建模，旨在识别出文本或文档集合所讨论的核心主题。

- **例子：** 分析一篇关于“电动汽车电池技术和续航里程”的文章，将其主题归类为“新能源汽车”。





## Sentence Pair Relationship Judgment

**句对关系判断**。

- **核心目标：** 这类任务的输入通常是**两个文本片段（句子对）**，模型需要判断这两个句子之间存在什么样的关系。

### QA - Question Answering

**问答 (QA - Question Answering):** 在某些形式的问答（如阅读理解）中，模型需要判断一个“问题”（句子A）和一个“候选答案”或“上下文段落”（句子B）之间的关系，以确定答案是否正确或从段落中抽取答案。

- **例子：** 给定上下文“企鹅是一种主要生活在南半球的鸟类”，问题是“企鹅生活在哪里？”，模型需要判断上下文和问题的关系，并给出“南半球”。

### Textual Entailment

**文本蕴含 (Textual Entailment):** 也称为自然语言推断（NLI）。判断一个“前提”句子是否能推断出另一个“假设”句子。关系通常分为三种：蕴含（entailment）、矛盾（contradiction）和中立（neutral）。

- **例子：** 前提：“一个男孩在开心地踢足球。” 假设：“有人在踢球。” 模型应判断出“蕴含”关系。

### Text Semantic Similarity

**文本语义相似性 (Text Semantic Similarity):** 判断两个句子的意思是否相似或相关。这在信息检索和文本去重等场景中非常有用。

- **例子：** 判断句子 “如何给我的电脑提速？” 和 “怎样让我的计算机运行得更快？” 语义上是高度相似的。



## Sequence Labeling

**序列标注**。

- **核心目标：** 这类任务的目标是为文本序列中的**每一个基本单元**（通常是字或词）分配一个预定义的标签。它的输出和输入序列在长度上是严格对应的，像是在给句子中的每个词“贴标签”。

### NER - Named Entity Recognition

**命名实体识别 (NER - Named Entity Recognition):** 这是最经典的序列标注任务。它的目的是从文本中识别出具有特定意义的实体，并为它们分类，比如人名（PER）、地名（LOC）、组织机构名（ORG）等。

- **例子：** 对于句子 “`[姚明]`<sub>PER</sub> 出生于 `[上海]`<sub>LOC</sub>。” 模型需要识别出 `姚明` 是人名，`上海` 是地名。

### Semantic Labeling

**语义标注 (Semantic Labeling):** 这是一个更广义的概念，包括但不限于语义角色标注（SRL）等。它旨在分析句子中各成分扮演的语义角色，例如“施事者”、“受事者”、“时间”、“地点”等，以更深入地理解句子结构。

- **例子：** 对于句子 “张三昨天在北京打了李四。” 模型需要标出 `张三` 是“施事者”，`李四` 是“受事者”，`昨天` 是“时间”，`北京` 是“地点”。

### Word Segmentation

**分词 (Word Segmentation):** 主要针对中文、日文等没有天然空格分隔的语言。任务是把一个连续的句子切分成一个一个有意义的词语序列。这通常是其他更高级中文NLP任务的预处理步骤。

- **例子：** 将 “我爱北京天安门” 切分为 “我 / 爱 / 北京 / 天安门”。

## Generative Tasks/Seq2Seq

生成式任务（Generative Tasks）和序列到序列（Seq2Seq）这类任务（如机器翻译、文本摘要、对话生成）的评估比分类任务要复杂得多，因为它们的输出是自由生成的文本序列，不存在唯一的“正确答案”。一个好的生成结果需要在语法、语义、流畅性、相关性等多个维度上表现出色。

目前，评估方法主要分为两大类：**自动评估指标** 和 **人工评估**。



### Machine Translation

**机器翻译 (Machine Translation):** 将源语言的文本自动翻译成目标语言的文本。这是最经典的生成式任务。

- **例子：** 将 “Hello, world!” 翻译成 “你好，世界！”。

#### BLEU (Bilingual Evaluation Understudy)

BLEU 是机器翻译领域最经典、最流行的评估指标，至今仍被广泛使用。

- **核心思想：** “一个好的机器翻译结果，应该与专业人工翻译的结果在词组（n-grams）上有更多的重叠。” BLEU通过计算模型生成译文（Candidate）与一个或多个参考译文（References）之间n-gram的**精确率**来衡量翻译质量。它会考察1-gram（单个词）、2-gram、3-gram直到通常的4-gram的匹配度。
- **计算特点：**
  1. **n-gram精确率：** 计算生成文本中的n-gram出现在参考文本中的比例。
  2. **重复惩罚 (Clipping)：** 为了避免模型通过生成大量重复的高频词（如 "the the the"）来刷高分数，一个词的匹配次数不能超过它在任何单个参考译文中出现的最大次数。
  3. **长度惩罚 (Brevity Penalty, BP)：** 如果模型生成的句子比参考译文短很多，会被施加惩罚。这是因为短句子天然更容易获得高的n-gram精确率，但这并不代表翻译质量好。如果生成句子的长度大于等于最短的参考译文，则惩罚为1（即不惩罚）。
- **主要应用场景：**
  - **机器翻译 (Machine Translation)**
  - 也可用于其他生成任务，如文本摘要、图像描述生成等，但效果不如在翻译中好。
- **优点：**
  - 计算速度快，成本低，便于大规模自动评估。
  - 与单语种的人工评估得分具有较好的相关性。
  - 是学术界和工业界评估翻译模型的通用基准。
- **缺点：**
  - **不关心召回率：** 只看生成文本中的词组是否在参考文本里，不关心参考文本中的词组是否被生成。
  - **语义忽视：** 不理解同义词或释义。例如，生成 "fast" 而参考答案是 "quick"，BLEU会认为不匹配。
  - **短文本评估不佳：** 对于单个句子的评估结果可能波动很大，更适合在大型测试集上评估语料库级别的整体性能。
  - **对语序不敏感：** 虽然n-gram（n>1）在一定程度上捕捉了语序，但对于长距离的语序颠倒不敏感。

计算过程：

假设我们要评估一句机器翻译的结果。

- **参考译文 1 (Reference 1):** `The cat is on the mat`
- **参考译文 2 (Reference 2):** `There is a cat on the mat`
- **机器译文 (Candidate):** `the cat the cat on the mat`

BLEU的计算主要包含四个步骤：

1. 计算修正的n-gram精确率 (Modified n-gram Precision)
2. 计算几何平均值 (Geometric Mean)
3. 计算长度惩罚 (Brevity Penalty)
4. 得出最终BLEU分数

第一步：计算修正的n-gram精确率 ($p_n$)

标准的BLEU会计算从1-gram到4-gram的精确率。为了方便理解，我们详细计算1-gram和2-gram，并简要说明3-gram和4-gram。

首先，列出机器译文中的所有1-gram（单个词）： `the`, `cat`, `the`, `cat`, `on`, `the`, `mat` (共7个词)

接下来，我们要计算这些词中有多少是“正确”的。这里的关键是**“修正”**，也叫**“裁剪 (Clipping)”**。一个词在机器译文中出现的次数，不能超过它在**任何单个**参考译文中出现的最大次数。

- **词 "the":**
  - 在机器译文中出现 **3** 次。
  - 在参考译文1中出现 **2** 次。
  - 在参考译文2中出现 **1** 次。
  - 最大次数是 **2**。所以 "the" 的有效匹配次数（Clipped Count）是 **2**。
- **词 "cat":**
  - 在机器译文中出现 **2** 次。
  - 在参考译文1中出现 **1** 次。
  - 在参考译文2中出现 **1** 次。
  - 最大次数是 **1**。所以 "cat" 的有效匹配次数是 **1**。
- **词 "on":**
  - 在机器译文中出现 **1** 次。
  - 在参考译文1和2中都出现 **1** 次。
  - 最大次数是 **1**。有效匹配次数是 **1**。
- **词 "mat":**
  - 在机器译文中出现 **1** 次。
  - 在参考译文1和2中都出现 **1** 次。
  - 最大次数是 **1**。有效匹配次数是 **1**。

$$
p_1 = \frac{所有词的有效匹配次数之和}{机器译文的1-gram总数}=\frac{2+1+1+1}{7}=\frac{5}{7}
$$

 **思考：** 如果没有裁剪机制，机器译文中的 "the" 会被算对3次，"cat" 会被算对2次，精确率会虚高。裁剪机制有效地惩罚了无意义的词语重复。

2. 计算 $p_2$ (2-gram 精确率)

列出机器译文中的所有2-gram（两个词的组合）： `the cat`, `cat the`, `the cat`, `cat on`, `on the`, `the mat` (共6个)

同样，我们进行裁剪匹配：

- **"the cat":** 在机器译文中出现 **2** 次。在参考译文1中出现1次，在参考译文2中0次。最大次数是 **1**。有效匹配次数是 **1**。
- **"cat the":** 在两个参考译文中都**未出现**。有效匹配次数是 **0**。
- **"cat on":** 在两个参考译文中都**未出现**。有效匹配次数是 **0**。
- **"on the":** 在机器译文中出现 **1** 次。在参考译文1和2中都出现1次。最大次数是 **1**。有效匹配次数是 **1**。
- **"the mat":** 在机器译文中出现 **1** 次。在参考译文1和2中都出现1次。最大次数是 **1**。有效匹配次数是 **1**。

**$p_2$ 的计算：**
$$
p_2 = \frac{1+0+0+1+1}{6}=\frac{1}{2}
$$


3. 计算 p3 和 p4

- **$p_3$ (3-gram):** 机器译文中的3-gram有 `the cat the`, `cat the cat`, `the cat on`, `cat on the`, `on the mat`。其中只有 `on the mat` 在两个参考译文中都出现了。所以 $p_3$=1/5。
- **$p_4$ (4-gram):** 机器译文中的4-gram有 `the cat the cat`, `cat the cat on`, `the cat on the`, `cat on the mat`。其中 `cat on the mat` 在参考译文2中出现。所以 $p_4$=1/4。

第二步：计算几何平均值

BLEU将这些精确率通过几何平均值组合起来，通常取N=4。四个p值相乘再开四次根号。

第三步：计算长度惩罚 (Brevity Penalty, BP)

这个步骤是为了惩罚那些翻译得太短的句子。

- **机器译文长度 (c):** 7
- **参考译文长度:** 参考译文1长度为6，参考译文2长度为7。
- **有效参考长度 (r):** 选择与机器译文长度**最接近**的参考译文长度。这里7和7最接近，所以 r = 7。

第四步：得出最终BLEU分数

最终的BLEU分数就是将几何平均值与长度惩罚相乘。



BLEU的几个核心机制：

1. **n-gram匹配：** 衡量词组级别的相似度。
2. **裁剪机制：** 防止模型通过重复高频词刷分。
3. **长度惩罚：** 确保译文长度不会过短。
4. **几何平均：** 综合各n-gram的精确率，鼓励在各个级别上都有较好的表现。

需要强调的是，BLEU分数在单个句子上可能没有太大意义，它的价值主要体现在对大规模测试集（成千上万个句子）进行整体评估。

这张表可以作为一个粗略的参考，帮助您快速判断一个BLEU分数所处的水平：

| BLEU 分数   | 翻译质量的大致描述                                           |
| ----------- | ------------------------------------------------------------ |
| **> 60**    | **非常高质量**。通常认为是与人类专业翻译质量相当或非常接近的水平。 |
| **40 - 60** | **高质量翻译**。意思连贯，语法正确，大部分内容流畅可读。     |
| **30 - 40** | **良好且可理解的翻译**。这是目前许多**主流NMT（神经机器翻译）模型在学术基准测试**上能够达到的范围。句子的主干意思基本正确，但可能存在一些语法错误或不自然的表达。 |
| **20 - 30** | **基本可懂的翻译**。句子的主旨大意可以被理解，但通常包含明显的语法错误和不通顺之处。在NMT出现之前，很多**SMT（统计机器翻译）系统**位于这个区间。 |
| **10 - 20** | **质量较差的翻译**。能够翻译出一些词组和片段，但整个句子可能支离破碎，难以连贯理解。 |
| **< 10**    | **极差的翻译**。基本上是无用的，大部分内容都无法理解。       |

为了让这个范围更具体，我们来看一些实际的例子：

1. **人类水平 (Human Level):** 有趣的是，即使是**人类专业译员之间，BLEU分数也不是100**。让一个译员去翻译，然后用另外几个译员的译文作为参考，他们之间的BLEU分数通常在 **60到80** 之间。这可以被看作是当前任务下BLEU分数的“天花板”。
2. **业界顶尖模型 (State-of-the-Art Models):**
   - 在一些主流的翻译任务评测（如WMT - Workshop on Machine Translation）中，对于**英语↔德语**、**英语↔法语**这类比较成熟的语言对，目前最顶尖的Transformer模型（如Google、DeepL、Microsoft等的系统）的BLEU分数大约在 **35到45** 之间。几年前这个数字还在20多。
   - 对于**英语↔汉语**、**英语↔日语**这类难度更大的语言对，由于语言结构差异大，顶尖模型的BLEU分数通常会低一些，可能在 **25到35** 之间。
3. **学术研究中的基线 (Baseline in Research):**
   - 在学术论文中，如果你看到一个针对标准数据集（如WMT14 En-De）的模型，其BLEU分数达到了 **28-30**，这通常被认为是一个非常扎实的基线（Baseline）模型。
   - 如果一个新的模型在此基础上提升了1-2个BLEU点（例如从29.5提升到31.0），这通常被认为是一个显著的进步（Significant Improvement）。

- **30-40分是当前强力NMT模型的一个常见区间。** 如果你看到一个模型在这个范围，可以认为它的性能是相当不错的。
- **不要脱离上下文比较BLEU分数。** 比较两个模型的BLEU分数，必须确保它们是在**完全相同**的测试集、参考译文和分词方法下得出的。
- **BLEU分数的提升并不总是等同于人类感知的质量提升。** 有时一个模型BLEU分数提高了0.5，但人类用户可能完全感受不到差异。
- BLEU只是一个评估工具，它有其局限性（比如不理解同义词），因此业界也在越来越多地使用如BERTScore等更侧重语义的指标，并最终结合人工评测来综合判断。





### Text Summarization

**文本摘要 (Text Summarization):** 为一篇长文档或文章自动生成一个简短、精炼的摘要，概括其核心内容。

- **例子：** 输入一篇数千字的新闻报道，模型输出一个一百字左右的内容提要。

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

ROUGE 是为了评估自动文本摘要而设计的指标，后来也被用于机器翻译等任务。与BLEU相反，它的核心是**召回率**。

- **核心思想：** “一个好的摘要，应该覆盖了参考摘要中的主要信息点。” ROUGE通过计算参考摘要中的n-gram有多少出现在了模型生成的摘要中来衡量摘要的质量。
- **常见变体：**
  - **ROUGE-N (例如, ROUGE-1, ROUGE-2):** 基于n-gram的召回率。ROUGE-1衡量词汇重叠，ROUGE-2衡量短语重叠。
  - **ROUGE-L (Longest Common Subsequence):** 基于最长公共子序列（LCS）。它不要求词组是连续的，只要在句子中保持了先后顺序即可。这使得它能更好地处理同义词替换或句子结构变化的情况。例如，“A cat is on the mat” 和 “The cat sat on the mat” 具有较长的LCS。
  - **ROUGE-S (Skip-Bigram):** 允许n-gram中的词之间有间隔，比ROUGE-L更灵活。
- **主要应用场景：**
  - **自动文本摘要 (Text Summarization)**
  - 机器翻译、问答系统等。
- **优点：**
  - **关注信息覆盖：** ROUGE-1/ROUGE-2的高分通常意味着摘要覆盖了关键实体和事实。
  - **ROUGE-L的灵活性：** 对句子结构的变化不那么敏感，能更好地评估摘要的流畅性。
- **缺点：**
  - **只关心召回率：** 为了刷高ROUGE分数，模型可能会生成非常冗长的摘要，包含所有可能的关键词，但这会牺牲摘要的简洁性和可读性（精确率低）。（注：实际报告ROUGE时，通常也会同时报告其精确率和F1分数）。
  - 与BLEU一样，它无法理解语义层面的相似性。



与BLEU类似，ROUGE分数也**没有一个绝对的标准**，它的数值分布与任务类型、数据集、摘要长度等因素密切相关。

但是，我们可以通过分析一些主流公开数据集上的SOTA（State-of-the-Art）模型表现，来建立一个对ROUGE分数数值的直观概念。在学术论文和实际项目中，最常报告的是 **ROUGE-1**, **ROUGE-2** 和 **ROUGE-L** 的 **F1-Score**。

这三个指标通常遵循一个固定的数值高低关系：

**ROUGE-1 > ROUGE-L > ROUGE-2**

- **ROUGE-1 (词重叠)**: 因为只考虑单个词的重叠，所以分数总是**最高**的。它主要反映了摘要是否抓住了关键的**实体词**和**主题词**。
- **ROUGE-2 (词对重叠)**: 要求连续的两个词都匹配，非常严格，所以分数总是**最低**的。它在一定程度上能反映摘要的**流畅性**和**短语级别的准确性**。
- **ROUGE-L (最长公共子序列)**: 介于两者之间。它比ROUGE-1严格（因为它要考虑词序），但比ROUGE-2灵活（因为它不要求词是连续的）。它能较好地反映摘要**句子结构的相似性**。

**分析技巧：** ROUGE-1和ROUGE-2之间的**差距 (Gap)** 常常被用来非正式地评估摘要的质量。**如果一个模型的ROUGE-1很高，但ROUGE-2很低，这可能意味着模型只是简单地堆砌了一些关键词，而没有形成流畅、可读的短语和句子。**

ROUGE分数的“好坏”极大程度上取决于数据集。我们以两个最著名的英文摘要数据集为例：**CNN/DailyMail** 和 **XSum**。

这个数据集的任务是为一篇几百字的新闻文章生成一个3-4句话的摘要。摘要内容与原文的重合度较高（比较“抽取式”）。

- **当前SOTA模型的表现 (例如 PEGASUS, BART):**
  - **ROUGE-1: ≈ 44 - 47**
  - **ROUGE-2: ≈ 21 - 24**
  - **ROUGE-L: ≈ 41 - 44**
- **人类水平 (Human Performance):**
  - 人类写的摘要与参考摘要对比，分数也远非100。大约在：
  - **ROUGE-1: ≈ 53**
  - **ROUGE-2: ≈ 29**
  - **ROUGE-L: ≈ 50**

**解读:** 在CNN/DailyMail这类任务上，如果你的模型能达到 **ROUGE-1 > 40**，**ROUGE-2 > 20**，**ROUGE-L > 40**，就可以被认为是一个**非常有竞争力的、表现出色的模型**。

这个数据集的任务非常具有挑战性，要求为一篇文章生成一个**单句**的、**高度概括性**的摘要，摘要中的词语和原文重合度很低（非常“生成式”）。

- **当前SOTA模型的表现:**
  - 由于任务更难，且摘要更短，分数分布与CNN/DM有所不同。
  - **ROUGE-1: ≈ 45 - 50** (因为摘要短，命中关键词相对容易)
  - **ROUGE-2: ≈ 22 - 25**
  - **ROUGE-L: ≈ 38 - 42**

**解读:** 尽管任务更难，但SOTA模型在XSum上的ROUGE-2分数与CNN/DM相近。这说明，要生成一个既高度概括又保留了原文核心短语结构的单句话是极为困难的。能在这个数据集上达到 **ROUGE-2 > 20** 就已经是非常强的表现。

1. **没有绝对的“好”分数：** 一个ROUGE-2达到15分在某些困难任务上可能是SOTA，但在另一些简单任务上可能不及格。**分数的好坏必须在同一数据集的背景下讨论。**
2. **数值范围参考：** 对于常见的英文新闻摘要任务，一个强大的NLG（自然语言生成）模型大致处于以下范围：
   - **ROUGE-1: 40s**
   - **ROUGE-2: 高 teens 到 低 20s (例如 18-24)**
   - **ROUGE-L: 接近 ROUGE-1，通常也在 40s**
3. **如何使用：** 在你自己的项目中，你应该首先找到与你任务最相似的公开数据集，查看该数据集上的SOTA模型分数，并以此作为你对标的**基准 (Benchmark)**。
4. **ROUGE的局限性：** 和BLEU一样，ROUGE只关心词汇重叠，不理解语义。它无法判断摘要是否事实准确、是否存在幻觉（Hallucination），也无法评估其真实的可读性。因此，它通常需要与其他指标（如BERTScore）和人工评估结合使用。

### Image Captioning

**图像描述生成 (Image Captioning):** 这是一个多模态任务，结合了计算机视觉（CV）和NLP。模型需要根据输入的图像，生成一句或一段描述图像内容的文字。

- **例子：** 输入一张“一只猫躺在阳光下的草地上”的图片，模型生成描述性文字：“A cat is lying on the grass in the sun.”。

## Text Matching and Retrieval





