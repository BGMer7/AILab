[[Cosine Annealing]]

热重启是一种学习率调度策略，旨在通过周期性地将学习率恢复到较高值并重新开始下降的方式来增强优化过程的效果，帮助模型跳出局部最优解。

热重启通常与学习率退火方法（如 **余弦退火**）结合使用。每当达到一个指定的周期时，学习率会“重启”，回到初始值，并在接下来的周期中再次按设定规则逐渐减小。

热重启（Warm Restarts）在深度学习优化中是为了解决以下问题而提出的：

### **1. 防止陷入局部最优解**
- **问题**：在优化过程中，学习率逐渐减小可能会导致模型容易陷入局部最优解，而无法进一步提升性能。
- **解决方法**：通过热重启，周期性地将学习率恢复到较高值，可以跳出局部最优点，从而探索更广的解空间。

### **2. 提高收敛速度**
- **问题**：学习率过小可能导致训练后期收敛速度缓慢。
- **解决方法**：热重启通过周期性地恢复学习率，能够加快优化器的收敛过程，同时保证局部探索的效果。

### **3. 解决学习率调整的僵化问题**
- **问题**：传统学习率调整策略（如 StepLR）通常是按固定步长或者监控某个指标进行学习率衰减，调整策略可能不够灵活。
- **解决方法**：热重启提供了一种周期性的动态调整机制，使模型训练过程更加灵活。

### **4. 增强模型的泛化能力**
- **问题**：在训练后期，模型可能过于贴合训练数据，导致泛化能力下降。
- **解决方法**：热重启增加了学习率的周期性变化，使优化过程更具随机性，帮助模型探索更多解，从而提高泛化能力。

### **5. 适应不均匀的数据分布**
- **问题**：数据分布不均匀可能导致训练过程中模型优化效率不稳定。
- **解决方法**：热重启通过周期性调整学习率，让模型在不同阶段适应数据特性变化，从而提高训练稳定性。

### **热重启的优点**

1. **避免局部最优**：通过周期性地恢复学习率，增加模型跳出局部最优的机会。
2. **提高泛化能力**：在重新探索更广解空间的同时，增强模型对新数据的适应性。
3. **与退火策略结合**：结合余弦退火等策略，进一步提升模型的收敛效果。

### **热重启的缺点**

1. **训练时间可能增加**：重启后学习率较大，可能导致模型需要更长时间再次收敛。
2. **超参数选择复杂**：需要调整 `T_0`、`T_mult` 等参数，可能增加调参成本。
3. **不适用所有任务**：对于某些任务或数据集，热重启可能无法显著提升性能。

ref:
https://arxiv.org/abs/1608.03983