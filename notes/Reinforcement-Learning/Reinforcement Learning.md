[[Policy]]
[[Supervised Learning]]
[[Markov Decision Process(MDP)]]
[[Actor-Critic]]
[[Policy based RL]]
[[Value based RL]]
[[On-Policy & Off-Policy]]

```
强化学习（Reinforcement Learning）
├── **基于模型（Model-Based）**
│   ├── 动态规划（Dynamic Programming）
│   │   ├── 策略迭代（Policy Iteration）
│   │   └── 价值迭代（Value Iteration）
│   └── 蒙特卡洛树搜索（MCTS）
│
├── **无模型（Model-Free）**
│   ├── **基于价值（Value-Based）**
│   │   ├── 时序差分（TD Learning）
│   │   │   ├── Q-Learning（Off-Policy）  
│   │   │   └── SARSA（On-Policy）
│   │   └── 深度Q网络（DQN）及其变体（DDQN, C51）
│   │
│   ├── **基于策略（Policy-Based）**
│   │   ├── 策略梯度（Policy Gradient）
│   │   │   └── REINFORCE
│   │   └── 近端策略优化（PPO）
│   │
│   └── **Actor-Critic**
│       ├── A3C（Asynchronous Advantage Actor-Critic）
│       └── DDPG（Deep Deterministic Policy Gradient）
│
├── **混合方法**
│   ├── 蒙特卡洛（Monte Carlo, MC）方法
│   └── TD(λ)（结合MC与TD）
│
└── **其他分支**
    ├── 逆强化学习（Inverse RL）
    └── 多智能体强化学习（MARL）
```

# Action
- action(简记为$a$)，玩超级玛丽的时候你会控制马里奥做三个动作，即向左走、向右走和向上跳，而马里奥做的这三个动作就是action
# Agent
- Agent，一般译为智能体，就是我们要训练的模型，类似玩超级玛丽的时候操纵马里奥做出相应的动作，而这个马里奥就是Agent
# Reward
- reward(简记为$r$)，这个奖赏可以类比为在明确目标的情况下，接近目标意味着做得好则奖，远离目标意味着做的不好则惩，最终达到收益/奖励最大化，且这个奖励是强化学习的核心
在强化学习中，奖励（reward）可以被视为针对 **状态转移**（state transition）的概念，而不是严格针对特定状态的概念。虽然某些情况下奖励可能与状态直接相关，但奖励通常被定义为一个智能体采取某个动作后，从当前状态转移到下一个状态时从环境获得的即时回报。

### 两种常见情况

1. **状态相关的奖励**
   
    - **定义**：奖励只与下一个状态有关，而与当前状态和动作无关。例如，奖励函数可以定义为 r(s′) 或 r(s′)，其中 s′ 是下一个状态。
      
    - **例子**：考虑一个网格世界（grid world）问题。假设某些状态（如终点）具有正奖励，而其他状态（如陷阱）具有负奖励。当智能体进入这些状态时就会立即获得相应的奖励，不管它从哪个方向进入该状态。
      
    - **数学形式**：奖励函数简化为 r(s′)，在某些文献中也表示为 r(s′) 或 R(s′)。
    
2. **状态转移相关的奖励**
   
    - **定义**：奖励与当前状态、动作和下一个状态都相关。例如，奖励函数可以定义为 r(s,a,s′)，它不仅取决于当前状态和动作，还取决于转移到的下一个状态。
      
    - **例子**：在自平衡车辆的控制场景中，奖励可能与车辆的速度变化有关。从某个状态到下一个状态的速度变化会影响奖励的大小。例如，如果车辆在加速过程中保持稳定，可能会获得正奖励，而如果速度变化导致车辆不稳定，则可能获得负奖励。
      
    - **数学形式**：奖励函数定义为 r(s,a,s′)，这是最一般的形式。
      

虽然奖励有时可以简化为与状态相关的概念，但更一般的情况是它与状态转移相关，即奖励由当前状态、动作和下一个状态共同决定。强化学习算法通常处理的是更一般的情况，即奖励函数 r(s,a,s′)，但在某些问题中，可以将其简化为只依赖于下一个状态 r(s′) 的形式。



# Return

在强化学习中，reward和return是两个不同的概念，

**奖励 Reward:**

奖励是在某个时间步上由环境给予智能体（Agent）的反馈信号，用以表明智能体在当前状态下采取某个动作的好坏。
奖励可以是正的（表示好的结果或期望的行为），也可以是负的（表示坏的结果或不期望的行为），或者是零（表示中性或无关紧要的结果）。
**回报 Return:**

回报是从当前时间步开始，智能体在未来一系列时间步内所能获得的奖励的总和，通常这个总和会被一个折扣因子（Discount Factor）所折扣，以体现未来奖励的不确定性或时间价值。
回报是一个累积的概念，它考虑了从当前时刻开始到未来某个时刻（可能是一个完整的回合结束）的所有奖励。
强化学习的目标通常是最大化期望回报，即智能体学习一个策略，使得从任意状态开始，按照该策略行动所能获得的长期累积奖励最大。

在数学表示上，return通常被定义为：
$$
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$


# State
- 环境的状态
# Policy



# RL和SL的区别
此外，RL和监督学习（supervised learning）的区别：

监督学习有标签告诉算法什么样的输入对应着什么样的输出（譬如分类、回归等问题）
所以对于监督学习，目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数，相当于最小化预测误差
最优模型 = arg minE { [损失函数(标签,模型(特征)] }

RL没有标签告诉它在某种情况下应该做出什么样的行为，只有一个做出一系列行为后最终反馈回来的reward，然后判断当前选择的行为是好是坏
相当于RL的目标是最大化智能体策略在和动态环境交互过程中的价值，而策略的价值可以等价转换成奖励函数的期望，即最大化累计下来的奖励期望
最优策略 = arg maxE { [奖励函数(状态,动作)] }

监督学习如果做了比较坏的选择则会立刻反馈给算法
RL的结果反馈有延时，有时候可能需要走了很多步以后才知道之前某步的选择是好还是坏
监督学习中输入是独立分布的，即各项数据之间没有关联
RL面对的输入总是在变化，每当算法做出一个行为，它就影响了下一次决策的输入

## Refs
[DQN 从入门到放弃1 DQN与增强学习 - 知乎](https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit)
[DQN 从入门到放弃2 增强学习与MDP - 知乎](https://zhuanlan.zhihu.com/p/21292697?refer=intelligentunit)
[DQN 从入门到放弃3 价值函数与Bellman方程 - 知乎](https://zhuanlan.zhihu.com/p/21340755)
[DQN 从入门到放弃4 动态规划与Q-Learning - 知乎](https://zhuanlan.zhihu.com/p/21378532)
[DQN从入门到放弃5 深度解读DQN算法 - 知乎](https://zhuanlan.zhihu.com/p/21421729)
[动手学强化学习](https://hrl.boyuai.com/)
[西湖大学工学院赵世钰在线课程【强化学习的数学原理】](https://www.westlake.edu.cn/academics/School_of_Engineering/NEWS/202212/t20221209_24267.shtml)