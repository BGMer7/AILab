> https://zh.wikipedia.org/zh-cn/%E8%B2%9D%E7%88%BE%E6%9B%BC%E6%96%B9%E7%A8%8B
> “**贝尔曼方程（Bellman Equation）**”也被称作“**动态规划方程（Dynamic Programming Equation）**”，由[理查·贝尔曼](https://zh.wikipedia.org/wiki/%E7%90%86%E6%9F%A5%E5%BE%B7%C2%B7%E8%B2%9D%E7%88%BE%E6%9B%BC "理查德·贝尔曼")（Richard Bellman）发现。贝尔曼方程是[动态规划](https://zh.wikipedia.org/wiki/%E5%8B%95%E6%85%8B%E8%A6%8F%E5%8A%83 "动态规划")（Dynamic Programming）这种数学最佳化方法能够达到[最佳化](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BD%B3%E5%8C%96 "最佳化")的[必要条件](https://zh.wikipedia.org/wiki/%E5%BF%85%E8%A6%81%E6%A2%9D%E4%BB%B6 "必要条件")。此方程将“决策问题在特定时间点的值”以“来自初始选择的报酬 及 由初始选择衍生的决策问题的值”的形式表示。藉这个方式将动态最佳化问题变成较简单的子问题，而这些子问题遵守由贝尔曼所提出的“最佳化原理”。
> 
> 贝尔曼方程最早应用在工程领域的[控制理论](https://zh.wikipedia.org/wiki/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AB%96 "控制理论")及其他应用数学领域，而后成为[经济学](https://zh.wikipedia.org/wiki/%E7%B6%93%E6%BF%9F%E5%AD%B8 "经济学")上的重要工具。
> 几乎所有可以用[最佳控制理论](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E6%8E%A7%E5%88%B6 "最优控制")（Optimal Control Theory）解决的问题也可以透过分析合适的贝尔曼方程得到解决。然而，“贝尔曼方程”通常指离散时间（discrete-time）最佳化问题的动态规划方程。处理连续时间（continuous-time）最佳化问题上，也有类似的偏微分方程，称作[汉弥尔顿-雅各比-贝尔曼方程](https://zh.wikipedia.org/wiki/%E6%BC%A2%E5%BD%8C%E7%88%BE%E9%A0%93-%E9%9B%85%E5%90%84%E6%AF%94-%E8%B2%9D%E7%88%BE%E6%9B%BC%E6%96%B9%E7%A8%8B "汉弥尔顿-雅各比-贝尔曼方程")（Hamilton–Jacobi–Bellman Equation, HJB Equation）。

**Bellman 方程**是强化学习和动态规划中的核心概念，用于描述某一状态下的价值与其他状态的价值之间的递归关系。它提供了一种通过当前状态的回报和未来状态的价值来计算当前状态的价值的方法。Bellman 方程在强化学习中用于计算最优策略，评估策略，甚至求解最优值函数。

### 1. Bellman 方程的基本概念

在强化学习中，Bellman 方程可以帮助我们通过递归的方式计算状态或状态-动作对的价值。

最简单的 Bellman 方程如果不考虑多种情况出现的概率的话，就是这样的定义式：

$$
V(s)=\mathbb{E}[R_{t+1}+\gamma V(S_{t+1}|S_t=s)]
$$

换句话说，贝尔曼方程只往前考虑一步。在某个状态下，我做了一个行动，得到了立即回报。我就可以将这个立即回报加上未来后续状态的价值函数做为我的总体回报。也就是上式中的  $R_{t+1}+\gamma V(S_{t+1})$。对这个总体回报求期望，就得出了状态  $s$ 的价值函数。

根据是否考虑动作，Bellman 方程有不同的形式：

- **状态价值函数（$V(s)$）的 Bellman 方程**
- **动作价值函数（$Q(s, a)$）的 Bellman 方程**



### 2. 状态价值函数的 Bellman 方程

**状态价值函数 $V(s)$** 描述了在状态 $s$ 下，智能体遵循某一策略 $\pi$ 后能够获得的期望回报。Bellman 方程通过当前状态的即时回报和后续状态的价值来定义状态的价值。

对于给定策略 $\pi$，状态价值函数的 Bellman 方程为：

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot | s)} \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s') \right]

$$

其中：

- $V^\pi(s)$：表示在状态 $s$ 下遵循策略 $\pi$ 所能获得的期望回报。
- $R(s, a)$：在状态 $s$ 下执行动作 $a$ 的即时奖励。
- $\gamma$：折扣因子，控制未来奖励的影响。
- $s'$：从状态 $s$ 执行动作 $a$ 后转移到的新状态。
- $P(s'|s, a)$：从状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率。

Bellman 方程通过期望的形式将当前状态的价值与其后续状态的价值关联起来。

### 3. 动作价值函数的 Bellman 方程

**动作价值函数 $Q(s, a)$** 描述了在状态 $s$ 下采取动作 $a$ 后，智能体从该状态和动作对开始，遵循某一策略 $\pi$ 后能获得的期望回报。

对于给定策略 $\pi$，动作价值函数的 Bellman 方程为：

$$
Q^\pi(s, a) = \mathbb{E}_{s' \sim P(\cdot | s, a)} \left[ R(s, a) + \gamma \sum_{a' \sim \pi(\cdot | s')} Q^\pi(s', a') \right]
$$

其中：

- $Q^\pi(s, a)$：表示在状态 $s$ 下采取动作 $a$ 后，遵循策略 $\pi$ 所能获得的期望回报。
- $R(s, a)$：在状态 $s$ 下执行动作 $a$ 的即时奖励。
- $\gamma$：折扣因子。
- $s'$：从状态 $s$ 执行动作 $a$ 后转移到的下一状态。
- $P(\cdot | s, a)$：表示状态转移概率。
- $\pi(\cdot | s')$：表示策略 $\pi$ 在状态 $s'$ 下的动作选择概率。



以下是状态价值函数和动作价值函数Bellman方程的完整推导过程：

**状态价值函数 $V(s)$ 的Bellman方程推导**

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[ G_t \mid S_t = s \right], \quad G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

> - $R_t$：表示在时间步 t 的**即时奖励**（immediate reward），是智能体在状态 St​ 执行动作 At​ 后，环境反馈的单步奖励。
> 
> - $G_t$：表示从时间步 t 开始的**累计回报**（cumulative return），是未来所有奖励的折扣总和。

**步骤1**：将回报 $G_t$ 分解为即时奖励和未来折扣回报：

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right]
$$

**步骤2**：展开期望，引入策略 $\pi(a|s)$ 和状态转移概率 $P(s'|s,a)$：

$$
V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma \mathbb{E}_{\pi}\left[ G_{t+1} \mid S_{t+1}=s' \right] \right]
$$

**步骤3**：将后续状态的期望替换为 $V^{\pi}(s')$：

$$
V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma V^{\pi}(s') \right]
$$

**最终方程**：

$$
V^{\pi}(s) = \sum_{a} \pi(a|s) \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi}(s') \right]
$$

---

### **2. 动作价值函数 $Q^{\pi}(s,a)$ 的Bellman方程推导**

**定义**：

$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[ G_t \mid S_t = s, A_t = a \right]
$$

**步骤1**：分解 $G_t$ 为即时奖励和未来折扣回报：

$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right]
$$

**步骤2**：展开期望，仅需考虑状态转移概率 $P(s'|s,a)$（动作已固定为 $a$）：

$$
Q^{\pi}(s,a) = \sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma \mathbb{E}_{\pi}\left[ G_{t+1} \mid S_{t+1}=s' \right] \right]
$$

**步骤3**：将后续状态的期望展开为策略 $\pi(a'|s')$ 和动作价值函数 $Q^{\pi}(s',a')$：

$$
Q^{\pi}(s,a) = \sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s',a') \right]
$$

**最终方程**：

$$
Q^{\pi}(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s') Q^{\pi}(s',a')
$$



### **推导逻辑总结**

1. **分解回报**：将 $G_t = R_{t+1} + \gamma G_{t+1}$ 代入定义。
2. **展开期望**：
   - 对状态价值函数 $V^{\pi}(s)$，需遍历所有动作 $a$（由策略 $\pi(a|s)$ 加权）和所有转移状态 $s'$（由 $P(s'|s,a)$ 加权）。
   - 对动作价值函数 $Q^{\pi}(s,a)$，动作已固定为 $a$，仅需遍历转移状态 $s'$。
3. **递归替换**：将后续状态的期望 $\mathbb{E}_{\pi}[G_{t+1}]$ 替换为 $V^{\pi}(s')$ 或 $\sum_{a'} \pi(a'|s') Q^{\pi}(s',a')$。

这些步骤的严格数学表达均可直接通过上述Latex代码复现。







### 4. 最优价值函数的 Bellman 方程

最优价值函数 $V^*(s)$ 是对于任何策略 $\pi$，能最大化期望回报的价值函数。它与 **最优动作价值函数 $Q^*(s, a)$** 相关，最优策略的目标是选择一个能最大化这些价值的动作。

#### 4.1 最优状态价值函数的 Bellman 方程

最优状态价值函数的 Bellman 方程为：

$$
V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right]
$$

即最优的状态价值函数 $V^*(s)$ 是在所有可能的动作中，选择能够获得最大期望回报的动作 $a$ 后，当前状态的价值。

#### 4.2 最优动作价值函数的 Bellman 方程

最优动作价值函数的 Bellman 方程为：

$$
Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q^*(s', a')
$$

这表明最优的动作价值函数 $Q^*(s, a)$ 是通过即时回报加上未来状态中所有可能动作的最优价值来递归计算的。

### 5. Bellman 方程的应用

**Bellman 方程**在强化学习中的主要应用包括：

#### 5.1 动态规划

动态规划方法（如 **值迭代** 和 **策略迭代**）基于 Bellman 方程来计算最优价值函数。值迭代算法通过不断更新价值函数来逼近最优解；而策略迭代通过交替进行策略评估和策略改进，最终收敛到最优策略。

#### 5.2 Q-learning 和 SARSA

在 **Q-learning** 和 **SARSA** 等强化学习算法中，Bellman 方程用于更新动作价值函数 $Q(s, a)$。Q-learning 是一种无模型的强化学习方法，它通过经验不断更新 $Q$ 值，最终收敛到最优动作价值函数 $Q^*(s, a)$。

#### 5.3 时序差分学习（TD学习）

时序差分学习（如 **TD(0)** 和 **TD(λ)**）结合了蒙特卡洛方法和动态规划的优点，也基于 Bellman 方程进行价值函数的更新。这些方法通过对当前估计值和实际回报之间的差异进行学习，从而实现对价值函数的逼近。

### 6. Bellman 方程的意义

Bellman 方程提供了一种自洽的递归方式来描述当前状态与未来状态之间的关系。它的意义在于：

- **递归性：** Bellman 方程通过当前状态的奖励和未来状态的价值来定义状态的价值，体现了强化学习中的“远见”。
- **最优性：** Bellman 方程用于推导最优策略，计算最优价值函数，并为许多强化学习算法提供理论基础。
- **解决实际问题：** 通过将问题转化为动态规划或值迭代的形式，Bellman 方程为求解复杂问题提供了计算方法。

### 7. 总结

**Bellman 方程**是强化学习中的核心工具，通过递归的方式将当前状态的价值与未来状态的价值联系起来。它在求解最优价值函数和最优策略方面具有重要作用，广泛应用于各种强化学习算法（如动态规划、Q-learning、SARSA等）。通过理解和应用 Bellman 方程，智能体能够有效地学习如何在复杂的环境中做出最优决策。

> 表达式 $a^*(s) = \arg\max_a Q^*(s, a)$ 的意思是：
> 
> 在状态 $s$ 下，最优策略 $a^*(s)$ 是使得动作价值函数 $Q^*(s, a)$ 最大化的动作。换句话说，智能体在状态 $s$ 下应该选择那个能够带来最大期望回报的动作。
> 
> - $Q^*(s, a)$：表示在状态 $s$ 下执行动作 $a$ 后，根据最优策略所能获得的期望回报。
> - **$a^*(s)$**：表示在状态 $s$ 下的最优动作，选择这个动作将使得智能体从状态 $s$ 开始能够获得最大的长期回报。
> 
> 通过该公式，最优策略 $a^*(s)$ 可以通过寻找在给定状态 $s$ 下，所有动作 $a$ 中使得 $Q^*(s, a)$ 最大的那个动作来得到。
> **举例：**
> 假设在某个状态 ss 下，有三个可能的动作：$a_1, a_2, a_3$，并且其对应的 $Q^*(s, a)$ 值分别为：
> 
> - $Q^*(s, a_1)$ = 10
> - $Q^*(s, a_2)$ = 15
> - $Q^*(s, a_3)$ = 5
> 
> 那么，最优动作 $a^*(s)$ 是选择 $a_2$，因为它的 $Q^*(s, a_2)$ = 15 是最大的。



### **$V(s)$推导的步骤2的核心逻辑**

**目标**：将状态价值函数 $V^{\pi}(s) = \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right]$ 的期望展开，显式引入策略 $\pi(a|s)$ 和状态转移概率 $P(s'|s,a)$。

**关键思想**：

1. **策略 $\pi(a|s)$**：在状态 $s$ 下，智能体选择动作 $a$ 的概率。
2. **状态转移概率 $P(s'|s,a)$**：在状态 $s$ 执行动作 $a$ 后，环境转移到状态 $s'$ 的概率。
3. **联合概率**：在状态 $s$ 时，选择动作 $a$ 并转移到状态 $s'$ 的联合概率为 $\pi(a|s) P(s'|s,a)$。

---

### **逐步推导**

**原始公式**（从步骤1开始）：

$$
V^{\pi}(s) = \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right]
$$



**步骤2的展开过程**：

1. **分解期望**：将总期望分解为对动作 $a$ 和下一状态 $s'$ 的遍历。
   
   $$
   V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a, S_{t+1} = s' \right]
   $$
   
   - 外层求和 $\sum_{a}$：遍历所有可能的动作 $a$，权重为策略 $\pi(a|s)$。
   - 内层求和 $\sum_{s'}$：遍历所有可能的下一状态 $s'$，权重为转移概率 $P(s'|s,a)$。

2. **分离即时奖励和未来回报**：
   
   $$
   \mathbb{E}_{\pi}\left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a, S_{t+1} = s' \right] = R(s,a) + \gamma \mathbb{E}_{\pi}\left[ G_{t+1} \mid S_{t+1} = s' \right]
   $$
   
   - **$R(s,a)$**：在状态 $s$ 执行动作 $a$ 后获得的**期望即时奖励**（已与环境转移概率 $P(s'|s,a)$ 无关）。
   - **$\gamma \mathbb{E}_{\pi}\left[ G_{t+1} \mid S_{t+1} = s' \right]$**：转移到状态 $s'$ 后，未来回报的期望值（即 $V^{\pi}(s')$）。

3. **合并结果**：
   
   $$
   V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma V^{\pi}(s') \right]
   $$

---

### **公式符号的详细说明**

1. **$\pi(a|s)$**：
   
   - **含义**：在状态 $s$ 下选择动作 $a$ 的概率。
   - **作用**：权重所有可能的动作，反映策略对动作的选择偏好。

2. **$P(s'|s,a)$**：
   
   - **含义**：在状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率。
   - **作用**：权重所有可能的下一状态，反映环境动态的不确定性。

3. **$R(s,a)$**：
   
   - **含义**：在状态 $s$ 执行动作 $a$ 的**期望即时奖励**，通常定义为：
     
     $$
     R(s,a) = \mathbb{E}\left[ R_{t+1} \mid S_t = s, A_t = a \right] = \sum_{s'} P(s'|s,a) R(s,a,s')
     $$
     
     其中 $R(s,a,s')$ 是从 $s$ 执行 $a$ 转移到 $s'$ 的具体奖励值。

4. **$\gamma V^{\pi}(s')$**：
   
   - **含义**：下一状态 $s'$ 的状态价值函数的折扣值。
   - **作用**：将未来回报递归地关联到当前状态的价值。

---

### **关键理解**

1. **期望的展开方式**：
   
   - 总期望 $\mathbb{E}_{\pi}$ 被分解为两层求和：
     1. **动作层**：遍历所有动作 $a$，权重为策略 $\pi(a|s)$。
     2. **状态转移层**：遍历所有下一状态 $s'$，权重为转移概率 $P(s'|s,a)$。

2. **条件期望的独立性**：
   
   - 在给定当前状态 $s$、动作 $a$ 和下一状态 $s'$ 后，未来回报 $G_{t+1}$ 仅依赖于 $s'$，因此可以简化为 $V^{\pi}(s')$。

3. **公式的物理意义**：
   
   - 状态价值 $V^{\pi}(s)$ 是所有可能动作和状态转移路径的加权平均回报：
     
     $$
     V^{\pi}(s) = \sum_{a} \pi(a|s) \underbrace{\sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma V^{\pi}(s') \right]}_{\text{动作 $a$ 的期望回报}}
     $$

---

### **示例辅助理解**

假设状态 $s$ 下有两个动作 $a_1$ 和 $a_2$，且策略为：

$$
\pi(a_1|s) = 0.6, \quad \pi(a_2|s) = 0.4
$$


每个动作的转移概率和奖励如下：

- **动作 $a_1$**：
  - 转移到 $s'=s_1$ 的概率 $P(s_1|s,a_1)=0.7$，奖励 $R(s,a_1)=5$。
  - 转移到 $s'=s_2$ 的概率 $P(s_2|s,a_1)=0.3$，奖励 $R(s,a_1)=2$。
- **动作 $a_2$**：
  - 转移到 $s'=s_3$ 的概率 $P(s_3|s,a_2)=1.0$，奖励 $R(s,a_2)=3$。

则状态价值 $V^{\pi}(s)$ 的计算为：

$$
\begin{aligned}
V^{\pi}(s) &= 0.6 \Big[ 0.7 \big(5 + \gamma V^{\pi}(s_1)\big) + 0.3 \big(2 + \gamma V^{\pi}(s_2)\big) \Big] \\
&\quad + 0.4 \Big[ 1.0 \big(3 + \gamma V^{\pi}(s_3)\big) \Big].
\end{aligned}
$$

---

### **总结**

步骤2通过显式引入策略 $\pi(a|s)$ 和转移概率 $P(s'|s,a)$，将期望分解为对动作和状态转移的遍历求和。这一过程清晰地揭示了状态价值的计算依赖于：

1. **策略对动作的选择**（权重 $\pi(a|s)$）。
2. **环境动态对状态转移的影响**（权重 $P(s'|s,a)$）。
3. **即时奖励与未来回报的平衡**（通过 $\gamma V^{\pi}(s')$ 实现递归关联）。

这种展开方式是Bellman方程的核心，也是动态规划（DP）和时序差分（TD）算法的基础。