> https://zh.wikipedia.org/zh-cn/%E8%B2%9D%E7%88%BE%E6%9B%BC%E6%96%B9%E7%A8%8B
> “**贝尔曼方程（Bellman Equation）**”也被称作“**动态规划方程（Dynamic Programming Equation）**”，由[理查·贝尔曼](https://zh.wikipedia.org/wiki/%E7%90%86%E6%9F%A5%E5%BE%B7%C2%B7%E8%B2%9D%E7%88%BE%E6%9B%BC "理查德·贝尔曼")（Richard Bellman）发现。贝尔曼方程是[动态规划](https://zh.wikipedia.org/wiki/%E5%8B%95%E6%85%8B%E8%A6%8F%E5%8A%83 "动态规划")（Dynamic Programming）这种数学最佳化方法能够达到[最佳化](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BD%B3%E5%8C%96 "最佳化")的[必要条件](https://zh.wikipedia.org/wiki/%E5%BF%85%E8%A6%81%E6%A2%9D%E4%BB%B6 "必要条件")。此方程将“决策问题在特定时间点的值”以“来自初始选择的报酬 及 由初始选择衍生的决策问题的值”的形式表示。藉这个方式将动态最佳化问题变成较简单的子问题，而这些子问题遵守由贝尔曼所提出的“最佳化原理”。
> 
> 贝尔曼方程最早应用在工程领域的[控制理论](https://zh.wikipedia.org/wiki/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AB%96 "控制理论")及其他应用数学领域，而后成为[经济学](https://zh.wikipedia.org/wiki/%E7%B6%93%E6%BF%9F%E5%AD%B8 "经济学")上的重要工具。
> 几乎所有可以用[最佳控制理论](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E6%8E%A7%E5%88%B6 "最优控制")（Optimal Control Theory）解决的问题也可以透过分析合适的贝尔曼方程得到解决。然而，“贝尔曼方程”通常指离散时间（discrete-time）最佳化问题的动态规划方程。处理连续时间（continuous-time）最佳化问题上，也有类似的偏微分方程，称作[汉弥尔顿-雅各比-贝尔曼方程](https://zh.wikipedia.org/wiki/%E6%BC%A2%E5%BD%8C%E7%88%BE%E9%A0%93-%E9%9B%85%E5%90%84%E6%AF%94-%E8%B2%9D%E7%88%BE%E6%9B%BC%E6%96%B9%E7%A8%8B "汉弥尔顿-雅各比-贝尔曼方程")（Hamilton–Jacobi–Bellman Equation, HJB Equation）。



**Bellman 方程**是强化学习和动态规划中的核心概念，用于描述某一状态下的价值与其他状态的价值之间的递归关系。它提供了一种通过当前状态的回报和未来状态的价值来计算当前状态的价值的方法。Bellman 方程在强化学习中用于计算最优策略，评估策略，甚至求解最优值函数。

### 1. Bellman 方程的基本概念

在强化学习中，Bellman 方程可以帮助我们通过递归的方式计算状态或状态-动作对的价值。

最简单的 Bellman 方程如果不考虑多种情况出现的概率的话，就是这样的定义式：

$$
V(s)=\mathbb{E}[R_{t+1}+\gamma V(S_{t+1}|S_t=s)]
$$

换句话说，贝尔曼方程只往前考虑一步。在某个状态下，我做了一个行动，得到了立即回报。我就可以将这个立即回报加上未来后续状态的价值函数做为我的总体回报。也就是上式中的  $R_{t+1}+\gamma V(S_{t+1})$。对这个总体回报求期望，就得出了状态  $s$ 的价值函数。



根据是否考虑动作，Bellman 方程有不同的形式：

- **状态价值函数（$V(s)$）的 Bellman 方程**
- **动作价值函数（$Q(s, a)$）的 Bellman 方程**

### 2. 状态价值函数的 Bellman 方程

**状态价值函数 $V(s)$** 描述了在状态 $s$ 下，智能体遵循某一策略 $\pi$ 后能够获得的期望回报。Bellman 方程通过当前状态的即时回报和后续状态的价值来定义状态的价值。

对于给定策略 $\pi$，状态价值函数的 Bellman 方程为：

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot | s)} \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s') \right]

$$

其中：

- $V^\pi(s)$：表示在状态 $s$ 下遵循策略 $\pi$ 所能获得的期望回报。
- $R(s, a)$：在状态 $s$ 下执行动作 $a$ 的即时奖励。
- $\gamma$：折扣因子，控制未来奖励的影响。
- $s'$：从状态 $s$ 执行动作 $a$ 后转移到的新状态。
- $P(s'|s, a)$：从状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率。

Bellman 方程通过期望的形式将当前状态的价值与其后续状态的价值关联起来。

### 3. 动作价值函数的 Bellman 方程

**动作价值函数 $Q(s, a)$** 描述了在状态 $s$ 下采取动作 $a$ 后，智能体从该状态和动作对开始，遵循某一策略 $\pi$ 后能获得的期望回报。

对于给定策略 $\pi$，动作价值函数的 Bellman 方程为：

$$
Q^\pi(s, a) = \mathbb{E}_{s' \sim P(\cdot | s, a)} \left[ R(s, a) + \gamma \sum_{a' \sim \pi(\cdot | s')} Q^\pi(s', a') \right]
$$

其中：

- $Q^\pi(s, a)$：表示在状态 $s$ 下采取动作 $a$ 后，遵循策略 $\pi$ 所能获得的期望回报。
- $R(s, a)$：在状态 $s$ 下执行动作 $a$ 的即时奖励。
- $\gamma$：折扣因子。
- $s'$：从状态 $s$ 执行动作 $a$ 后转移到的下一状态。
- $P(\cdot | s, a)$：表示状态转移概率。
- $\pi(\cdot | s')$：表示策略 $\pi$ 在状态 $s'$ 下的动作选择概率。

### 4. 最优价值函数的 Bellman 方程

最优价值函数 $V^*(s)$ 是对于任何策略 $\pi$，能最大化期望回报的价值函数。它与 **最优动作价值函数 $Q^*(s, a)$** 相关，最优策略的目标是选择一个能最大化这些价值的动作。

#### 4.1 最优状态价值函数的 Bellman 方程

最优状态价值函数的 Bellman 方程为：

$$
V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right]
$$

即最优的状态价值函数 $V^*(s)$ 是在所有可能的动作中，选择能够获得最大期望回报的动作 $a$ 后，当前状态的价值。

#### 4.2 最优动作价值函数的 Bellman 方程

最优动作价值函数的 Bellman 方程为：

$$
Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q^*(s', a')
$$

这表明最优的动作价值函数 $Q^*(s, a)$ 是通过即时回报加上未来状态中所有可能动作的最优价值来递归计算的。

### 5. Bellman 方程的应用

**Bellman 方程**在强化学习中的主要应用包括：

#### 5.1 动态规划

动态规划方法（如 **值迭代** 和 **策略迭代**）基于 Bellman 方程来计算最优价值函数。值迭代算法通过不断更新价值函数来逼近最优解；而策略迭代通过交替进行策略评估和策略改进，最终收敛到最优策略。

#### 5.2 Q-learning 和 SARSA

在 **Q-learning** 和 **SARSA** 等强化学习算法中，Bellman 方程用于更新动作价值函数 $Q(s, a)$。Q-learning 是一种无模型的强化学习方法，它通过经验不断更新 $Q$ 值，最终收敛到最优动作价值函数 $Q^*(s, a)$。

#### 5.3 时序差分学习（TD学习）

时序差分学习（如 **TD(0)** 和 **TD(λ)**）结合了蒙特卡洛方法和动态规划的优点，也基于 Bellman 方程进行价值函数的更新。这些方法通过对当前估计值和实际回报之间的差异进行学习，从而实现对价值函数的逼近。

### 6. Bellman 方程的意义

Bellman 方程提供了一种自洽的递归方式来描述当前状态与未来状态之间的关系。它的意义在于：

- **递归性：** Bellman 方程通过当前状态的奖励和未来状态的价值来定义状态的价值，体现了强化学习中的“远见”。
- **最优性：** Bellman 方程用于推导最优策略，计算最优价值函数，并为许多强化学习算法提供理论基础。
- **解决实际问题：** 通过将问题转化为动态规划或值迭代的形式，Bellman 方程为求解复杂问题提供了计算方法。

### 7. 总结

**Bellman 方程**是强化学习中的核心工具，通过递归的方式将当前状态的价值与未来状态的价值联系起来。它在求解最优价值函数和最优策略方面具有重要作用，广泛应用于各种强化学习算法（如动态规划、Q-learning、SARSA等）。通过理解和应用 Bellman 方程，智能体能够有效地学习如何在复杂的环境中做出最优决策。
