[[Stochastic Process]]

MDP基于这样一种假设：

> 未来只取决于当前  

什么意思呢？

就是如果我们站在上帝视角下看，我们知道这个世界的每个物体的状态，那么未来的变化只跟当前的状态相关，和过去没有关系。

用数学的话来描述就是：
$$
P(s_{t+1}|s_t) = P(s_{t+1}|s_t, s_{t-1},...,s_1,s_{0})
$$
简单的说就是下一个状态仅取决于当前的状态和当前的动作。注意**这里的状态是完全可观察的全部的环境状态（也就是上帝视角）**。

当然，对于一些游戏比如围棋在游戏的世界中就是完全可观察的。上面的公式可以用概率论的方法来证明。这里，我们说一个更通俗易懂的方式：举个栗子，在理想环境中有一个球，以某一个速度v斜向上45度抛出，受重力G影响，求这个球的运行轨迹？初中的物理题。显然，我们知道了球的初速度，受力情况，我们可以完全计算出球在每一个时间点上位置和速度。**而且我们只要知道某一个时间点上球的速度和受力情况，下一个时间点的速度和位置就可以求出。这就是一个MDP过程。**

增强学习的问题都可以模型化为MDP的问题。

一个基本的MDP可以用（S,A,P）来表示，S表示状态，A表示动作，P表示[状态转移概率](https://zhida.zhihu.com/search?content_id=700794&content_type=Article&match_order=1&q=状态转移概率&zhida_source=entity)，也就是根据当前的状态st和at转移到st+1的概率。如果我们知道了转移概率P，也就是称为我们获得了**模型Model**，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为[Model-based](https://zhida.zhihu.com/search?content_id=700794&content_type=Article&match_order=1&q=Model-based&zhida_source=entity)的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有[[Model Free]]的方法来寻找最优的动作。