[[Reinforcement Learning]]

**Model-Free（无模型）** 是强化学习中的一个核心概念，指算法不依赖于环境动态模型（environment dynamics model）即可学习最优策略。具体来说，算法无需事先知道环境的以下信息：
1. **状态转移概率**：$P(s' \mid s, a)$（在状态$s$执行动作$a$后转移到状态$s'$的概率）；
2. **奖励函数**：$R(s, a, s')$（在状态$s$执行动作$a$并转移到$s'$后获得的即时奖励）。

---

### **Model-Free vs. Model-Based**
| **特性**         | **Model-Free**               | **Model-Based**               |
|------------------|------------------------------|--------------------------------|
| 依赖环境模型      | ❌ 不需要                    | ✅ 需要已知或学习的模型        |
| 数据利用方式      | 通过试错直接学习策略或值函数 | 利用模型进行规划（如动态规划） |
| 样本效率          | 通常较低（需大量交互数据）   | 较高（利用模型模拟减少交互）   |
| 典型算法          | Q-learning、SARSA、DQN       | 动态规划（Value Iteration）    |

---

### **为什么需要Model-Free方法？**
1. **现实环境复杂**：多数实际问题（如游戏、机器人控制）的环境动态（状态转移和奖励规则）难以精确建模。
2. **避免模型误差**：Model-Based方法依赖模型准确性，模型错误会导致策略失效。
3. **灵活性高**：直接通过与环境交互学习，适应性强。

---

### **Model-Free的工作原理**
1. **试错学习**：智能体通过与环境交互（执行动作→观察状态和奖励）收集数据。
2. **更新策略或值函数**：基于经验数据（如$s, a, r, s'$）直接优化：
   - **Q-learning**：更新Q值表，公式：  
     $$ Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right] $$
   - **策略梯度（Policy Gradient）**：直接优化策略函数$\pi(a|s)$的参数。

---

### **Model-Free的优缺点**
| **优点**                          | **缺点**                          |
|-----------------------------------|-----------------------------------|
| 无需环境模型，适用性广            | 样本效率低，需大量交互数据        |
| 避免模型不准确带来的偏差          | 训练过程可能不稳定（如Q-learning）|
| 适合高维/连续状态-动作空间（结合深度学习） | 对探索策略敏感（如$\epsilon$-greedy需调参） |

---

### **典型应用场景**
1. **游戏AI**（如AlphaGo、Atari游戏）：环境规则复杂，难以显式建模。
2. **机器人控制**：物理仿真环境动态难以精确描述。
3. **推荐系统**：用户行为模式未知，需通过交互学习。

---

### **一句话总结**
**Model-Free = 无环境模型 + 通过试错直接学习**。它放弃了依赖模型的“规划”，转而通过实践（experience）直接找到最优策略，是解决复杂未知环境问题的核心方法。