# Notation

在概率论中，符号和表示方法是理解和应用概率理论的重要工具。以下是一些常见的概率论符号及其意义：

## Basic Notation

- $P(A)$ 或 $ \Pr(A) $：事件 $A$ 发生的概率。
- $A, B, C, \ldots$：事件。
- $\Omega$ 或 $S$：样本空间，即所有可能结果的集合。
- $A \cup B$：事件 $A$ 或事件 $B$ 发生（并集）。
- $A \cap B$：事件 $A$ 和事件 $B$ 同时发生（交集）。
- $A^c$ 或 $\overline{A}$：事件 $A$ 的补集，即 $A$ 不发生。
- $\emptyset$：空事件，即不发生的事件。

## Random Variable And Distribution

- $X, Y, Z$：随机变量。
- $P(X = x)$：离散随机变量 $X$ 取值为 $x$​ 的概率。
- $L(\theta | \mathbf{x}) = P(\mathbf{x} | \theta)$：似然函数用于表示在给定参数下观测数据的可能性。
- $f_X(x)$ 或 $p_X(x)$：离散随机变量 $X$ 的概率质量函数（PMF）。
- $F_X(x)$：随机变量 $X$ 的累积分布函数（CDF），即 $P(X \leq x)$。
- $f_X(x)$：连续随机变量 $X$ 的概率密度函数（PDF）。

## Conditional Probablity

- $P(A | B)$：在 $B$ 发生的条件下 $A$ 发生的概率。
- 乘法法则：
  $$
  P(A \cap B) = P(A)P(B | A) = P(B)P(A | B)
  $$
- $A \perp B$ 或 $A \indep B$：事件 $A$ 和事件 $B$ 相互独立，即
  $$
  P(A \cap B) = P(A)P(B)
  $$

## Expection And Variance

- $E(X)$ 或 $\mathbb{E}[X]$：随机变量 $X$ 的期望值。
- $\text{Var}(X)$ 或 $\mathbb{V}[X]$：随机变量 $X$ 的方差。
- $\sigma_X$：随机变量 $X$ 的标准差，
  $$
  \sigma_X = \sqrt{\text{Var}(X)}
  $$

## Covariance And Correlation

- $\text{Cov}(X, Y)$：随机变量 $X$ 和 $Y$ 的协方差。
- $\rho_{X,Y}$ 或 $\text{Corr}(X, Y)$：随机变量 $X$ 和 $Y$ 的相关系数。

## Distribution

- $\mathcal{N}(\mu, \sigma^2)$：均值为 $\mu$、方差为 $\sigma^2$ 的正态分布。
- $\text{Bernoulli}(p)$：参数为 $p$ 的伯努利分布。
- $\text{Binomial}(n, p)$：参数为 $n$ 和 $p$ 的二项分布。
- $\text{Poisson}(\lambda)$：参数为 $\lambda$ 的泊松分布。
- $\text{Exponential}(\lambda)$：参数为 $\lambda$ 的指数分布。

## Bayes Theorem

- 贝叶斯定理，用于计算条件概率：
  $$
  P(A | B) = \frac{P(B | A) P(A)}{P(B)}
  $$

## 总结

理解这些符号和它们的意义是掌握概率论的基础。它们广泛应用于各种概率论和统计学问题中，包括计算概率、期望值、方差、条件概率、独立性、协方差、相关性以及不同的概率分布。通过熟悉这些符号及其应用，你将能够更有效地处理和分析概率问题。



# Concepts

## Sample Space
样本空间

## Event
事件


## Classical Probability
古典概率：概率是以假设为基础的，即假定随机现象所发生的事件是有限的、互不相容的，而且每个基本事件发生的可能性等。一般来讲，如果在全部可能出现的基本事件范围内构成事件A的基本事件有a个，不构成事件A的有b个，那么事件A出现的概率为
$$
P(A) = \frac{a}{a+b}
$$


## Permutation
排列数：从 m 个不同元素中取出 n (n<=m) 个元素（被取出的元素各不相同），并按照一定的顺序排成一列，叫做从 m 个不同元素中取出 n 个元素的一个排列，记作 A(m, n)。
$$
 A(m, n) = A^n_m = m(m-1)(m-2)...(m-n+1) = \frac{n!}{m-n!}
$$
其中规定 $0!$ = 1

推导：把𝑛个不同的元素任选𝑚个排序，按计数原理**分步进行**：

取第一个：有𝑛种取法；  
取第二个：有(𝑛−1)种取法；  
取第三个：有(𝑛−2)种取法；  
……  
取第𝑚个：有(𝑛−𝑚+1)种取法；

根据分步乘法原理，得出上述公式。


## Combination
组合数：从 m 个不同元素中取出 n (n<=m) 个元素（不放回），叫做从 m 个元素中取出 n 个元素的一个组合，记作 C(m, n)。
$$
 C(m, n) = C^n_m =\frac{A^n_m}{A^n_n} =  A^n_m = m(m-1)(m-2)...(m-n+1) = \frac{n!}{m-n!}
$$
证明：利用排列和组合之间的关系以及排列的公式进行推倒证明。
将部分排列问题分解为两个步骤：
第一步：从m个元素中抽取出n个元素，不排序，即为组合问题$C^n_m$。
第二步：把这n个元素进行全排列，即为$A^n_n$。

得到关系：$A^n_m = C^n_m A^n_n$

[排列组合的一些公式及推导(非常详细易懂) - 樱花赞 - 博客园 (cnblogs.com)](https://www.cnblogs.com/1024th/p/10623541.html)



## Random Variable

### Discrete Random Variable
离散随机变量：取值为有限个或可数无限个的变量。例如，掷一枚骰子的结果就是一个离散随机变量，它的取值范围是{1, 2, 3, 4, 5, 6}。

#### PDF

PDF ( Probability Density Function )

概率密度是用于描述连续随机变量取值的相对可能性的函数。

概率密度函数：设 X 是一个随机变量, 它定义在离散的结果 空间 Ω 上 (Ω 是有限的或至多可数的)。

那么 X 的概率密度函数 (常记作 $f_X$ ) 就 是 X 取某个特定值的概率:
$$
f_X(x)=Prob(ω∈Ω: X(ω)=x)
$$
注意，有些教材用概率质量函数的说法，而非概率密度函数。概率密度函数的值总是大于或等于 0，并且和始终为 1。



#### CDF

累积分布函数：这个函数对于连续型随机变量更加有用，但是对于离散随机变量仍然有用。

鉴于 PDF 是 X 取某个特定值的概率，则累积分布函数表示不超过某个特定值的概率。
$$
f_X(x) = Prob(ω ∈ Ω : X(ω) = x) \\
F_X(x) = Prob(ω ∈ Ω : X(ω) \leq x).
$$
这种写法来源于微积分。在微积分中, 我们常用 *f* 表示函数, 用*F* 表示 *f* 的原函数, 并且 *F* 与曲线下方的面积有关。



### Continuous Random Variable
连续随机变量：取值为连续区间内的变量。例如，一个人的身高可以看作是一个连续随机变量，因为它可以取任意实数值。

通常，随机变量用大写字母（如X, Y, Z）表示，其取值用对应的小写字母（如x, y, z）表示。



#### PDF & CDF

连续型随机变量的 PDF 的三个条件：

1. $f_X(x) \geq 0$，即概率密度函数的取值非负。 
1. $f_X(x)$ 是一个分段连续函数。
2. $\int_{-\infty}^{\infty} f_X(x) \, dx = 1$，即概率密度函数下的总面积为1。

我们简单地叙述了$f_X$ 成为概率密度函数必须要满足的条件。第一个是 $f_X$ 是分段连续的函数，这保证了函数可以求积分 （具体地说，利用微积分基本定理求积

分）。



**PDF 和 CDF 的关系**

1. **由 PDF 求 CDF**：    累积分布函数 $F_X(x)$ 是概率密度函数 $f_X(x)$ 的积分，具体关系为：   $$   F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt   $$ 
2. **由 CDF 求 PDF**：    如果累积分布函数 $F_X(x)$ 是可微的，则其导数就是概率密度函数 $f_X(x)$，即：   $$   f_X(x) = \frac{d}{dx} F_X(x)   $$

不过, 我们必须要小心。记住，概率密度函数必须是分段连续的。虽然累积分布函数是连续的，但不一定是处处可微的。

#### Normalization of a Potential PDF

1. 计算积分

   首先，计算 $g(x)$ 在整个定义域上的积分，即
   $$
   C = \int_{-\infty}^{\infty} g(x) \, dx
   $$
   

2. 标准化

   然后，通过将 $g(x)$ 除以积分常数 $C$，得到标准化后的概率密度函数 $f(x)$，即
   $$
   f(x) = \frac{g(x)}{C}
   $$
   其中 $f(x)$ 满足归一性条件：

      $$   \int_{-\infty}^{\infty} f(x) \, dx = \int_{-\infty}^{\infty} \frac{g(x)}{C} \, dx = \frac{1}{C} \int_{-\infty}^{\infty} g(x) \, dx = \frac{C}{C} = 1   $$





## Distribution
分布指的是描述随机变量所有可能的取值及其对应概率或者概率密度的函数。



## Likelihood
在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。

1. 概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性。 比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的。
2. 而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）。 还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们运用出现的结果来判断这个事情本身的性质（参数），也就是似然。



给定一堆数据，假如我们知道它是从某一种分布中随机取出来的，可是我们并不知道这个分布具体的参数，即“模型已定，参数未知”。例如，我们知道这个分布是正态分布，但是不知道均值和方差；或者是二项分布，但是不知道均值。这时候的问题便是似然了。



**总结**：

1. 概率用于表示固定的参数 $\theta$ 下，某一个事件 $x$ 发生的可能性。
2. 似然用于表示已经观察到事件 $x$ ，而参数 $\theta$ 的可能性。

简单来说，概率是用来描述数据的生成模型，而似然是用来描述参数的合理性。

似然函数是许多统计推断方法的基础，包括但不限于：

- 最大似然估计（MLE）
- 贝叶斯推断
- 似然比检验

 

**举例**：

考虑一个投掷硬币的例子，其中硬币正面出现的概率是 ( p )，反面出现的概率是 ( 1-p )。

若我们观察到了3次正面和2次反面，似然函数可以写作：
$$
L(p|data) = p^3(1-p)^2
$$
这个式子表示了在观察到3次正面和2次反面之后，不同的p值有多合理。






# Operations
## Expectation
期望：


## Variance
方差：


## Covariance
协方差：
