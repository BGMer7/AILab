[[AdaBoost]]
[[XGBoost]]
[[LightGBM]]

Boosting 是个非常强大的学习方法, 它也是一个监督的分类学习方法。它组合许多“弱”分类器来产生一个强大的分类器组。一个弱分类器的性能只是比随机选择好一点，因此它可以被设计的非常简单并且不会有太大的计算花费。将很多弱分类器结合起来组成一个集成的类似于SVM或者神经网络的强分类器。

现在我们知道boosting是组合多个弱学习器形成一个强学习器，那么一个自然而然的问题就是“boosting如何确定弱的规则？”为了发现弱的规则，我们可以应用不同分配下的基础的（机器）学习算法，每个算法都会生成一个弱规则，这是一个迭代的过程，多次迭代后，Boosting算法可以将它们组合成一个强大的决策规则。为了选择正确的分配方式，可以遵循下面几个步骤：

- 步骤1：所有分布下的基础学习器对于每个观测值都应该有相同的权重
- 步骤2：如果第一个基础的学习算法预测错误，则该点在下一次的基础学习算法中有更高的权重
- 步骤3：迭代第2步，直到到达预定的学习器数量或预定的预测精度。

最后，将输出的多个弱学习器组合成一个强的学习器，提高模型的整体预测精度。Boosting总是更加关注被错误分类的弱规则。

Boosting算法的底层可以是任何算法，关于[boosting算法](https://zhida.zhihu.com/search?content_id=146121506&content_type=Article&match_order=1&q=boosting%E7%AE%97%E6%B3%95&zhida_source=entity)，我们需要知道其中最有名的3个算法：

- AdaBoost(Adaptive Boosting)
- GBM(Gradient Boosting Machine)
- XGBoost