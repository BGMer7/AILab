import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# ================= é…ç½® =================
# ğŸ”´ ç¡®è®¤ä½ çš„æ¨¡å‹è·¯å¾„
MODEL_PATH = r"D:\Learning\Notes\AILab\experiments\fine-tuning\LLaMA-Factory\saves\Custom\full\train_2026-02-05-21-21-29"
DATA_FILE = "dataset_test.json"
TEST_SIZE = 50


def load_model():
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        exit()


def parse_json_output(text):
    """å°è¯•ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON"""
    try:
        # 1. ç®€å•æ¸…æ´— Markdown
        text = text.replace("```json", "").replace("```", "").strip()
        # 2. å°è¯•è§£æ
        return json.loads(text)
    except:
        return None


def run_benchmark():
    # 1. è¯»å–æ•°æ®
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    test_data = data[:TEST_SIZE]
    print(f"ğŸ“Š å¼€å§‹æµ‹è¯•å‰ {len(test_data)} æ¡æ•°æ®...")

    tokenizer, model = load_model()

    correct_count = 0
    format_error_count = 0

    # 2. å¾ªç¯æµ‹è¯•
    # æ³¨æ„ï¼šä¸ºäº†é¿å… tqdm è¿›åº¦æ¡å’Œ print æ··åœ¨ä¸€èµ·ï¼Œè¿™é‡Œå¯ä»¥æŠŠ tqdm å»æ‰ï¼Œæˆ–è€…å¿å—ä¸€ä¸‹åˆ·å±
    print("\n" + "=" * 50)
    print("ğŸš€ å¼€å§‹é€æ¡æ¨ç†å±•ç¤º")
    print("=" * 50 + "\n")

    for i, item in enumerate(test_data):
        user_query = item["input"]

        # è·å–æ ‡å‡†ç­”æ¡ˆæ ‡ç­¾ï¼Œç”¨äºå¯¹æ¯”
        try:
            gt_json = json.loads(item["output"])
            gt_label = gt_json.get("need_watchlist_context")
        except:
            gt_label = "æœªçŸ¥"

        # # æ„é€ ä¸€ä¸ªå¼ºåŠ› System Prompt
        # strong_system_prompt = f"""
        # {item['instruction']}

        # ã€ä¸¥æ ¼çº¦æŸã€‘
        # 1. è¿™æ˜¯ä¸€ä¸ªAPIæ¥å£ï¼Œä¸è¦è¿›è¡Œä»»ä½•æ€è€ƒæˆ–å¯¹è¯ã€‚
        # 2. å¿…é¡»ä¸”åªèƒ½è¾“å‡ºä¸€ä¸ªåˆæ³•çš„ JSON å­—ç¬¦ä¸²ã€‚
        # 3. æ ¼å¼å¿…é¡»æ˜¯ï¼š{{"need_watchlist_context": true}} æˆ– {{"need_watchlist_context": false}}
        # 4. ç¦æ­¢è¾“å‡º <think> æ ‡ç­¾ï¼Œç¦æ­¢è¾“å‡º markdownï¼Œç¦æ­¢è¾“å‡ºä»»ä½•è§£é‡Šã€‚
        # """

        # messages = [
        #     {"role": "system", "content": strong_system_prompt},
        #     {"role": "user", "content": user_query},
        # ]

        messages = [
            {"role": "system", "content": item["instruction"]},
            {"role": "user", "content": user_query},
        ]

        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True, enable_thinking=False
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs, max_new_tokens=128, do_sample=False
            )

        generated_ids = [
            output_ids[len(input_ids) :]
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[
            0
        ]

        # ==========================================
        # ğŸŸ¢ æ–°å¢ï¼šå¼ºåˆ¶æ‰“å°æ¯ä¸€æ¡çš„è¯¦ç»†è¾“å‡º
        # ==========================================
        print(f"[{i+1}/{TEST_SIZE}] Query: {user_query}")
        print(f"ğŸ‘‰ Model Output (Raw): \n{response_text}")
        print("-" * 30)
        # ==========================================

        # 3. ç»“æœåˆ¤å®š
        pred_json = parse_json_output(response_text)

        if pred_json is None:
            format_error_count += 1
            print(f"âŒ æ ¼å¼åˆ¤å®š: å¤±è´¥ (Not JSON)")
        else:
            pred_label = pred_json.get("need_watchlist_context")
            if pred_label == gt_label:
                correct_count += 1
                print(f"âœ… ç»“æœåˆ¤å®š: æ­£ç¡®")
            else:
                print(f"âŒ ç»“æœåˆ¤å®š: é”™è¯¯ (é¢„æœŸ {gt_label} vs å®é™… {pred_label})")

        print("\n")  # ç©ºä¸€è¡Œï¼Œæ–¹ä¾¿é˜…è¯»

    # 4. è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    accuracy = (correct_count / len(test_data)) * 100
    print("=" * 30)
    print(f"ğŸ“‰ æµ‹è¯•æ€»ç»“")
    print(f"âœ… æ­£ç¡®: {correct_count}")
    print(f"âš ï¸ æ ¼å¼é”™è¯¯: {format_error_count}")
    print(f"ğŸ† å‡†ç¡®ç‡: {accuracy:.2f}%")
    print("=" * 30)


if __name__ == "__main__":
    run_benchmark()
