import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# ================= é…ç½® =================
# ğŸ”´ æŒ‡å‘ä½ åˆšæ‰ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
MODEL_PATH = "./models/qwen/Qwen2___5-3B-Instruct"
DATA_FILE = "dataset_test.json"
TEST_SIZE = 50  # åªæµ‹è¯•å‰50æ¡ï¼ŒèŠ‚çœæ—¶é—´ã€‚å¦‚æœæ•°æ®ä¸å¤Ÿ50æ¡åˆ™å…¨æµ‹ã€‚


def load_model():
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        # device_map="auto" ä¼šè‡ªåŠ¨è°ƒç”¨æ˜¾å¡
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            device_map="auto",
            torch_dtype=torch.float16,  # èŠ‚çœæ˜¾å­˜
            trust_remote_code=True,
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        exit()


def parse_json_output(text):
    """å°è¯•ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON"""
    try:
        # å»æ‰ Markdown æ ‡è®°
        text = text.replace("```json", "").replace("```", "").strip()
        # å°è¯•ç›´æ¥è§£æ
        return json.loads(text)
    except:
        return None


def run_benchmark():
    # 1. è¯»å–æ•°æ®
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    # æˆªå–æµ‹è¯•é›†
    test_data = data[:TEST_SIZE]
    print(f"ğŸ“Š å¼€å§‹æµ‹è¯•å‰ {len(test_data)} æ¡æ•°æ®...")

    tokenizer, model = load_model()

    correct_count = 0
    format_error_count = 0

    # 2. å¾ªç¯æµ‹è¯•
    for i, item in enumerate(tqdm(test_data)):
        user_query = item["input"]

        # è§£ææ ‡å‡†ç­”æ¡ˆ (Ground Truth)
        try:
            ground_truth = json.loads(item["output"])
            gt_label = ground_truth.get("need_watchlist_context")
        except:
            print(f"âš ï¸ ç¬¬ {i+1} æ¡æ•°æ®æ ‡å‡†ç­”æ¡ˆæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡ã€‚")
            continue

        # æ„é€  Prompt (å¿…é¡»å’Œå¾®è°ƒæ—¶çš„ Instruction ä¿æŒä¸€è‡´)
        messages = [
            {"role": "system", "content": item["instruction"]},
            {"role": "user", "content": user_query},
        ]

        # å¼ºåŠ› Prompt
        strong_system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ç”¨æˆ·çš„ Query æ˜¯å¦éœ€è¦æŸ¥è¯¢ã€è‡ªé€‰è‚¡æ•°æ®ã€‘ã€‚
        è¯·åŠ¡å¿…åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šã€‚
        æ ¼å¼è¦æ±‚ï¼š{"need_watchlist_context": true/false, "reason": "..."}
        """

        messages = [
            {"role": "system", "content": strong_system_prompt},
            {"role": "user", "content": user_query},
        ]

        # è½¬æ¢æˆæ¨¡å‹è¾“å…¥
        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        # æ¨ç†
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=128,
                do_sample=False,  # <--- ä¿®æ”¹è¿™é‡Œï¼šå…³é—­é‡‡æ ·ï¼Œå¯ç”¨è´ªå©ªè§£ç 
            )

        # è§£ç è¾“å‡º
        generated_ids = [
            output_ids[len(input_ids) :]
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[
            0
        ]

        # 3. ç»“æœåˆ¤å®š
        pred_json = parse_json_output(response_text)

        if pred_json is None:
            # æ ¼å¼é”™è¯¯ï¼ˆæ²¡è¾“å‡ºJSONï¼‰
            format_error_count += 1
            print(f"\nâŒ [æ ¼å¼é”™è¯¯] Query: {user_query}")
            print(f"   Model Output: {response_text}")
        else:
            pred_label = pred_json.get("need_watchlist_context")

            if pred_label == gt_label:
                correct_count += 1
            else:
                # æ‰“å°é”™è¯¯æ¡ˆä¾‹ï¼Œæ–¹ä¾¿ä½ åˆ†æ
                print(f"\nâŒ [é¢„æµ‹é”™è¯¯] Query: {user_query}")
                print(f"   é¢„æœŸ: {gt_label} | å®é™…: {pred_label}")
                print(f"   ç†ç”±: {pred_json.get('reason', 'æ— ç†ç”±')}")

    # 4. è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    accuracy = (correct_count / len(test_data)) * 100
    print("\n" + "=" * 30)
    print(f"ğŸ“‰ åŸºçº¿æµ‹è¯•æŠ¥å‘Š (Baseline Report)")
    print(f"=" * 30)
    print(f"æµ‹è¯•æ€»æ•°: {len(test_data)}")
    print(f"âœ… æ­£ç¡®æ•°é‡: {correct_count}")
    print(f"âš ï¸ æ ¼å¼é”™è¯¯: {format_error_count} (æ¨¡å‹æ²¡æŒ‰JSONè¾“å‡º)")
    print(f"ğŸ† å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"=" * 30)


if __name__ == "__main__":
    run_benchmark()
