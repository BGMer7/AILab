{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d07da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 transformers 库 (如果尚未安装)\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227e06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择一个预训练模型名称\n",
    "model_name = \"bert-base-uncased\" # 你可以尝试其他模型，例如 \"gpt2\", \"roberta-base\"\n",
    "\n",
    "# 加载对应的 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer for {model_name}: {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e243ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的 Tokenization\n",
    "text = \"Hello, how are you today?\"\n",
    "\n",
    "# 使用 tokenizer 对文本进行分词\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码 (Encoding) 文本为模型输入\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\") # return_tensors='pt' 返回 PyTorch tensors\n",
    "print(\"Encoded Input:\", encoded_input)\n",
    "# 你会注意到 encoded_input 包含 input_ids (tokens 的数字表示) 和 attention_mask (指示哪些 tokens 应该被关注)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea261d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码 (Decoding) Token IDs 回文本\n",
    "# 获取 input_ids\n",
    "input_ids = encoded_input[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码 token IDs\n",
    "decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True) # skip_special_tokens=True 忽略特殊 token\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理多个句子\n",
    "sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here is another one.\"\n",
    "]\n",
    "\n",
    "encoded_batch = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Encoded Batch:\", encoded_batch)\n",
    "# 这里我们使用了 padding=True 来将序列填充到相同的长度，truncation=True 来截断超过模型最大长度的序列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35dfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看词汇表和特殊 tokens\n",
    "print(\"Vocabulary Size:\", tokenizer.vocab_size)\n",
    "print(\"Special Tokens Map:\", tokenizer.special_tokens_map)\n",
    "print(\"Padding Token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"Separator Token:\", tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(\"Classification Token:\", tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(\"Unknown Token:\", tokenizer.unk_token, tokenizer.unk_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
