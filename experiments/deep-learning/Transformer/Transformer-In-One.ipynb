{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/648127076"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/xiabingquan/eb2ceb583a1e8858c23da23ac7a4a340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_mask(b: int, max_len: int, feat_lens: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "    attn_mask = torch.ones((b, max_len, max_len), device=device)\n",
    "    for i in range(b):\n",
    "        attn_mask[i, :, :feat_lens[i]] = 0\n",
    "    return attn_mask.to(torch.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequent_mask(b: int, max_len: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        b: batch-size.\n",
    "        max_len: the length of the whole seqeunce.\n",
    "        device: cuda or cpu.\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones((b, max_len, max_len), device=device), diagonal=1).to(torch.bool)     # or .to(torch.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enc_dec_mask(\n",
    "    b: int, max_feat_len: int, feat_lens: torch.Tensor, max_label_len: int, device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    attn_mask = torch.zeros((b, max_label_len, max_feat_len), device=device)       # (b, seq_q, seq_k)\n",
    "    for i in range(b):\n",
    "        attn_mask[i, :, feat_lens[i]:] = 1\n",
    "    return attn_mask.to(torch.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_sinusoid_embedding(seq_len, d_model):\n",
    "    embeddings = torch.zeros((seq_len, d_model))\n",
    "    for i in range(d_model):\n",
    "        f = torch.sin if i % 2 == 0 else torch.cos\n",
    "        embeddings[:, i] = f(torch.arange(0, seq_len) / np.power(1e4, 2 * (i // 2) / d_model))\n",
    "    return embeddings.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_v, d_model, num_heads, p=0.):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        # linear projections\n",
    "        self.W_Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W_out = nn.Linear(d_v * num_heads, d_model)\n",
    "\n",
    "        # Normalization\n",
    "        # References: <<Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification>>\n",
    "        nn.init.normal_(self.W_Q.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.W_K.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.W_V.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "        nn.init.normal_(self.W_out.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask, **kwargs):\n",
    "        N = Q.size(0)\n",
    "        q_len, k_len = Q.size(1), K.size(1)\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        num_heads = self.num_heads\n",
    "\n",
    "        # multi_head split\n",
    "        Q = self.W_Q(Q).view(N, -1, num_heads, d_k).transpose(1, 2)\n",
    "        K = self.W_K(K).view(N, -1, num_heads, d_k).transpose(1, 2)\n",
    "        V = self.W_V(V).view(N, -1, num_heads, d_v).transpose(1, 2)\n",
    "        \n",
    "        # pre-process mask \n",
    "        if attn_mask is not None:\n",
    "            assert attn_mask.size() == (N, q_len, k_len)\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, num_heads, 1, 1)    # broadcast\n",
    "            attn_mask = attn_mask.bool()\n",
    "\n",
    "        # calculate attention weight\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        if attn_mask is not None:\n",
    "            scores.masked_fill_(attn_mask, -1e4)\n",
    "        attns = torch.softmax(scores, dim=-1)        # attention weights\n",
    "        attns = self.dropout(attns)\n",
    "\n",
    "        # calculate output\n",
    "        output = torch.matmul(attns, V)\n",
    "\n",
    "        # multi_head merge\n",
    "        output = output.transpose(1, 2).contiguous().reshape(N, -1, d_v * num_heads)\n",
    "        output = self.W_out(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, p=0.):\n",
    "        super(PoswiseFFN, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.conv1 = nn.Conv1d(d_model, d_ff, 1, 1, 0)\n",
    "        self.conv2 = nn.Conv1d(d_ff, d_model, 1, 1, 0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv1(X.transpose(1, 2))     # (N, d_model, seq_len) -> (N, d_ff, seq_len)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out).transpose(1, 2)   # (N, d_ff, seq_len) -> (N, d_model, seq_len)\n",
    "        out = self.dropout(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, n, dff, dropout_posffn, dropout_attn):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: input dimension\n",
    "            n: number of attention heads\n",
    "            dff: dimention of PosFFN (Positional FeedForward)\n",
    "            dropout_posffn: dropout ratio of PosFFN\n",
    "            dropout_attn: dropout ratio of attention module\n",
    "        \"\"\"\n",
    "        assert dim % n == 0\n",
    "        hdim = dim // n     # dimension of each attention head\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        # MultiHeadAttention\n",
    "        self.multi_head_attn = MultiHeadAttention(hdim, hdim, dim, n, dropout_attn)\n",
    "        # Position-wise Feedforward Neural Network\n",
    "        self.poswise_ffn = PoswiseFFN(dim, dff, p=dropout_posffn)\n",
    "\n",
    "    def forward(self, enc_in, attn_mask):\n",
    "        # reserve original input for later residual connections\n",
    "        residual = enc_in\n",
    "        # MultiHeadAttention forward\n",
    "        context = self.multi_head_attn(enc_in, enc_in, enc_in, attn_mask)\n",
    "        # residual connection and norm\n",
    "        out = self.norm1(residual + context)\n",
    "        residual = out\n",
    "        # position-wise feedforward\n",
    "        out = self.poswise_ffn(out)\n",
    "        # residual connection and norm\n",
    "        out = self.norm2(residual + out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, dropout_emb, dropout_posffn, dropout_attn,\n",
    "            num_layers, enc_dim, num_heads, dff, tgt_len,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_emb: dropout ratio of Position Embeddings.\n",
    "            dropout_posffn: dropout ratio of PosFFN.\n",
    "            dropout_attn: dropout ratio of attention module.\n",
    "            num_layers: number of encoder layers\n",
    "            enc_dim: input dimension of encoder\n",
    "            num_heads: number of attention heads\n",
    "            dff: dimensionf of PosFFN\n",
    "            tgt_len: the maximum length of sequences\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # The maximum length of input sequence\n",
    "        self.tgt_len = tgt_len\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(pos_sinusoid_embedding(tgt_len, enc_dim), freeze=True)\n",
    "        self.emb_dropout = nn.Dropout(dropout_emb)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(enc_dim, num_heads, dff, dropout_posffn, dropout_attn) for _ in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, X, X_lens, mask=None):\n",
    "        # add position embedding\n",
    "        batch_size, seq_len, d_model = X.shape\n",
    "        out = X + self.pos_emb(torch.arange(seq_len, device=X.device))  # (batch_size, seq_len, d_model)\n",
    "        out = self.emb_dropout(out)\n",
    "        # encoder layers\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim, n, dff, dropout_posffn, dropout_attn):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: input dimension\n",
    "            n: number of attention heads\n",
    "            dff: dimention of PosFFN (Positional FeedForward)\n",
    "            dropout_posffn: dropout ratio of PosFFN\n",
    "            dropout_attn: dropout ratio of attention module\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        assert dim % n == 0\n",
    "        hdim = dim // n\n",
    "        # LayerNorms\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        # Position-wise Feed-Forward Networks\n",
    "        self.poswise_ffn = PoswiseFFN(dim, dff, p=dropout_posffn)\n",
    "        # MultiHeadAttention, both self-attention and encoder-decoder cross attention)\n",
    "        self.dec_attn = MultiHeadAttention(hdim, hdim, dim, n, dropout_attn)\n",
    "        self.enc_dec_attn = MultiHeadAttention(hdim, hdim, dim, n, dropout_attn)\n",
    "\n",
    "    def forward(self, dec_in, enc_out, dec_mask, dec_enc_mask, cache=None, freqs_cis=None):\n",
    "        # decoder's self-attention\n",
    "        residual = dec_in\n",
    "        context = self.dec_attn(dec_in, dec_in, dec_in, dec_mask)\n",
    "        dec_out = self.norm1(residual + context)\n",
    "        # encoder-decoder cross attention\n",
    "        residual = dec_out\n",
    "        context = self.enc_dec_attn(dec_out, enc_out, enc_out, dec_enc_mask)\n",
    "        dec_out = self.norm2(residual + context)\n",
    "        # position-wise feed-forward networks\n",
    "        residual = dec_out\n",
    "        out = self.poswise_ffn(dec_out)\n",
    "        dec_out = self.norm3(residual + out)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, dropout_emb, dropout_posffn, dropout_attn,\n",
    "            num_layers, dec_dim, num_heads, dff, tgt_len, tgt_vocab_size,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_emb: dropout ratio of Position Embeddings.\n",
    "            dropout_posffn: dropout ratio of PosFFN.\n",
    "            dropout_attn: dropout ratio of attention module.\n",
    "            num_layers: number of encoder layers\n",
    "            dec_dim: input dimension of decoder\n",
    "            num_heads: number of attention heads\n",
    "            dff: dimensionf of PosFFN\n",
    "            tgt_len: the target length to be embedded.\n",
    "            tgt_vocab_size: the target vocabulary size.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # output embedding\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, dec_dim)\n",
    "        self.dropout_emb = nn.Dropout(p=dropout_emb)                            # embedding dropout\n",
    "        # position embedding\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(pos_sinusoid_embedding(tgt_len, dec_dim), freeze=True)\n",
    "        # decoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(dec_dim, num_heads, dff, dropout_posffn, dropout_attn) for _ in\n",
    "                range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, labels, enc_out, dec_mask, dec_enc_mask, cache=None):\n",
    "        # output embedding and position embedding\n",
    "        tgt_emb = self.tgt_emb(labels)\n",
    "        pos_emb = self.pos_emb(torch.arange(labels.size(1), device=labels.device))\n",
    "        dec_out = self.dropout_emb(tgt_emb + pos_emb)\n",
    "        # decoder layers\n",
    "        for layer in self.layers:\n",
    "                dec_out = layer(dec_out, enc_out, dec_mask, dec_enc_mask)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, frontend: nn.Module, encoder: nn.Module, decoder: nn.Module,\n",
    "            dec_out_dim: int, vocab: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.frontend = frontend     # feature extractor\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.linear = nn.Linear(dec_out_dim, vocab)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, X_lens: torch.Tensor, labels: torch.Tensor):\n",
    "        X_lens, labels = X_lens.long(), labels.long()\n",
    "        b = X.size(0)\n",
    "        device = X.device\n",
    "        # frontend\n",
    "        out = self.frontend(X)\n",
    "        max_feat_len = out.size(1)                            # compute after frontend because of optional subsampling\n",
    "        max_label_len = labels.size(1)\n",
    "        # encoder\n",
    "        enc_mask = get_len_mask(b, max_feat_len, X_lens, device)\n",
    "        enc_out = self.encoder(out, X_lens, enc_mask)\n",
    "        # decoder\n",
    "        dec_mask = get_subsequent_mask(b, max_label_len, device)\n",
    "        dec_enc_mask = get_enc_dec_mask(b, max_feat_len, X_lens, max_label_len, device)\n",
    "        dec_out = self.decoder(labels, enc_out, dec_mask, dec_enc_mask)\n",
    "        logits = self.linear(dec_out)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 26])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # constants\n",
    "    batch_size = 16                 # batch size\n",
    "    max_feat_len = 100              # the maximum length of input sequence\n",
    "    fbank_dim = 80                  # the dimension of input feature\n",
    "    hidden_dim = 512                # the dimension of hidden layer\n",
    "    vocab_size = 26                 # the size of vocabulary\n",
    "    max_lable_len = 100             # the maximum length of output sequence\n",
    "\n",
    "    # dummy data\n",
    "    fbank_feature = torch.randn(batch_size, max_feat_len, fbank_dim)        # input sequence\n",
    "    feat_lens = torch.randint(1, max_feat_len, (batch_size,))               # the length of each input sequence in the batch\n",
    "    labels = torch.randint(0, 26, (batch_size, max_lable_len))              # output sequence\n",
    "    label_lens = torch.randint(1, 10, (batch_size,))                        # the length of each output sequence in the batch\n",
    "\n",
    "    # model\n",
    "    feature_extractor = nn.Linear(fbank_dim, hidden_dim)                    # a single layer to simulate the audio feature extractor\n",
    "    encoder = Encoder(\n",
    "        dropout_emb=0.1, dropout_posffn=0.1, dropout_attn=0.,\n",
    "        num_layers=6, enc_dim=hidden_dim, num_heads=8, dff=2048, tgt_len=2048\n",
    "    )\n",
    "    decoder = Decoder(\n",
    "        dropout_emb=0.1, dropout_posffn=0.1, dropout_attn=0.,\n",
    "        num_layers=6, dec_dim=hidden_dim, num_heads=8, dff=2048, tgt_len=2048, tgt_vocab_size=vocab_size\n",
    "    )\n",
    "    transformer = Transformer(feature_extractor, encoder, decoder, hidden_dim, vocab_size)\n",
    "\n",
    "    # forward check\n",
    "    logits = transformer(fbank_feature, feat_lens, labels)\n",
    "    print(logits.shape)     # (batch_size, max_label_len, vocab_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "ipykernel-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
