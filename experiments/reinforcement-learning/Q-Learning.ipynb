{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网格世界环境\n",
    "class GridWorld:\n",
    "    def __init__(self, agent_position=(0, 0), goal_position=(4, 4), width=5, height=5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.agent_position = agent_position  # 初始位置：左上角\n",
    "        self.goal_position = goal_position  # 目标位置：右下角\n",
    "        self.obstacles = [(1, 1), (2, 2), (3, 1)]  # 障碍物位置\n",
    "        \n",
    "        # 定义动作 (上, 右, 下, 左)\n",
    "        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.action_names = [\"上\", \"右\", \"下\", \"左\"]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_position = (0, 0)\n",
    "        return self.agent_position\n",
    "    \n",
    "    def is_valid_position(self, position):\n",
    "        x, y = position\n",
    "        return (0 <= x < self.width and \n",
    "                0 <= y < self.height and \n",
    "                position not in self.obstacles)\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        action = self.actions[action_idx]\n",
    "        \n",
    "        # 计算新位置\n",
    "        new_position = (self.agent_position[0] + action[0], \n",
    "                        self.agent_position[1] + action[1])\n",
    "        \n",
    "        # 检查是否是有效位置\n",
    "        if self.is_valid_position(new_position):\n",
    "            self.agent_position = new_position\n",
    "            \n",
    "        # 计算奖励和是否终止\n",
    "        if self.agent_position == self.goal_position:\n",
    "            reward = 100  # 达到目标的奖励\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # 每一步的惩罚，鼓励找到最短路径\n",
    "            done = False\n",
    "            \n",
    "        return self.agent_position, reward, done\n",
    "    \n",
    "    def render(self, q_table=None):\n",
    "        grid = [['□' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        \n",
    "        # 放置障碍物\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = '■'\n",
    "        \n",
    "        # 放置目标\n",
    "        grid[self.goal_position[0]][self.goal_position[1]] = '★'\n",
    "        \n",
    "        # 放置智能体\n",
    "        grid[self.agent_position[0]][self.agent_position[1]] = '○'\n",
    "        \n",
    "        # 可视化Q值（可选）\n",
    "        if q_table is not None:\n",
    "            print(\"当前位置的 Q(s,a) 值：\")\n",
    "            state = self.agent_position\n",
    "            for i, action_name in enumerate(self.action_names):\n",
    "                print(f\"{action_name}: {q_table[state[0], state[1], i]:.2f}\", end=\", \")\n",
    "            print()\n",
    "                \n",
    "        # 打印网格\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print()\n",
    "\n",
    "    def visualize_policy(self, q_table):\n",
    "        policy_grid = [['□' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        \n",
    "        # 放置障碍物\n",
    "        for obs in self.obstacles:\n",
    "            policy_grid[obs[0]][obs[1]] = '■'\n",
    "        \n",
    "        # 放置目标\n",
    "        policy_grid[self.goal_position[0]][self.goal_position[1]] = '★'\n",
    "        \n",
    "        # 为每个状态显示最佳动作\n",
    "        direction_symbols = ['↑', '→', '↓', '←']\n",
    "        \n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if (i, j) not in self.obstacles and (i, j) != self.goal_position:\n",
    "                    # 获取最佳动作\n",
    "                    best_action = np.argmax(q_table[i, j])\n",
    "                    policy_grid[i][j] = direction_symbols[best_action]\n",
    "        \n",
    "        # 打印策略网格\n",
    "        print(\"最优策略：\")\n",
    "        for row in policy_grid:\n",
    "            print(' '.join(row))\n",
    "        print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha      # 学习率\n",
    "        self.gamma = gamma      # 折扣因子\n",
    "        self.epsilon = epsilon  # 探索率\n",
    "        \n",
    "        # 初始化Q表 - 状态为(x, y)，动作为0-3\n",
    "        self.q_table = np.zeros((env.width, env.height, len(env.actions)))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # 使用ε-greedy策略选择动作\n",
    "        if random.random() < self.epsilon:\n",
    "            # 四个action中随机选择一个作为下一个action，展示exploration的思路\n",
    "            return random.randint(0, len(self.env.actions) - 1)  # 探索：随机选择\n",
    "        else:\n",
    "            # state[0], state[1]代表了当前的状态，用该状态去Q表中检索\n",
    "            return np.argmax(self.q_table[state[0], state[1]])   # 利用：选择最大Q值\n",
    "    \n",
    "    def train(self, episodes=1000):\n",
    "        rewards_history = []\n",
    "        steps_history = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # 选择动作\n",
    "                action = self.choose_action(state)\n",
    "                \n",
    "                # 执行动作，观察下一个状态和奖励\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                # Q-learning更新\n",
    "                current_q = self.q_table[state[0], state[1], action]\n",
    "                \n",
    "                # Q-learning使用下一状态的最大Q值，而不是下一个选择动作的Q值\n",
    "                # 这是Q-learning和SARSA之间的关键区别\n",
    "                next_max_q = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "                \n",
    "                # Q(s,a) ← Q(s,a) + α[r + γ·max_a' Q(s',a') - Q(s,a)]\n",
    "                self.q_table[state[0], state[1], action] = \\\n",
    "                    current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)\n",
    "                \n",
    "                # 更新状态\n",
    "                state = next_state\n",
    "            \n",
    "            rewards_history.append(total_reward)\n",
    "            steps_history.append(steps)\n",
    "            \n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"回合 {episode + 1}/{episodes}, 平均奖励: {np.mean(rewards_history[-100:]):.2f}, 平均步数: {np.mean(steps_history[-100:]):.2f}\")\n",
    "        \n",
    "        return rewards_history, steps_history\n",
    "    \n",
    "    def evaluate(self, render=True):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        if render:\n",
    "            print(\"评估模式 - 展示学习到的策略:\")\n",
    "            self.env.render(self.q_table)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        while not done:\n",
    "            # 选择最优动作（不再探索）\n",
    "            action = np.argmax(self.q_table[state[0], state[1]])\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                self.env.render(self.q_table)\n",
    "                time.sleep(1)\n",
    "        \n",
    "        if render:\n",
    "            print(f\"评估完成 - 总奖励: {total_reward}, 总步数: {steps}\")\n",
    "        \n",
    "        return total_reward, steps\n",
    "    \n",
    "    def visualize_policy(self):\n",
    "        \"\"\"可视化学习到的策略\"\"\"\n",
    "        policy_grid = [['□' for _ in range(self.env.width)] for _ in range(self.env.height)]\n",
    "        \n",
    "        # 放置障碍物\n",
    "        for obs in self.env.obstacles:\n",
    "            policy_grid[obs[0]][obs[1]] = '■'\n",
    "        \n",
    "        # 放置目标\n",
    "        policy_grid[self.env.goal_position[0]][self.env.goal_position[1]] = '★'\n",
    "        \n",
    "        # 为每个状态显示最佳动作\n",
    "        direction_symbols = ['↑', '→', '↓', '←']\n",
    "        \n",
    "        for i in range(self.env.height):\n",
    "            for j in range(self.env.width):\n",
    "                if (i, j) not in self.env.obstacles and (i, j) != self.env.goal_position:\n",
    "                    # 获取最佳动作\n",
    "                    best_action = np.argmax(self.q_table[i, j])\n",
    "                    policy_grid[i][j] = direction_symbols[best_action]\n",
    "        \n",
    "        # 打印策略网格\n",
    "        print(\"Q-learning 最优策略：\")\n",
    "        for row in policy_grid:\n",
    "            print(' '.join(row))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化学习进度\n",
    "def plot_learning(rewards, steps):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('reward per episode')\n",
    "    plt.xlabel('episode number')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps)\n",
    "    plt.title('steps per episode')\n",
    "    plt.xlabel('episode number')\n",
    "    plt.ylabel('step number')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def main():\n",
    "    # 创建环境和智能体\n",
    "    env = GridWorld(width=5, height=5)\n",
    "    q_learning_agent = QLearning(env, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "    \n",
    "    # 训练\n",
    "    print(\"开始训练...\")\n",
    "    rewards, steps = q_learning_agent.train(episodes=100)\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    plot_learning(rewards, steps)\n",
    "    \n",
    "    # 评估\n",
    "    q_learning_agent.evaluate(render=True)\n",
    "    \n",
    "    # 显示学习到的策略\n",
    "    print(\"\\n学习到的策略 (最优动作):\")\n",
    "    for i in range(env.width):\n",
    "        for j in range(env.height):\n",
    "            if (i, j) in env.obstacles:\n",
    "                print(\"■\", end=\"\\t\")\n",
    "            elif (i, j) == env.goal_position:\n",
    "                print(\"★\", end=\"\\t\")\n",
    "            else:\n",
    "                best_action = np.argmax(q_learning_agent.q_table[i, j])\n",
    "                print(env.action_names[best_action], end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较 Q-learning 和 SARSA\n",
    "def compare_algorithms(env, episodes=500):\n",
    "    # 训练 Q-learning\n",
    "    print(\"训练 Q-learning 算法...\")\n",
    "    q_table, q_rewards, q_steps = q_learning(env, episodes)\n",
    "    \n",
    "    # 显示 Q-learning 的最优策略\n",
    "    env.visualize_policy(q_table)\n",
    "    \n",
    "    # 重置环境\n",
    "    env = GridWorld()\n",
    "    \n",
    "    # 训练 SARSA\n",
    "    print(\"\\n训练 SARSA 算法...\")\n",
    "    sarsa_table, sarsa_rewards, sarsa_steps = sarsa(env, episodes)\n",
    "    \n",
    "    # 显示 SARSA 的最优策略\n",
    "    env.visualize_policy(sarsa_table)\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 平滑化奖励曲线\n",
    "    def smooth(data, window=10):\n",
    "        return [sum(data[max(0, i-window):i])/min(i, window) for i in range(1, len(data)+1)]\n",
    "    \n",
    "    smooth_q_rewards = smooth(q_rewards)\n",
    "    smooth_sarsa_rewards = smooth(sarsa_rewards)\n",
    "    \n",
    "    # 绘制奖励曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(smooth_q_rewards, label='Q-learning')\n",
    "    plt.plot(smooth_sarsa_rewards, label='SARSA')\n",
    "    plt.xlabel('回合')\n",
    "    plt.ylabel('平均奖励')\n",
    "    plt.title('奖励曲线')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 绘制步数曲线\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(smooth(q_steps), label='Q-learning')\n",
    "    plt.plot(smooth(sarsa_steps), label='SARSA')\n",
    "    plt.xlabel('回合')\n",
    "    plt.ylabel('平均步数')\n",
    "    plt.title('步数曲线')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('algorithm_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"学习曲线已保存为 algorithm_comparison.png\")\n",
    "    \n",
    "    # 测试最优策略的性能\n",
    "    def test_policy(env, q_table, n_episodes=100):\n",
    "        total_rewards = 0\n",
    "        total_steps = 0\n",
    "        success_count = 0\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while not done and steps < 100:\n",
    "                # 根据策略选择动作\n",
    "                action = np.argmax(q_table[state[0], state[1]])\n",
    "                state, reward, done = env.step(action)\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                if done and reward == 100:  # 成功到达目标\n",
    "                    success_count += 1\n",
    "            \n",
    "            total_rewards += episode_reward\n",
    "            total_steps += steps\n",
    "        \n",
    "        avg_reward = total_rewards / n_episodes\n",
    "        avg_steps = total_steps / n_episodes\n",
    "        success_rate = success_count / n_episodes\n",
    "        \n",
    "        return avg_reward, avg_steps, success_rate\n",
    "    \n",
    "    # 测试 Q-learning 和 SARSA 的性能\n",
    "    q_avg_reward, q_avg_steps, q_success_rate = test_policy(GridWorld(), q_table)\n",
    "    sarsa_avg_reward, sarsa_avg_steps, sarsa_success_rate = test_policy(GridWorld(), sarsa_table)\n",
    "    \n",
    "    print(\"\\n性能评估结果:\")\n",
    "    print(f\"Q-learning - 平均奖励: {q_avg_reward:.2f}, 平均步数: {q_avg_steps:.2f}, 成功率: {q_success_rate:.2%}\")\n",
    "    print(f\"SARSA - 平均奖励: {sarsa_avg_reward:.2f}, 平均步数: {sarsa_avg_steps:.2f}, 成功率: {sarsa_success_rate:.2%}\")\n",
    "    \n",
    "    return q_table, sarsa_table\n",
    "\n",
    "# 运行示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建环境\n",
    "    env = GridWorld()\n",
    "    \n",
    "    # 比较 Q-learning 和 SARSA\n",
    "    q_table, sarsa_table = compare_algorithms(env, episodes=500)\n",
    "    \n",
    "    # 演示 Q-learning 策略\n",
    "    print(\"\\n演示 Q-learning 最优策略:\")\n",
    "    env = GridWorld()\n",
    "    state = env.reset()\n",
    "    env.render(q_table)\n",
    "    \n",
    "    # 最多走20步\n",
    "    for _ in range(20):\n",
    "        action = np.argmax(q_table[state[0], state[1]])\n",
    "        state, reward, done = env.step(action)\n",
    "        env.render(q_table)\n",
    "        \n",
    "        if done:\n",
    "            print(\"目标达成!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
