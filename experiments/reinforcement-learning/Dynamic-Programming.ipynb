{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网格世界\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4, goal=(3, 3), reward=1):\n",
    "        self.size = size\n",
    "        self.goal = goal\n",
    "        self.reward = reward\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # 右、左、下、上\n",
    "        self.discount = 0.9\n",
    "\n",
    "    def is_valid(self, state):\n",
    "        return 0 <= state[0] < self.size and 0 <= state[1] < self.size\n",
    "\n",
    "    def step(self, state, action):\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if not self.is_valid(next_state):\n",
    "            next_state = state  # 如果超出边界，保持原位\n",
    "        reward = self.reward if next_state == self.goal else 0\n",
    "        done = next_state == self.goal\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 价值迭代算法\n",
    "def value_iteration(grid_world, threshold=1e-4):\n",
    "    # 初始化状态价值函数\n",
    "    V = np.zeros((grid_world.size, grid_world.size))\n",
    "    delta = float('inf')\n",
    "\n",
    "    while delta > threshold:\n",
    "        delta = 0\n",
    "        for i in range(grid_world.size):\n",
    "            for j in range(grid_world.size):\n",
    "                state = (i, j)\n",
    "                if state == grid_world.goal:\n",
    "                    continue  # 终止状态的价值为0，无需更新\n",
    "                v = V[state]\n",
    "                # 计算每个动作的期望价值\n",
    "                action_values = []\n",
    "                for action in grid_world.actions:\n",
    "                    next_state, reward, _ = grid_world.step(state, action)\n",
    "                    action_values.append(reward + grid_world.discount * V[next_state])\n",
    "                V[state] = max(action_values)\n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "    \n",
    "    # 从价值函数中提取最优策略\n",
    "    policy = np.zeros((grid_world.size, grid_world.size), dtype=int)\n",
    "    for i in range(grid_world.size):\n",
    "        for j in range(grid_world.size):\n",
    "            state = (i, j)\n",
    "            if state == grid_world.goal:\n",
    "                continue\n",
    "            action_values = []\n",
    "            for action in grid_world.actions:\n",
    "                next_state, reward, _ = grid_world.step(state, action)\n",
    "                action_values.append(reward + grid_world.discount * V[next_state])\n",
    "            print(i, j, action_values)\n",
    "            policy[state] = np.argmax(action_values)\n",
    "\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [0.6561000000000001, 0.5904900000000002, 0.6561000000000001, 0.5904900000000002]\n",
      "0 1 [0.7290000000000001, 0.5904900000000002, 0.7290000000000001, 0.6561000000000001]\n",
      "0 2 [0.81, 0.6561000000000001, 0.81, 0.7290000000000001]\n",
      "0 3 [0.81, 0.7290000000000001, 0.9, 0.81]\n",
      "1 0 [0.7290000000000001, 0.6561000000000001, 0.7290000000000001, 0.5904900000000002]\n",
      "1 1 [0.81, 0.6561000000000001, 0.81, 0.6561000000000001]\n",
      "1 2 [0.9, 0.7290000000000001, 0.9, 0.7290000000000001]\n",
      "1 3 [0.9, 0.81, 1.0, 0.81]\n",
      "2 0 [0.81, 0.7290000000000001, 0.6561000000000001, 0.6561000000000001]\n",
      "2 1 [0.9, 0.7290000000000001, 0.7290000000000001, 0.7290000000000001]\n",
      "2 2 [1.0, 0.81, 0.81, 0.81]\n",
      "3 0 [0.7290000000000001, 0.6561000000000001, 0.6561000000000001, 0.7290000000000001]\n",
      "3 1 [0.81, 0.6561000000000001, 0.7290000000000001, 0.81]\n",
      "3 2 [0.9, 0.7290000000000001, 0.81, 0.9]\n",
      "3 3 [0.9, 0.81, 0.9, 1.0]\n",
      "状态价值函数 V:\n",
      "[[0.6561 0.729  0.81   0.9   ]\n",
      " [0.729  0.81   0.9    1.    ]\n",
      " [0.81   0.9    1.     0.    ]\n",
      " [0.729  0.81   0.9    1.    ]]\n",
      "\n",
      "最优策略（动作索引）:\n",
      "[[0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [0 0 0 0]\n",
      " [0 0 0 3]]\n",
      "\n",
      "最优策略（可读形式）:\n",
      "[['→' '→' '→' '↓']\n",
      " ['→' '→' '→' '↓']\n",
      " ['→' '→' '→' '→']\n",
      " ['→' '→' '→' '↑']]\n"
     ]
    }
   ],
   "source": [
    "grid_world = GridWorld(goal=(2, 3))\n",
    "V, policy = value_iteration(grid_world)\n",
    "\n",
    "print(\"状态价值函数 V:\")\n",
    "print(V)\n",
    "\n",
    "print(\"\\n最优策略（动作索引）:\")\n",
    "print(policy)\n",
    "\n",
    "# 将动作索引转换为可读的动作\n",
    "action_names = {0: \"→\", 1: \"←\", 2: \"↓\", 3: \"↑\"}\n",
    "policy_readable = np.vectorize(action_names.get)(policy)\n",
    "print(\"\\n最优策略（可读形式）:\")\n",
    "print(policy_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6561, 0.729 , 0.81  , 0.9   ],\n",
       "       [0.729 , 0.81  , 0.9   , 1.    ],\n",
       "       [0.81  , 0.9   , 1.    , 0.    ],\n",
       "       [0.729 , 0.81  , 0.9   , 1.    ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6561, 0.729 , 0.81  , 0.9   ])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9, 1. , 0. , 1. ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([0, 2, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
