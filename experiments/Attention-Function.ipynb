{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def attention(Q, K, V, d_k):\n",
    "    # step1: calculate similarity dot product between Q and K\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # step2: softmax normalization: softmax function: exp / sum\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    \n",
    "    # step3: alpha dot product V\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[1, 0], [0, 1]])  # 2x2 Query matrix\n",
    "K = np.array([[1, 0], [0, 1]])  # 2x2 Key matrix\n",
    "V = np.array([[1, 2], [3, 4]])  # 2x2 Value matrix\n",
    "d_k = K.shape[1]  # Dimension of the Key (for scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.66976155 0.33023845]\n",
      " [0.33023845 0.66976155]]\n",
      "Output after applying attention:\n",
      " [[1.6604769 2.6604769]\n",
      " [2.3395231 3.3395231]]\n"
     ]
    }
   ],
   "source": [
    "output, attention_weights = attention(Q, K, V, d_k)\n",
    "\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Output after applying attention:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['三国演义', '上卷'], ['罗贯中'], ['滚滚', '长江', '东', '逝水', '浪花', '淘尽', '英雄', '是非成败', '转头', '空', '青山', '依旧', '在', '几度', '夕阳红'], ['白发', '渔樵', '江渚上', '惯看', '秋月春风', '一壶', '浊酒', '喜相逢', '古今', '多少', '事', '都', '付笑谈', '中'], ['--', '调寄', '临江仙']]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../dataset/sanguo.txt\", 'r',encoding='utf-8') #读入文本\n",
    "\n",
    "lines = []\n",
    "for line in f: #分别对每段分词\n",
    "    temp = jieba.lcut(line)  #结巴分词 精确模式\n",
    "    words = []\n",
    "    for i in temp:\n",
    "        #过滤掉所有的标点符号\n",
    "        i = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'””《》]+|[+——！，。？、~@#￥%……&*（）：；‘]+\", \"\", i)\n",
    "        if len(i) > 0:\n",
    "            words.append(i)\n",
    "    if len(words) > 0:\n",
    "        lines.append(words)\n",
    "print(lines[0:5])#预览前5行分词结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用Word2Vec训练\n",
    "# 参数：size: 词向量维度；window: 上下文的宽度，min_count为考虑计算的单词的最低词频阈值\n",
    "model = Word2Vec(lines,vector_size = 20, window = 2 , min_count = 3, epochs=7, negative=10, sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('先主', 0.9160001873970032),\n",
       " ('玄德', 0.8978016376495361),\n",
       " ('使者', 0.8872831463813782),\n",
       " ('周瑜', 0.8821434378623962),\n",
       " ('关公', 0.8793385028839111),\n",
       " ('陆逊', 0.8752183318138123),\n",
       " ('心中', 0.8747538924217224),\n",
       " ('孙权', 0.8602688312530518),\n",
       " ('庞统', 0.8565940856933594),\n",
       " ('门吏', 0.8525434136390686),\n",
       " ('司马昭', 0.8510890007019043),\n",
       " ('袁术', 0.8493541479110718),\n",
       " ('二嫂', 0.8472496867179871),\n",
       " ('密书', 0.8466708660125732),\n",
       " ('鲁肃', 0.8466097116470337),\n",
       " ('维', 0.8464145660400391),\n",
       " ('魏主', 0.846045732498169),\n",
       " ('孙夫人', 0.8451597094535828),\n",
       " ('孙策', 0.8408129215240479),\n",
       " ('后主', 0.8386663198471069)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('孔明', topn = 20) # 与孔明最相关的前20个词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孔明的词向量：\n",
      " [ 0.03546751 -0.23976095  0.42774808 -0.12599747  0.4775886  -0.49785823\n",
      "  0.62812436  1.4118224  -0.26054758  0.9620116   0.4333564  -0.18006714\n",
      "  0.11103736 -0.8641684   0.8370761   0.60063756  0.31567514  0.09167805\n",
      " -0.674838   -0.6923421 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"孔明的词向量：\\n\", model.wv.get_vector('孔明'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取词向量（只读）\n",
    "word_vectors = model.wv\n",
    "\n",
    "# 创建一个新的字典来存储归一化后的词向量\n",
    "normalized_vectors = {}\n",
    "\n",
    "# 对词向量进行归一化\n",
    "for word in word_vectors.index_to_key:\n",
    "    vector = word_vectors[word]\n",
    "    norm = np.linalg.norm(vector)\n",
    "    # 如果向量的范数不为零，则进行归一化\n",
    "    if norm != 0:\n",
    "        normalized_vectors[word] = vector / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孔明的归一化词向量：\n",
      " [ 0.01321955 -0.08936434  0.1594314  -0.04696211  0.1780081  -0.18556304\n",
      "  0.23411618  0.52621824 -0.09711199  0.3585635   0.16152175 -0.06711511\n",
      "  0.04138614 -0.32209516  0.31199723  0.22387123  0.11765928  0.03417049\n",
      " -0.25152743 -0.2580516 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"孔明的归一化词向量：\\n\", normalized_vectors['孔明'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "周瑜的归一化词向量：\n",
      " [ 0.15286243 -0.10232241  0.07909361  0.05580199  0.01053218 -0.04018738\n",
      "  0.14878486  0.32779312 -0.11989477  0.32229838  0.03918505 -0.04621497\n",
      "  0.08911306 -0.5541      0.3462584   0.24091484  0.17802759 -0.07364184\n",
      " -0.20492955 -0.3626902 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"周瑜的归一化词向量：\\n\", normalized_vectors['周瑜'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算相似度的函数\n",
    "def calculate_attention(query, key):\n",
    "    d_k = query.shape[-1]  # 获取向量的维度\n",
    "    similarity = np.dot(query, key) / np.sqrt(d_k)  # 计算点积并归一化\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19725328262457173"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_attention(normalized_vectors['孔明'], normalized_vectors['周瑜'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16912493055189184"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_attention(normalized_vectors['孔明'], normalized_vectors['张飞'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15584850617715132"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_attention(normalized_vectors['刘备'], normalized_vectors['刘禅'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17991281670659112"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_attention(normalized_vectors['刘备'], normalized_vectors['曹操'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14587914604470847"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_attention(normalized_vectors['曹操'], normalized_vectors['关羽'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
