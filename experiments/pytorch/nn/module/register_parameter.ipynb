{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: 0.weight, Parameter size: torch.Size([5, 10])\n",
      "Parameter name: 0.bias, Parameter size: torch.Size([5])\n",
      "Parameter name: 2.weight, Parameter size: torch.Size([1, 5])\n",
      "Parameter name: 2.bias, Parameter size: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DynamicLinear(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DynamicLinear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        \n",
    "        # 动态创建权重和偏置参数，但不使用内置的 nn.Linear\n",
    "        weight = torch.randn(output_features, input_features)\n",
    "        bias = torch.randn(output_features)\n",
    "        \n",
    "        # 注册参数\n",
    "        self.register_parameter('weight', nn.Parameter(weight))\n",
    "        self.register_parameter('bias', nn.Parameter(bias))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.linear(x, self.weight, self.bias)\n",
    "\n",
    "# 使用自定义的 DynamicLinear\n",
    "model = nn.Sequential(\n",
    "    DynamicLinear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    DynamicLinear(5, 1)\n",
    ")\n",
    "\n",
    "# 打印模型的参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Parameter size: {param.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without bias:\n",
      "Parameter name: weight, Parameter size: torch.Size([5, 10])\n",
      "\n",
      "With bias:\n",
      "Parameter name: weight, Parameter size: torch.Size([5, 10])\n",
      "Parameter name: bias, Parameter size: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "class OptionalBias(nn.Module):\n",
    "    def __init__(self, input_features, output_features, use_bias=True):\n",
    "        super(OptionalBias, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(output_features, input_features))\n",
    "        \n",
    "        if use_bias:\n",
    "            bias = torch.randn(output_features)\n",
    "            self.register_parameter('bias', nn.Parameter(bias))\n",
    "        else:\n",
    "            # 如果不使用偏置，则将 bias 设置为 None\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.linear(x, self.weight, self.bias)\n",
    "\n",
    "# 实例化不使用偏置的层\n",
    "layer_no_bias = OptionalBias(10, 5, use_bias=False)\n",
    "\n",
    "# 实例化使用偏置的层\n",
    "layer_with_bias = OptionalBias(10, 5, use_bias=True)\n",
    "\n",
    "# 检查参数\n",
    "print(\"Without bias:\")\n",
    "for name, param in layer_no_bias.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Parameter size: {param.size() if param is not None else 'None'}\")\n",
    "\n",
    "print(\"\\nWith bias:\")\n",
    "for name, param in layer_with_bias.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Parameter size: {param.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: scale_factor, Parameter value: 2.0\n",
      "Parameter name: shift_value, Parameter value: 1.0\n",
      "Output: 6.0\n"
     ]
    }
   ],
   "source": [
    "class ExternalParameterModule(nn.Module):\n",
    "    def __init__(self, parameter_dict):\n",
    "        super(ExternalParameterModule, self).__init__()\n",
    "        for name, value in parameter_dict.items():\n",
    "            param = nn.Parameter(value)\n",
    "            self.register_parameter(name, param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 假设我们使用这些参数进行一些计算\n",
    "        for name, param in self.named_parameters():\n",
    "            x = x * param\n",
    "        return x\n",
    "\n",
    "# 从外部加载参数值\n",
    "external_params = {\n",
    "    'scale_factor': torch.tensor(2.0),\n",
    "    'shift_value': torch.tensor(1.0)\n",
    "}\n",
    "\n",
    "# 创建模型实例\n",
    "model = ExternalParameterModule(external_params)\n",
    "\n",
    "# 查看模型的参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Parameter value: {param.item()}\")\n",
    "\n",
    "# 使用模型进行计算\n",
    "input_data = torch.tensor([3.0])\n",
    "output = model(input_data)\n",
    "print(f\"Output: {output.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
