{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "linear_layer = nn.Linear(in_features=20, out_features=30)\n",
    "input_tensor = torch.randn(128, 20)\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "print(output_tensor.shape)  # torch.Size([128, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "lazy_linear = nn.LazyLinear(out_features=30)\n",
    "input_tensor = torch.randn(128, 20)\n",
    "output_tensor = lazy_linear(input_tensor)\n",
    "print(output_tensor.shape)  # torch.Size([128, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 20])\n"
     ]
    }
   ],
   "source": [
    "print(lazy_linear.weight.shape)  # torch.Size([30, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码展示了如何使用 `nn.Linear` 和 `nn.LazyLinear` 来实现线性变换，并通过打印输出张量的形状和权重的形状来验证它们的行为。下面是对代码的详细解释以及相关的数学原理。\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "#### 1. 使用 `nn.Linear`\n",
    "```python\n",
    "linear_layer = nn.Linear(in_features=20, out_features=30)\n",
    "input_tensor = torch.randn(128, 20)\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "print(output_tensor.shape)  # torch.Size([128, 30])\n",
    "```\n",
    "\n",
    "- **`nn.Linear` 的初始化**：\n",
    "  - 创建了一个线性层 `linear_layer`，输入特征维度为 20（`in_features=20`），输出特征维度为 30（`out_features=30`）。\n",
    "  - 在初始化时，`nn.Linear` 会自动初始化权重矩阵 \\( W \\) 和偏置向量 \\( b \\)：\n",
    "    - 权重矩阵 \\( W \\) 的形状为 `(out_features, in_features)`，即 `(30, 20)`。\n",
    "    - 偏置向量 \\( b \\) 的形状为 `(out_features,)`，即 `(30,)`。\n",
    "\n",
    "- **输入张量**：\n",
    "  - `input_tensor` 的形状为 `(128, 20)`，表示有 128 个样本，每个样本有 20 个特征。\n",
    "\n",
    "- **线性变换**：\n",
    "  - 线性层执行的操作是 \\( y = xW^T + b \\)。\n",
    "  - 输入张量 \\( x \\) 的形状为 `(128, 20)`，权重矩阵 \\( W \\) 的形状为 `(30, 20)`。\n",
    "  - 计算 \\( xW^T \\) 时，矩阵乘法的结果形状为 `(128, 30)`。\n",
    "  - 最终输出张量的形状为 `(128, 30)`，表示每个样本被映射到 30 维的输出空间。\n",
    "\n",
    "#### 2. 使用 `nn.LazyLinear`\n",
    "```python\n",
    "lazy_linear = nn.LazyLinear(out_features=30)\n",
    "input_tensor = torch.randn(128, 20)\n",
    "output_tensor = lazy_linear(input_tensor)\n",
    "print(output_tensor.shape)  # torch.Size([128, 30])\n",
    "print(lazy_linear.weight.shape)  # torch.Size([30, 20])\n",
    "```\n",
    "\n",
    "- **`nn.LazyLinear` 的初始化**：\n",
    "  - 创建了一个 `nn.LazyLinear` 模块，仅指定了输出特征维度为 30（`out_features=30`）。\n",
    "  - 在初始化时，`nn.LazyLinear` **不会** 初始化权重矩阵 \\( W \\) 和偏置向量 \\( b \\)，因为输入特征维度尚未确定。\n",
    "\n",
    "- **第一次前向传播**：\n",
    "  - 当第一次调用 `lazy_linear(input_tensor)` 时，`nn.LazyLinear` 会根据输入张量的形状自动推断输入特征维度（`in_features`）。\n",
    "  - 输入张量的形状为 `(128, 20)`，因此 `in_features` 被推断为 20。\n",
    "  - 此时，`nn.LazyLinear` 会自动初始化权重矩阵 \\( W \\) 和偏置向量 \\( b \\)：\n",
    "    - 权重矩阵 \\( W \\) 的形状为 `(30, 20)`。\n",
    "    - 偏置向量 \\( b \\) 的形状为 `(30,)`。\n",
    "\n",
    "- **线性变换**：\n",
    "  - 与 `nn.Linear` 相同，`nn.LazyLinear` 执行的操作也是 \\( y = xW^T + b \\)。\n",
    "  - 输入张量 \\( x \\) 的形状为 `(128, 20)`，权重矩阵 \\( W \\) 的形状为 `(30, 20)`。\n",
    "  - 最终输出张量的形状为 `(128, 30)`。\n",
    "\n",
    "- **权重形状**：\n",
    "  - 打印 `lazy_linear.weight.shape` 的结果是 `(30, 20)`，这与 `nn.Linear` 的权重形状一致。\n",
    "\n",
    "### 数学原理\n",
    "\n",
    "无论是 `nn.Linear` 还是 `nn.LazyLinear`，它们的核心操作都是线性变换：\n",
    "\\[ y = xW^T + b \\]\n",
    "\n",
    "- \\( x \\) 是输入张量，形状为 `(batch_size, in_features)`。\n",
    "- \\( W \\) 是权重矩阵，形状为 `(out_features, in_features)`。\n",
    "- \\( b \\) 是偏置向量，形状为 `(out_features,)`。\n",
    "- \\( y \\) 是输出张量，形状为 `(batch_size, out_features)`。\n",
    "\n",
    "在矩阵乘法 \\( xW^T \\) 中：\n",
    "- \\( x \\) 的形状为 `(batch_size, in_features)`。\n",
    "- \\( W^T \\) 的形状为 `(in_features, out_features)`。\n",
    "- 结果 \\( y \\) 的形状为 `(batch_size, out_features)`。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- **`nn.Linear`**：\n",
    "  - 需要在初始化时明确指定输入特征维度（`in_features`）。\n",
    "  - 权重和偏置在初始化时立即初始化。\n",
    "\n",
    "- **`nn.LazyLinear`**：\n",
    "  - 在初始化时不指定输入特征维度，而是通过第一次前向传播自动推断。\n",
    "  - 权重和偏置在第一次前向传播时初始化。\n",
    "  - 在第一次前向传播后，`nn.LazyLinear` 自动转换为 `nn.Linear`。\n",
    "\n",
    "两者在数学原理上完全一致，只是初始化方式不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入张量 x，形状为 (batch_size, in_features)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "\n",
    "# 定义权重矩阵 W，形状为 (out_features, in_features)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3],\n",
    "                  [0.4, 0.5, 0.6],\n",
    "                  [0.7, 0.8, 0.9],\n",
    "                  [1.0, 1.1, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4000,  3.2000,  5.0000,  6.8000],\n",
      "        [ 3.2000,  7.7000, 12.2000, 16.7000]])\n"
     ]
    }
   ],
   "source": [
    "# 计算 xW^T\n",
    "output = torch.matmul(x, W.T)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "ipykernel-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
