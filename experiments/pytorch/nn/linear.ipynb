{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 5])\n",
      "Output data: tensor([[ 0.8077,  0.7671, -1.3354, -0.8978,  1.0544],\n",
      "        [ 0.3077,  0.3016, -0.4576, -0.8128,  0.9606],\n",
      "        [ 0.0640,  0.3347,  0.4794,  0.2482,  0.0634]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的全连接层\n",
    "fc = nn.Linear(10, 5)  # 输入特征维度为 10，输出维度为 5\n",
    "\n",
    "# 输入数据\n",
    "input_data = torch.randn(3, 10)  # 3 个样本，每个样本 10 个特征\n",
    "\n",
    "# 前向传播\n",
    "output = fc(input_data)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear的初始化是kaiming初始化，权重随机，并且服从kaiming正态分布，或者kaiming均匀分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: torch.Size([5, 10])\n",
      "Weights: Parameter containing:\n",
      "tensor([[ 0.2481,  0.2493,  0.0071, -0.1585, -0.2911,  0.2051, -0.2996,  0.1113,\n",
      "         -0.0541,  0.1248],\n",
      "        [-0.2266, -0.2820,  0.1351,  0.0176, -0.2856, -0.2698,  0.2831, -0.1588,\n",
      "          0.1559,  0.0903],\n",
      "        [-0.2083, -0.0895, -0.1894, -0.0167,  0.2028,  0.0179, -0.3083,  0.0767,\n",
      "         -0.1286, -0.3130],\n",
      "        [-0.0173, -0.2401,  0.1969, -0.2898,  0.2351, -0.2757, -0.0765, -0.0886,\n",
      "         -0.1438, -0.3039],\n",
      "        [-0.0906,  0.1289, -0.1114,  0.2279, -0.1368,  0.2503,  0.1781, -0.1895,\n",
      "          0.1165,  0.2085]], requires_grad=True)\n",
      "Bias shape: torch.Size([5])\n",
      "Bias: Parameter containing:\n",
      "tensor([ 0.2559, -0.1903, -0.2190, -0.1814,  0.2980], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 查看权重\n",
    "print(\"Weights shape:\", fc.weight.shape)\n",
    "print(\"Weights:\", fc.weight)\n",
    "\n",
    "# 查看偏置\n",
    "print(\"Bias shape:\", fc.bias.shape)\n",
    "print(\"Bias:\", fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6059,  0.1285,  0.4780, -0.4500, -0.0201],\n",
       "        [-0.8102,  0.7428, -0.3083,  0.4793, -0.0139],\n",
       "        [-0.9811,  0.0890, -1.1209, -0.1263, -1.2466]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# 创建一个权重张量\n",
    "weight_tensor = torch.empty(3, 5)  # 假设输出维度为 3，输入维度为 5\n",
    "\n",
    "# mode：可以是 'fan_in' 或 'fan_out'。'fan_in' 用于正向传播，'fan_out' 用于反向传播。\n",
    "# nonlinearity：指定激活函数，通常为 'relu' 或 'leaky_relu'。\n",
    "\n",
    "# 使用 Kaiming Normal 初始化\n",
    "init.kaiming_normal_(weight_tensor, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2740, -0.3445, -0.3596,  0.3580,  0.4891],\n",
       "        [ 0.6418,  0.8994, -0.2666,  0.1239, -0.7271],\n",
       "        [-0.5327, -1.0143,  1.0899, -0.5319, -0.3720]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Kaiming Uniform 初始化\n",
    "init.kaiming_uniform_(weight_tensor, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 nn.Linear 完全使用指南\n",
      "======================================================================\n",
      "🎯 nn.Linear 基础用法\n",
      "==================================================\n",
      "📊 线性层信息:\n",
      "   输入维度: 3\n",
      "   输出维度: 5\n",
      "   权重形状: torch.Size([5, 3])\n",
      "   偏置形状: torch.Size([5])\n",
      "\n",
      "🔍 参数详情:\n",
      "   权重矩阵 W:\n",
      "tensor([[-0.4768,  0.3516,  0.0821],\n",
      "        [-0.0012, -0.2304, -0.2823],\n",
      "        [-0.1321, -0.1995,  0.0219],\n",
      "        [ 0.2142, -0.3229,  0.3560],\n",
      "        [ 0.0937,  0.1607, -0.2104]])\n",
      "   偏置向量 b:\n",
      "tensor([-0.0685, -0.4356,  0.2827,  0.3736, -0.5558])\n",
      "\n",
      "⚡ 前向传播:\n",
      "   输入: tensor([[1., 2., 3.]])\n",
      "   输入形状: torch.Size([1, 3])\n",
      "   输出: tensor([[ 0.4041, -1.7445, -0.1827,  1.0100, -0.7719]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "   输出形状: torch.Size([1, 5])\n",
      "\n",
      "✅ 手动计算验证:\n",
      "   y = xW^T + b\n",
      "   手动计算结果: tensor([[ 0.4041, -1.7445, -0.1827,  1.0100, -0.7719]], grad_fn=<AddBackward0>)\n",
      "   是否相等: True\n",
      "\n",
      "📦 批处理示例\n",
      "==================================================\n",
      "单个样本:\n",
      "   输入形状: torch.Size([4])\n",
      "   输出形状: torch.Size([2])\n",
      "\n",
      "批处理:\n",
      "   输入形状: torch.Size([3, 4])\n",
      "   输出形状: torch.Size([3, 2])\n",
      "\n",
      "高维批处理:\n",
      "   输入形状: torch.Size([2, 5, 4])\n",
      "   输出形状: torch.Size([2, 5, 2])\n",
      "   📝 注意: Linear只对最后一维进行变换\n",
      "\n",
      "🎲 参数初始化\n",
      "==================================================\n",
      "默认初始化:\n",
      "   权重范围: [-0.441, 0.501]\n",
      "   偏置范围: [-0.530, 0.162]\n",
      "\n",
      "Xavier初始化:\n",
      "   权重范围: [-1.020, 0.674]\n",
      "   偏置: Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "\n",
      "自定义初始化:\n",
      "   权重:\n",
      "Parameter containing:\n",
      "tensor([[0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000]], requires_grad=True)\n",
      "   偏置: Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "\n",
      "🚫 无偏置层\n",
      "==================================================\n",
      "无偏置线性层:\n",
      "   权重形状: torch.Size([2, 3])\n",
      "   是否有偏置: False\n",
      "\n",
      "📊 对比:\n",
      "   输入: tensor([[-0.7042, -0.5253,  0.1248]])\n",
      "   无偏置输出: tensor([[-0.1513,  0.2838]], grad_fn=<MmBackward0>)\n",
      "   有偏置输出: tensor([[0.1302, 0.4546]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "🧠 多层感知机 (MLP)\n",
      "==================================================\n",
      "MLP结构:\n",
      "SimpleMLP(\n",
      "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "📊 参数统计:\n",
      "   fc1.weight: torch.Size([20, 10]) -> 200 个参数\n",
      "   fc1.bias: torch.Size([20]) -> 20 个参数\n",
      "   fc2.weight: torch.Size([20, 20]) -> 400 个参数\n",
      "   fc2.bias: torch.Size([20]) -> 20 个参数\n",
      "   fc3.weight: torch.Size([3, 20]) -> 60 个参数\n",
      "   fc3.bias: torch.Size([3]) -> 3 个参数\n",
      "   总参数量: 703\n",
      "\n",
      "⚡ 前向传播:\n",
      "   输入形状: torch.Size([5, 10])\n",
      "   输出形状: torch.Size([5, 3])\n",
      "\n",
      "🤖 Transformer中的Linear层\n",
      "==================================================\n",
      "Transformer参数:\n",
      "   d_model: 512\n",
      "   heads: 8\n",
      "   d_k: 64\n",
      "\n",
      "🔍 注意力线性层:\n",
      "   Q线性层: 512 -> 512\n",
      "   K线性层: 512 -> 512\n",
      "   V线性层: 512 -> 512\n",
      "   输出层: 512 -> 512\n",
      "\n",
      "🍽️ 前馈网络:\n",
      "   第一层: 512 -> 2048\n",
      "   第二层: 2048 -> 512\n",
      "\n",
      "📊 数据流:\n",
      "   输入: torch.Size([2, 10, 512])\n",
      "   Q,K,V: torch.Size([2, 10, 512])\n",
      "   前馈输出: torch.Size([2, 10, 512])\n",
      "\n",
      "💡 实用技巧\n",
      "==================================================\n",
      "🎯 选择输入输出维度:\n",
      "   - 输入维度必须与数据最后一维匹配\n",
      "   - 输出维度根据任务需求确定\n",
      "   - 常用维度: 64, 128, 256, 512, 768, 1024\n",
      "\n",
      "🔧 参数初始化:\n",
      "   - 默认: Kaiming初始化 (适合ReLU)\n",
      "   - Xavier: 适合Sigmoid/Tanh\n",
      "   - 自定义: 根据具体需求\n",
      "\n",
      "⚡ 性能优化:\n",
      "   - 批处理提高效率\n",
      "   - GPU加速计算\n",
      "   - 混合精度训练\n",
      "\n",
      "🐛 常见错误:\n",
      "   - 维度不匹配: 检查in_features\n",
      "   - 梯度消失: 注意初始化和激活函数\n",
      "   - 过拟合: 添加dropout或正则化\n",
      "\n",
      "🆚 Linear vs Conv 对比\n",
      "==================================================\n",
      "Linear层 (全连接):\n",
      "   参数量: 7,850\n",
      "   特点: 每个输入都连接到每个输出\n",
      "\n",
      "Conv层 (卷积):\n",
      "   参数量: 100\n",
      "   特点: 局部连接，权重共享\n",
      "\n",
      "📋 使用场景:\n",
      "   Linear: 分类层、全连接网络、Transformer\n",
      "   Conv: 图像处理、特征提取、CNN\n",
      "\n",
      "🔧 调试Linear层\n",
      "==================================================\n",
      "🔍 参数检查:\n",
      "   权重是否需要梯度: True\n",
      "   偏置是否需要梯度: True\n",
      "   权重梯度: None\n",
      "\n",
      "⚡ 梯度信息:\n",
      "   输入梯度形状: torch.Size([2, 5])\n",
      "   权重梯度形状: torch.Size([3, 5])\n",
      "   偏置梯度形状: torch.Size([3])\n",
      "\n",
      "✅ 梯度检查:\n",
      "   权重梯度范围: [-1.6369, 2.5533]\n",
      "   是否有NaN: False\n",
      "\n",
      "🎊 nn.Linear 教程完成!\n",
      "记住: Linear层就是 y = xW^T + b 的矩阵运算！\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def basic_linear_layer():\n",
    "    \"\"\"nn.Linear 基础用法\"\"\"\n",
    "    print(\"🎯 nn.Linear 基础用法\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 创建线性层：输入3维，输出5维\n",
    "    linear = nn.Linear(in_features=3, out_features=5)\n",
    "    \n",
    "    print(f\"📊 线性层信息:\")\n",
    "    print(f\"   输入维度: {linear.in_features}\")\n",
    "    print(f\"   输出维度: {linear.out_features}\")\n",
    "    print(f\"   权重形状: {linear.weight.shape}\")\n",
    "    print(f\"   偏置形状: {linear.bias.shape}\")\n",
    "    \n",
    "    # 查看参数\n",
    "    print(f\"\\n🔍 参数详情:\")\n",
    "    print(f\"   权重矩阵 W:\\n{linear.weight.data}\")\n",
    "    print(f\"   偏置向量 b:\\n{linear.bias.data}\")\n",
    "    \n",
    "    # 前向传播\n",
    "    input_data = torch.tensor([[1.0, 2.0, 3.0]])  # [1, 3]\n",
    "    output = linear(input_data)\n",
    "    \n",
    "    print(f\"\\n⚡ 前向传播:\")\n",
    "    print(f\"   输入: {input_data}\")\n",
    "    print(f\"   输入形状: {input_data.shape}\")\n",
    "    print(f\"   输出: {output}\")\n",
    "    print(f\"   输出形状: {output.shape}\")\n",
    "    \n",
    "    # 手动计算验证\n",
    "    manual_output = torch.matmul(input_data, linear.weight.T) + linear.bias\n",
    "    print(f\"\\n✅ 手动计算验证:\")\n",
    "    print(f\"   y = xW^T + b\")\n",
    "    print(f\"   手动计算结果: {manual_output}\")\n",
    "    print(f\"   是否相等: {torch.allclose(output, manual_output)}\")\n",
    "\n",
    "def batch_processing():\n",
    "    \"\"\"批处理示例\"\"\"\n",
    "    print(\"\\n📦 批处理示例\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 创建线性层\n",
    "    linear = nn.Linear(4, 2)\n",
    "    \n",
    "    # 单个样本\n",
    "    single_input = torch.randn(4)\n",
    "    single_output = linear(single_input)\n",
    "    print(f\"单个样本:\")\n",
    "    print(f\"   输入形状: {single_input.shape}\")\n",
    "    print(f\"   输出形状: {single_output.shape}\")\n",
    "    \n",
    "    # 批处理\n",
    "    batch_input = torch.randn(3, 4)  # 3个样本，每个4维\n",
    "    batch_output = linear(batch_input)\n",
    "    print(f\"\\n批处理:\")\n",
    "    print(f\"   输入形状: {batch_input.shape}\")\n",
    "    print(f\"   输出形状: {batch_output.shape}\")\n",
    "    \n",
    "    # 更高维度的批处理\n",
    "    high_dim_input = torch.randn(2, 5, 4)  # [batch, seq, features]\n",
    "    high_dim_output = linear(high_dim_input)\n",
    "    print(f\"\\n高维批处理:\")\n",
    "    print(f\"   输入形状: {high_dim_input.shape}\")\n",
    "    print(f\"   输出形状: {high_dim_output.shape}\")\n",
    "    print(f\"   📝 注意: Linear只对最后一维进行变换\")\n",
    "\n",
    "def parameter_initialization():\n",
    "    \"\"\"参数初始化示例\"\"\"\n",
    "    print(\"\\n🎲 参数初始化\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 默认初始化\n",
    "    linear_default = nn.Linear(3, 2)\n",
    "    print(f\"默认初始化:\")\n",
    "    print(f\"   权重范围: [{linear_default.weight.min():.3f}, {linear_default.weight.max():.3f}]\")\n",
    "    print(f\"   偏置范围: [{linear_default.bias.min():.3f}, {linear_default.bias.max():.3f}]\")\n",
    "    \n",
    "    # Xavier初始化\n",
    "    linear_xavier = nn.Linear(3, 2)\n",
    "    nn.init.xavier_uniform_(linear_xavier.weight)\n",
    "    nn.init.zeros_(linear_xavier.bias)\n",
    "    print(f\"\\nXavier初始化:\")\n",
    "    print(f\"   权重范围: [{linear_xavier.weight.min():.3f}, {linear_xavier.weight.max():.3f}]\")\n",
    "    print(f\"   偏置: {linear_xavier.bias}\")\n",
    "    \n",
    "    # 自定义初始化\n",
    "    linear_custom = nn.Linear(3, 2)\n",
    "    with torch.no_grad():\n",
    "        linear_custom.weight.fill_(0.1)\n",
    "        linear_custom.bias.fill_(0.0)\n",
    "    print(f\"\\n自定义初始化:\")\n",
    "    print(f\"   权重:\\n{linear_custom.weight}\")\n",
    "    print(f\"   偏置: {linear_custom.bias}\")\n",
    "\n",
    "def no_bias_example():\n",
    "    \"\"\"无偏置示例\"\"\"\n",
    "    print(\"\\n🚫 无偏置层\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 创建无偏置的线性层\n",
    "    linear_no_bias = nn.Linear(3, 2, bias=False)\n",
    "    \n",
    "    print(f\"无偏置线性层:\")\n",
    "    print(f\"   权重形状: {linear_no_bias.weight.shape}\")\n",
    "    print(f\"   是否有偏置: {linear_no_bias.bias is not None}\")\n",
    "    \n",
    "    # 对比有偏置和无偏置\n",
    "    linear_with_bias = nn.Linear(3, 2, bias=True)\n",
    "    \n",
    "    input_data = torch.randn(1, 3)\n",
    "    output_no_bias = linear_no_bias(input_data)\n",
    "    output_with_bias = linear_with_bias(input_data)\n",
    "    \n",
    "    print(f\"\\n📊 对比:\")\n",
    "    print(f\"   输入: {input_data}\")\n",
    "    print(f\"   无偏置输出: {output_no_bias}\")\n",
    "    print(f\"   有偏置输出: {output_with_bias}\")\n",
    "\n",
    "def mlp_example():\n",
    "    \"\"\"多层感知机示例\"\"\"\n",
    "    print(\"\\n🧠 多层感知机 (MLP)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    class SimpleMLP(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    # 创建MLP\n",
    "    mlp = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    \n",
    "    print(f\"MLP结构:\")\n",
    "    print(mlp)\n",
    "    \n",
    "    # 统计参数\n",
    "    total_params = sum(p.numel() for p in mlp.parameters())\n",
    "    print(f\"\\n📊 参数统计:\")\n",
    "    for name, param in mlp.named_parameters():\n",
    "        print(f\"   {name}: {param.shape} -> {param.numel()} 个参数\")\n",
    "    print(f\"   总参数量: {total_params}\")\n",
    "    \n",
    "    # 前向传播\n",
    "    input_data = torch.randn(5, 10)  # 5个样本\n",
    "    output = mlp(input_data)\n",
    "    print(f\"\\n⚡ 前向传播:\")\n",
    "    print(f\"   输入形状: {input_data.shape}\")\n",
    "    print(f\"   输出形状: {output.shape}\")\n",
    "\n",
    "def transformer_linear_usage():\n",
    "    \"\"\"Transformer中的Linear层使用\"\"\"\n",
    "    print(\"\\n🤖 Transformer中的Linear层\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    d_model = 512\n",
    "    heads = 8\n",
    "    d_k = d_model // heads\n",
    "    \n",
    "    print(f\"Transformer参数:\")\n",
    "    print(f\"   d_model: {d_model}\")\n",
    "    print(f\"   heads: {heads}\")\n",
    "    print(f\"   d_k: {d_k}\")\n",
    "    \n",
    "    # 注意力机制中的Linear层\n",
    "    q_linear = nn.Linear(d_model, d_model)\n",
    "    k_linear = nn.Linear(d_model, d_model)\n",
    "    v_linear = nn.Linear(d_model, d_model)\n",
    "    out_linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    print(f\"\\n🔍 注意力线性层:\")\n",
    "    print(f\"   Q线性层: {d_model} -> {d_model}\")\n",
    "    print(f\"   K线性层: {d_model} -> {d_model}\")\n",
    "    print(f\"   V线性层: {d_model} -> {d_model}\")\n",
    "    print(f\"   输出层: {d_model} -> {d_model}\")\n",
    "    \n",
    "    # 前馈网络中的Linear层\n",
    "    d_ff = 2048\n",
    "    ff_linear1 = nn.Linear(d_model, d_ff)\n",
    "    ff_linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    print(f\"\\n🍽️ 前馈网络:\")\n",
    "    print(f\"   第一层: {d_model} -> {d_ff}\")\n",
    "    print(f\"   第二层: {d_ff} -> {d_model}\")\n",
    "    \n",
    "    # 模拟数据流\n",
    "    batch_size, seq_len = 2, 10\n",
    "    input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    print(f\"\\n📊 数据流:\")\n",
    "    print(f\"   输入: {input_tensor.shape}\")\n",
    "    \n",
    "    # 注意力计算\n",
    "    q = q_linear(input_tensor)\n",
    "    k = k_linear(input_tensor)\n",
    "    v = v_linear(input_tensor)\n",
    "    print(f\"   Q,K,V: {q.shape}\")\n",
    "    \n",
    "    # 前馈网络\n",
    "    ff_output = ff_linear2(F.relu(ff_linear1(input_tensor)))\n",
    "    print(f\"   前馈输出: {ff_output.shape}\")\n",
    "\n",
    "def practical_tips():\n",
    "    \"\"\"实用技巧\"\"\"\n",
    "    print(\"\\n💡 实用技巧\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"🎯 选择输入输出维度:\")\n",
    "    print(\"   - 输入维度必须与数据最后一维匹配\")\n",
    "    print(\"   - 输出维度根据任务需求确定\")\n",
    "    print(\"   - 常用维度: 64, 128, 256, 512, 768, 1024\")\n",
    "    \n",
    "    print(\"\\n🔧 参数初始化:\")\n",
    "    print(\"   - 默认: Kaiming初始化 (适合ReLU)\")\n",
    "    print(\"   - Xavier: 适合Sigmoid/Tanh\")\n",
    "    print(\"   - 自定义: 根据具体需求\")\n",
    "    \n",
    "    print(\"\\n⚡ 性能优化:\")\n",
    "    print(\"   - 批处理提高效率\")\n",
    "    print(\"   - GPU加速计算\")\n",
    "    print(\"   - 混合精度训练\")\n",
    "    \n",
    "    print(\"\\n🐛 常见错误:\")\n",
    "    print(\"   - 维度不匹配: 检查in_features\")\n",
    "    print(\"   - 梯度消失: 注意初始化和激活函数\")\n",
    "    print(\"   - 过拟合: 添加dropout或正则化\")\n",
    "\n",
    "def linear_vs_conv():\n",
    "    \"\"\"Linear vs Conv 对比\"\"\"\n",
    "    print(\"\\n🆚 Linear vs Conv 对比\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Linear层 - 全连接\n",
    "    linear = nn.Linear(784, 10)  # MNIST分类\n",
    "    print(f\"Linear层 (全连接):\")\n",
    "    print(f\"   参数量: {784 * 10 + 10:,}\")\n",
    "    print(f\"   特点: 每个输入都连接到每个输出\")\n",
    "    \n",
    "    # Conv层 - 局部连接\n",
    "    conv = nn.Conv2d(1, 10, kernel_size=3)\n",
    "    print(f\"\\nConv层 (卷积):\")\n",
    "    print(f\"   参数量: {1 * 10 * 3 * 3 + 10}\")\n",
    "    print(f\"   特点: 局部连接，权重共享\")\n",
    "    \n",
    "    print(f\"\\n📋 使用场景:\")\n",
    "    print(\"   Linear: 分类层、全连接网络、Transformer\")\n",
    "    print(\"   Conv: 图像处理、特征提取、CNN\")\n",
    "\n",
    "def debug_linear():\n",
    "    \"\"\"调试Linear层\"\"\"\n",
    "    print(\"\\n🔧 调试Linear层\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    linear = nn.Linear(5, 3)\n",
    "    \n",
    "    # 检查参数\n",
    "    print(\"🔍 参数检查:\")\n",
    "    print(f\"   权重是否需要梯度: {linear.weight.requires_grad}\")\n",
    "    print(f\"   偏置是否需要梯度: {linear.bias.requires_grad}\")\n",
    "    print(f\"   权重梯度: {linear.weight.grad}\")\n",
    "    \n",
    "    # 前向传播\n",
    "    input_data = torch.randn(2, 5, requires_grad=True)\n",
    "    output = linear(input_data)\n",
    "    \n",
    "    # 反向传播\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"\\n⚡ 梯度信息:\")\n",
    "    print(f\"   输入梯度形状: {input_data.grad.shape}\")\n",
    "    print(f\"   权重梯度形状: {linear.weight.grad.shape}\")\n",
    "    print(f\"   偏置梯度形状: {linear.bias.grad.shape}\")\n",
    "    \n",
    "    # 梯度检查\n",
    "    print(f\"\\n✅ 梯度检查:\")\n",
    "    print(f\"   权重梯度范围: [{linear.weight.grad.min():.4f}, {linear.weight.grad.max():.4f}]\")\n",
    "    print(f\"   是否有NaN: {torch.isnan(linear.weight.grad).any()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎉 nn.Linear 完全使用指南\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 运行所有示例\n",
    "    basic_linear_layer()\n",
    "    batch_processing()\n",
    "    parameter_initialization()\n",
    "    no_bias_example()\n",
    "    mlp_example()\n",
    "    transformer_linear_usage()\n",
    "    practical_tips()\n",
    "    linear_vs_conv()\n",
    "    debug_linear()\n",
    "    \n",
    "    print(\"\\n🎊 nn.Linear 教程完成!\")\n",
    "    print(\"记住: Linear层就是 y = xW^T + b 的矩阵运算！\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
