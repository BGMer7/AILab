{"cells":[{"cell_type":"markdown","source":["# 8.5 Batch Normalization"]},{"cell_type":"markdown","source":["Training deep neural networks is difficult. Getting them to converge in a reasonable amount of time can be tricky. In this section, we describe batch normalization, a popular and effective technique that consistently accelerates the convergence of deep networks (Ioffe and Szegedy, 2015). \n","\n","Together with residual blocks—covered later in Section 8.6—batch normalization has made it possible for practitioners to routinely train networks with over 100 layers. A secondary (serendipitous) benefit of batch normalization lies in its inherent regularization."]},{"cell_type":"code","source":["import torch\n","from d2l import torch as d2l\n","from torch import nn"]},{"cell_type":"markdown","source":["## 8.5.2. Batch Normalization Layers"]},{"cell_type":"markdown","source":["### 8.5.2.1. Fully Connected Layers"]},{"cell_type":"markdown","source":["When applying batch normalization to fully connected layers, Ioffe and Szegedy (2015), in their original paper **inserted batch normalization after the affine transformation and before the nonlinear activation function**. Later applications experimented with inserting batch normalization right after activation functions."]},{"cell_type":"markdown","source":["### 8.5.2.2. Convolutional Layers"]},{"cell_type":"markdown","source":["Similarly, with convolutional layers, we can **apply batch normalization after the convolution but before the nonlinear activation function**. "]},{"cell_type":"markdown","source":["The key difference from batch normalization in fully connected layers is that we **apply the operation on a per-channel basis across all locations**.\n","\n","针对每个通道都会单独做BN操作。"]},{"cell_type":"markdown","source":["在计算平均值和方差时，我们会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。"]},{"cell_type":"markdown","source":["### 8.5.2.4. Batch Normalization During Prediction"]},{"cell_type":"markdown","source":["As we mentioned earlier, batch normalization typically behaves differently in training mode than in prediction mode.\n","\n","训练中和预测中的BN操作不完全一致，主要是因为在训练中的噪声用来减少过拟合，而在测试环节中，已经不需要噪声去优化模型。"]},{"cell_type":"markdown","source":["## 8.5.3. Implementation from Scratch"]},{"cell_type":"code","source":["def batch_norm(X, gamma, beta, moving_mean, moving_var, epsilon, momentum):\n","    # Use is_grad_enabled to determine whether we are in training mode\n","    if not torch.is_grad_enabled():\n","        # In prediction mode, use mean and variance obtained by moving average\n","        X_hat = (X - moving_mean) / torch.sqrt(moving_var + epsilon)\n","    else:\n","        assert len(X.shape) in (2, 4)\n","        if len(X.shape) == 2:\n","            # When using a fully connected layer, calculate the mean and\n","            # variance on the feature dimension\n","            mean = X.mean(dim=0)\n","            var = ((X - mean) ** 2).mean(dim=0)\n","        else:\n","            # When using a two-dimensional convolutional layer, calculate the\n","            # mean and variance on the channel dimension (axis=1). Here we\n","            # need to maintain the shape of X, so that the broadcasting\n","            # operation can be carried out later\n","            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n","            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n","        # In training mode, the current mean and variance are used\n","        X_hat = (X - mean) / torch.sqrt(var + epsilon)\n","        # Update the mean and variance using moving average\n","        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n","        moving_var = (1.0 - momentum) * moving_var + momentum * var\n","    Y = gamma * X_hat + beta  # Scale and shift\n","    return Y, moving_mean.data, moving_var.data"]},{"cell_type":"code","source":["class BatchNorm(nn.Module):\n","    # num_features: the number of outputs for a fully connected layer \n","    # or the number of output channels for a convolutional layer. \n","    # num_dims: 2 for a fully connected layer and 4 for a convolutional layer\n","    def __init__(self, num_features, num_dims):\n","        super().__init__()\n","        if num_dims == 2:\n","            shape = (1, num_features)\n","        else:\n","            shape = (1, num_features, 1, 1)\n","        # The scale parameter and the shift parameter (model parameters) are\n","        # initialized to 1 and 0, respectively\n","        self.gamma = nn.Parameter(torch.ones(shape))\n","        self.beta = nn.Parameter(torch.zeros(shape))\n","        # The variables that are not model parameters are initialized to 0 and 1\n","        self.moving_mean = torch.zeros(shape)\n","        self.moving_var = torch.ones(shape)\n","\n","    def forward(self, X):\n","        # If X is not on the main memory, copy moving_mean and moving_var to\n","        # the device where X is located\n","        if self.moving_mean.device != X.device:\n","            self.moving_mean = self.moving_mean.to(X.device)\n","            self.moving_var = self.moving_var.to(X.device)\n","        # Save the updated moving_mean and moving_var\n","        Y, self.moving_mean, self.moving_var = batch_norm(\n","            X, self.gamma, self.beta, self.moving_mean,\n","            self.moving_var, epsilon=1e-5, momentum=0.1)\n","        return Y"]},{"cell_type":"code","source":["# 来自AI生成的简单版BN函数\n","import numpy as np\n","\n","def batch_norm_simplified(x, gamma, beta, epsilon=1e-5):\n","    # 步骤 1：计算当前小批次的均值和方差\n","    mu = np.mean(x, axis=0)\n","    var = np.var(x, axis=0)\n","    \n","    # 步骤 2：归一化\n","    x_normalized = (x - mu) / np.sqrt(var + epsilon)\n","    \n","    # 步骤 3：缩放和偏移\n","    out = gamma * x_normalized + beta\n","    \n","    return out\n"]},{"cell_type":"markdown","source":["## 8.5.4. LeNet with Batch Normalization"]},{"cell_type":"code","source":["class BNLeNetScratch(d2l.Classifier):\n","    def __init__(self, lr=0.1, num_classes=10):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.net = nn.Sequential(\n","            nn.LazyConv2d(6, kernel_size=5), \n","            BatchNorm(6, num_dims=4),\n","            nn.Sigmoid(), \n","            nn.AvgPool2d(kernel_size=2, stride=2),\n","\n","            nn.LazyConv2d(16, kernel_size=5), \n","            BatchNorm(16, num_dims=4),\n","            nn.Sigmoid(), \n","            nn.AvgPool2d(kernel_size=2, stride=2),\n","            \n","            nn.Flatten(), \n","\n","            nn.LazyLinear(120),\n","            BatchNorm(120, num_dims=2), \n","            nn.Sigmoid(), \n","\n","            nn.LazyLinear(84),\n","            BatchNorm(84, num_dims=2), \n","            nn.Sigmoid(),\n","\n","            nn.LazyLinear(num_classes))"]},{"cell_type":"markdown","source":["> Flatten 的作用是在神经网络中将多维输入数据展平成一维数组，通常用于连接卷积层（Conv Layer）和全连接层（Fully Connected Layer, FC Layer）。\n","具体功能：\n","Flatten 操作将高维（多维）张量转换为一维向量。它不会改变数据的内容，只是将维度结构拉平。例如，在卷积神经网络（CNN）中，卷积层的输出通常是三维的（例如 height × width × channels），而全连接层需要一维输入，因此在连接卷积层和全连接层时需要使用 Flatten。"]},{"cell_type":"code","source":["trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n","data = d2l.FashionMNIST(batch_size=128)\n","model = BNLeNetScratch(lr=0.1)\n","model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n","trainer.fit(model, data)"]},{"cell_type":"markdown","source":["### 8.5.5. Concise Implementation"]},{"cell_type":"code","source":["class BNLeNet(d2l.Classifier):\n","    def __init__(self, lr=0.1, num_classes=10):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.net = nn.Sequential(\n","            nn.LazyConv2d(6, kernel_size=5), \n","            # pytorch implements: 2d BN\n","            nn.LazyBatchNorm2d(),\n","            nn.Sigmoid(), \n","            nn.AvgPool2d(kernel_size=2, stride=2),\n","\n","            nn.LazyConv2d(16, kernel_size=5), \n","            nn.LazyBatchNorm2d(),\n","            nn.Sigmoid(), \n","            nn.AvgPool2d(kernel_size=2, stride=2),\n","            \n","            nn.Flatten(), \n","            \n","            nn.LazyLinear(120), \n","            # pytorch implements: 1d BN\n","            nn.LazyBatchNorm1d(),\n","            nn.Sigmoid(),\n","\n","            nn.LazyLinear(84), \n","            nn.LazyBatchNorm1d(),\n","            nn.Sigmoid(), \n","            \n","            nn.LazyLinear(num_classes))"]},{"cell_type":"code","source":["trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n","data = d2l.FashionMNIST(batch_size=128)\n","model = BNLeNet(lr=0.1)\n","model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n","trainer.fit(model, data)"]}],"metadata":{"kernelspec":{"display_name":"ipykernel-d2l","language":"python","name":"ipykernel-d2l"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":2}
