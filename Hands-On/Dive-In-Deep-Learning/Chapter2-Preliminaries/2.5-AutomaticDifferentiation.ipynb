{"cells":[{"cell_type":"markdown","source":"# 2.5 Automatic Differentiation 自动微分","id":"74fb4289920c3519"},{"cell_type":"markdown","source":"## 2.5.1 A Simple Function","id":"f3db361f18cf8b54"},{"cell_type":"code","source":"import torch","id":"92c7c2c9bb8bb05c"},{"cell_type":"code","source":["x = torch.arange(4.0)\n","x"],"id":"dff7ced301dc4d4a"},{"cell_type":"code","source":["# Can also create x = torch.arange(4.0, requires_grad=True)\n","x.requires_grad_(True)\n","x.grad  # The gradient is None by default"],"id":"da427f3168ebc6e6"},{"cell_type":"code","source":["y = 2 * torch.dot(x, x) # 2*(0+1+4+9)\n","y"],"id":"568d9bd5ae9f2aa1"},{"cell_type":"code","source":["y.backward()\n","# y = 2xTx\n","# y' = 4x\n","x.grad"],"id":"a78917bf4968175"},{"cell_type":"code","source":"x.grad == 4*x","id":"9b3f456a3bb076e9"},{"cell_type":"markdown","source":["Note that PyTorch does not automatically reset the gradient buffer when we record a new gradient. \n","\n","Instead, the new gradient **is added to the already-stored gradient**. \n","\n","This behavior comes in handy when we want to optimize the sum of multiple objective functions. "],"id":"a8fd9d43078f1141"},{"cell_type":"code","source":["# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n","x.grad.zero_()\n","y = x.sum()\n","y.backward()\n","x.grad"],"id":"9f52114294487976"},{"cell_type":"markdown","source":"## 2.5.2 Backward for Non-Scalar Variables 非标量的反向传播","id":"25ae7733d6d71205"},{"cell_type":"markdown","source":["PyTorch 通过自动微分 (automatic differentiation) 实现了反向传播 (backpropagation) 的过程。在 PyTorch 中，反向传播的核心是 torch.autograd 模块，它能够自动计算神经网络中张量的梯度。这是通过维护一个动态计算图（computational graph）实现的。\n","\n","以下是 PyTorch 中反向传播的工作原理的详细过程：\n","\n","1. 计算图的构建\n","在前向传播 (forward pass) 过程中，PyTorch 会自动构建一个计算图，这个图描述了张量之间的计算关系。\n","在这个图中，每个节点是一个张量，每条边表示的是这些张量之间的操作。每当你对张量进行操作时，PyTorch 都会记录这些操作，并将它们添加到计算图中。\n","节点 (Node): 代表张量，特别是有 requires_grad=True 的张量，这意味着需要计算这些张量的梯度。\n","边 (Edge): 代表张量之间的操作，如加法、乘法、矩阵乘法等。\n","\n","2. 计算损失函数\n","前向传播完成后，我们通常会计算一个损失函数 (loss function)，它用来衡量模型的输出与目标之间的差距。这个损失函数也是计算图的一部分。\n","\n","3. 反向传播 (Backward Pass)\n","反向传播的核心是通过链式法则 (chain rule) 计算每个参数对损失函数的梯度。在 PyTorch 中，你可以通过调用 loss.backward() 方法触发反向传播，这会执行以下步骤：\n","计算梯度: 从损失函数开始，PyTorch 通过链式法则自动计算各个参数的梯度。这个过程是递归进行的，从输出层开始逐步回溯到输入层。\n","具体地，PyTorch 从损失函数对输出的导数开始，然后沿着计算图反向传播，逐个计算各个中间变量和输入变量的梯度。\n","存储梯度: 计算出来的梯度会被存储在每个张量的 .grad 属性中。需要注意的是，只有设置了 requires_grad=True 的张量才会被计算和存储梯度。\n","\n","4. 梯度更新\n","反向传播完成后，通常会使用优化器（如 SGD、Adam 等）来更新模型的参数。优化器会使用这些计算得到的梯度来更新模型的参数，从而最小化损失函数。\n","\n","5. 清除计算图\n","为了节省内存和提高计算效率，PyTorch 默认在每次调用 .backward() 之后会释放计算图。这样做是因为大部分情况下，计算图只会在每次前向传播中使用一次。如果需要保留计算图（例如在 RNN 中），可以将 retain_graph=True 传递给 .backward() 方法。\n","\n","在这个例子中，z 是由 x 和 y 计算得来的张量。loss.backward() 调用之后，PyTorch 自动计算出 x 和 y 对 loss 的梯度，并将结果保存在 x.grad 和 y.grad 中。\n","\n","总结来说，PyTorch 的反向传播是通过自动微分技术和动态计算图实现的，它能够高效地计算出各个参数的梯度，并且极大地简化了深度学习模型的训练过程。"],"id":"195c5c66a7d83ca9"},{"cell_type":"code","source":["# 创建张量\n","x = torch.tensor(2.0, requires_grad=True)\n","y = torch.tensor(3.0, requires_grad=True)\n","\n","# 前向传播：计算图的构建\n","z = x * y + y**2\n","\n","# 计算损失\n","loss = z\n","\n","# 反向传播：计算梯度\n","loss.backward()\n","\n","# 输出梯度\n","print(x.grad)  # 对 x 的梯度\n","print(y.grad)  # 对 y 的梯度"],"id":"9cb15c2b7970b5eb"},{"cell_type":"code","source":["x = torch.arange(4.0, requires_grad=True)\n","y = 2 * torch.dot(x, x)\n","\n","y = x * x\n","y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n","x.grad"],"id":"c0d2b4e1500df861"},{"cell_type":"markdown","source":"## 2.5.3 Detaching Computation 分离计算","id":"3dd483f35ac6d553"},{"cell_type":"code","source":["x.grad.zero_()\n","y = x * x\n","u = y.detach()\n","u"],"id":"5771d0e8a5725f41"},{"cell_type":"code","source":["z = u * x\n","z"],"id":"fe4991290dc2749e"},{"cell_type":"code","source":"z == x*x*x","id":"e8da1806ce0f546"},{"cell_type":"code","source":["z.sum().backward()\n","x.grad == u"],"id":"21d546e0648abb22"},{"cell_type":"code","source":["x.grad.zero_()\n","y.sum().backward()\n","x.grad == 2 * x"],"id":"c61eb6d5c858c330"},{"cell_type":"markdown","source":"## 2.5.6 Exercises","id":"3d84dcf1bb431921"},{"cell_type":"markdown","source":"2. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。","id":"a9a82ab114abffc"},{"cell_type":"code","source":["a = torch.tensor([1.0, 3.0, 5.0, 7.0, 9.0], requires_grad=True)\n","b = torch.tensor([2.0, 4.0, 6.0, 9.0, 10.0], requires_grad=True)\n","\n","c = a * b\n","s = c.sum()\n","s"],"id":"3cf786290cb8885f"},{"cell_type":"code","source":"s.backward()","id":"e10f3c40b95f3b0"},{"cell_type":"code","source":"a.grad","id":"69a8bc212be8bbcd"},{"cell_type":"code","source":"s.backward() # RuntimeError","id":"7e5e28df3da8900b"},{"cell_type":"markdown","source":"4. 令f(x)=sinx，绘制f(x)和df(x)/dx的图像，其中后者不使用f'(x)=cosx","id":"c8fcb7cdf25c63f0"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","x = torch.arange(0.0,10.0,0.1)\n","x.requires_grad_(True)\n","x1 = x.detach()\n","y1 = torch.sin(x1)\n","y2 = torch.sin(x)\n","y2.sum().backward()\n","plt.plot(x1,y1)\n","plt.plot(x1,x.grad)"],"id":"7bb8f7760c552907"},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 定义 x 的范围，并设置 requires_grad=True 以启用自动微分\n","x = torch.linspace(0, 2 * np.pi, 100, requires_grad=True)\n","\n","# 定义函数 f(x) = sin(x)\n","f_x = torch.sin(x)\n","\n","# 通过自动微分计算 df(x)/dx\n","# 这里 torch.ones_like(x) 是用来确保梯度计算覆盖每个样本。\n","f_x.backward(torch.ones_like(x))\n","df_dx = x.grad\n","\n","# 将张量转换为 numpy 数组以便使用 Matplotlib 进行绘图\n","x_np = x.detach().numpy()\n","f_x_np = f_x.detach().numpy()\n","df_dx_np = df_dx.detach().numpy()\n","\n","# 使用 Matplotlib 绘制 f(x) 和 df(x)/dx 的图像\n","plt.figure(figsize=(10, 6))\n","plt.plot(x_np, f_x_np, label='f(x) = sin(x)', color='blue')\n","plt.plot(x_np, df_dx_np, label=\"df(x)/dx\", color='red', linestyle='--')\n","plt.title('f(x) = sin(x) and df(x)/dx')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"id":"2b1ce07418b1860c"},{"cell_type":"code","source":["# f_x.backward() # RunTimeError: grad can be implicitly created only for scalar outputs\n","# 必须要同一个维度才可以求导，否则就像下面的例子一样标量求导"],"id":"7db355ae1c90ce10"},{"cell_type":"code","source":["import numpy as np\n","x = torch.linspace(0, 3*np.pi, 128)\n","x.requires_grad_(True)\n","y = torch.sin(x)  # y = sin(x)\n","\n","y.sum().backward()\n","\n","plt.plot(x.detach(), y.detach(), label='y=sin(x)') \n","plt.plot(x.detach(), x.grad, label='∂y/∂x=cos(x)')  # dy/dx = cos(x)\n","plt.legend(loc='upper right')\n","plt.show()"],"id":"905637838bb097d"},{"cell_type":"code","source":"","id":"f6b626095ac18330"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"}},"nbformat":4,"nbformat_minor":5}
