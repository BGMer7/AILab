{"cells":[{"cell_type":"markdown","source":["# 3.4. Linear Regression Implementation from Scratch"]},{"cell_type":"code","source":["%matplotlib inline\n","import torch\n","from d2l import torch as d2l"]},{"cell_type":"code","source":["torch.arange(3)"]},{"cell_type":"markdown","source":["## 3.4.1. Defining the Model"]},{"cell_type":"code","source":["class LinearRegressionScratch(d2l.Module):  #@save\n","    \"\"\"The linear regression model implemented from scratch.\"\"\"\n","    def __init__(self, weights_number, learning_rate, sigma=0.01):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        # 权重矩阵，需要求导\n","        self.w = torch.normal(0, sigma, (weights_number, 1), requires_grad=True)\n","        # 偏置量，标量\n","        self.b = torch.zeros(1, requires_grad=True)"]},{"cell_type":"code","source":["# decorator的这种方式是为了适配jupyter notebook的方式，不至于一个格子里写完一个class的所有代码，而是通过装饰器一点一点往class里添加\n","@d2l.add_to_class(LinearRegressionScratch)  #@save\n","def forward(self, X):\n","    return torch.matmul(X, self.w) + self.b"]},{"cell_type":"markdown","source":["## 3.4.2. Defining the Loss Function"]},{"cell_type":"code","source":["@d2l.add_to_class(LinearRegressionScratch) #@Save\n","def loss(self, y_hat, y):\n","    l = (y_hat - y) ** 2/2\n","    return l.mean()"]},{"cell_type":"markdown","source":["## 3.4.3. Defining the Optimization Algorithm"]},{"cell_type":"code","source":["class SGD(d2l.HyperParameters):\n","    \"\"\"minibatch stochastic gradient descent\"\"\"\n","    def __init__(self, params, learning_rate):\n","        self.save_hyperparameters()\n","\n","    def step(self):\n","        for param in self.params:\n","            param -= self.learning_rate * param.grad\n","        print(self.params)\n","\n","    def zero_grad(self):\n","        for param in self.params:\n","            if param.grad is not None:\n","                param.grad.zero_()"]},{"cell_type":"code","source":["@d2l.add_to_class(LinearRegressionScratch) #@Save\n","def configure_optimizers(self):\n","    return SGD([self.w, self.b], self.learning_rate)"]},{"cell_type":"markdown","source":["## 3.4.4. Training"]},{"cell_type":"code","source":["@d2l.add_to_class(d2l.Trainer)  #@save\n","def prepare_batch(self, batch):\n","    return batch\n","\n","@d2l.add_to_class(d2l.Trainer)  #@save\n","def fit_epoch(self):\n","    self.model.train()\n","    for batch in self.train_dataloader:\n","        loss = self.model.training_step(self.prepare_batch(batch))\n","        self.optim.zero_grad()\n","        with torch.no_grad():\n","            loss.backward()\n","            if self.gradient_clip_val > 0:  # To be discussed later\n","                self.clip_gradients(self.gradient_clip_val, self.model)\n","            self.optim.step()\n","        self.train_batch_idx += 1\n","    if self.val_dataloader is None:\n","        return\n","    self.model.eval()\n","    for batch in self.val_dataloader:\n","        with torch.no_grad():\n","            self.model.validation_step(self.prepare_batch(batch))\n","        self.val_batch_idx += 1"]},{"cell_type":"code","source":["model = LinearRegressionScratch(2, learning_rate=0.03)\n","data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n","data.get_dataloader"]},{"cell_type":"code","source":["trainer = d2l.Trainer(max_epochs=5)\n","trainer.fit(model, data)"]},{"cell_type":"code","source":["trainer = d2l.Trainer(max_epochs=5)\n","trainer.fit(model, data)"]},{"cell_type":"code","source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":2}
