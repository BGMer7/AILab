{"cells":[{"cell_type":"markdown","source":"# 3.1. Linear Regression","id":"94231eb20dca2308"},{"cell_type":"code","source":["%matplotlib inline\n","import math\n","import time\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","from d2l import torch as d2l"],"id":"db0389fab8982ed9"},{"cell_type":"markdown","source":"## 3.1.2. Vectorization for Speed","id":"651c85e040e24974"},{"cell_type":"code","source":["n = 10000\n","a = torch.ones(n)\n","b = torch.ones(n)\n","c = torch.zeros(n)"],"id":"7eb5d70e2cb61e1f"},{"cell_type":"code","source":["t = time.time()\n","for i in range(n):\n","    c[i] = a[i] + b[i]\n","f'{time.time() - t:.5f} sec'"],"id":"d7c6e0c417148394"},{"cell_type":"code","source":["t = time.time()\n","d = a + b\n","f'{time.time() - t:.5f} sec'"],"id":"7fbde9912b529874"},{"cell_type":"markdown","source":"## 3.1.3. The Normal Distribution and Squared Loss","id":"d27fddb358a62e7e"},{"cell_type":"markdown","source":"$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$","id":"2c309a2d4e493fe7"},{"cell_type":"code","source":["# normal distribution\n","def normal_distribution(x, mu, sigma):\n","    p = 1 / math.sqrt(2 * math.pi * sigma ** 2)\n","    return p * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))"],"id":"52874c8ef727fe25"},{"cell_type":"code","source":["# data generating\n","x = np.linspace(-4, 4, 1000)\n","y0 = normal_distribution(x, 0, 1)\n","y1 = normal_distribution(x, 2, 1)\n","y2 = normal_distribution(x, 0, 0.5)\n","\n","# 绘制正态分布曲线\n","plt.figure(figsize=(8, 6))\n","plt.plot(x, y0, label='Normal Distribution (mean=0, std=1)', color='blue')\n","plt.plot(x, y1, label='Normal Distribution (mean=2, std=1)', color='red')\n","plt.plot(x, y2, label='Normal Distribution (mean=0, std=0.5)', color='green')\n","\n","plt.title('Normal Distribution')\n","plt.xlabel('x')\n","plt.ylabel('Probability Density')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"id":"92afe4a2f31fd564"},{"cell_type":"code","source":"# TODO 关于最大似然还是需要复习","id":"12757d053bba0fcf"},{"cell_type":"markdown","source":"## 3.1.4. Linear Regression as a Neural Network","id":"a1827e70fa5c902f"},{"cell_type":"markdown","source":"## 3.1.6. Exercises","id":"761cf23d727b5f09"},{"cell_type":"markdown","source":["1. 假设我们有一些数据，$x_1....x_n$，这些数据都是实数。我们的目标是找到一个常数b，使得$\\sum_{i=1}^n (x_i - b)^2$最小。\n","\n","损失函数为$f(b) = \\sum_{i=1}^n (x_i - b)^2$，求b，使得f(x)最小，则对b求导。\n","\n","$$  \\frac{df(b)}{db} = -2 \\sum_{i=1}^n x_i + 2n b $$\n","\n","f(b)连续，所以令导数为零。求得$ b = \\frac{1}{n} \\sum_{i=1}^n x_i $，也即b为x的平均值。"],"id":"4a60a18ca40236a4"},{"cell_type":"code","source":"","id":"174531f06ba7293b"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"}},"nbformat":4,"nbformat_minor":5}
