{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3. Attention Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.1. Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "见experiments/Attention-Function.ipynb\n",
    "\n",
    "归根到底就是先计算attention scores，再利用这个值计算softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2.1. Masked Softmax Operation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Dive  into  Deep    Learning\n",
    "Learn to    code    <blank>\n",
    "Hello world <blank> <blank>\n",
    "\n",
    "These special tokens do not carry meaning.\n",
    "目的是让这些不携带任何信息token基本权重为0。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):  #@save\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    # x: 一个三维张量，通常表示模型输出的得分或注意力权重。\n",
    "    # valid_lens: 一个一维或二维张量，用于指示序列中有效部分的长度。用于对输入 X 进行掩码操作，使得无效位置不参与 softmax 计算\n",
    "    \n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个带掩码的 `softmax` 函数 (`masked_softmax`)，用于在深度学习任务中处理序列数据时，对某些位置的值进行掩码处理，以避免无效的值影响计算。它特别适用于自然语言处理（NLP）中的序列模型，比如注意力机制（Attention）。以下是代码的解释：\n",
    "\n",
    "### 核心功能解释\n",
    "\n",
    "#### 1. **masked_softmax函数**：\n",
    "\n",
    "该函数接收两个输入：\n",
    "- **X**：一个三维张量，通常表示模型输出的得分或注意力权重。\n",
    "- **valid_lens**：一个一维或二维张量，用于指示序列中有效部分的长度。用于对输入 `X` 进行掩码操作，使得无效位置不参与 `softmax` 计算。\n",
    "\n",
    "#### 2. **_sequence_mask函数**：\n",
    "\n",
    "这是一个内部函数，目的是根据 `valid_lens` 对输入 `X` 进行掩码：\n",
    "- **X**：表示输入的序列张量。\n",
    "- **valid_len**：表示有效的序列长度，对于超出长度的部分，填充为指定的 `value`（默认是0）。\n",
    "- **mask**：用来创建掩码矩阵，其中 `True` 表示有效部分，`False` 表示无效部分。\n",
    "\n",
    "#### 3. **流程**：\n",
    "\n",
    "- **没有`valid_lens`的情况**：直接在最后一个维度上进行 `softmax`。\n",
    "- **有`valid_lens`的情况**：\n",
    "  - 使用 `_sequence_mask` 对 `X` 的无效位置（长度之外的位置）进行掩码，并将它们替换为非常大的负数（`-1e6`）。在进行 `softmax` 时，这些非常大的负数会被映射到接近于 0 的概率。\n",
    "  - 通过 `softmax` 函数计算得到概率分布。\n",
    "\n",
    "\n",
    "### 代码细节：\n",
    "\n",
    "1. **_sequence_mask**：根据 `valid_len` 对 `X` 的无效部分掩码，并将其值替换为 `value`。\n",
    "   - `mask`：创建一个布尔掩码矩阵，表示哪些元素应该保留，哪些应该被掩盖。\n",
    "   - `X[~mask] = value`：对 `mask` 中为 `False` 的位置，设置为 `value`（默认是 0）。\n",
    "\n",
    "2. **valid_lens的处理**：\n",
    "   - 如果 `valid_lens` 是一维的，它会被扩展为与 `X` 的形状匹配。\n",
    "   - 然后通过 `_sequence_mask` 对 `X` 进行掩码处理，将无效的部分替换为非常小的数值（`-1e6`）。\n",
    "\n",
    "3. **softmax计算**：最后在掩码后的 `X` 上应用 `softmax`，从而得到有效部分的概率分布。\n",
    "\n",
    "### 总结\n",
    "\n",
    "这个 `masked_softmax` 函数特别适用于处理变长序列，在深度学习模型（例如 Transformer）中对不同长度的输入序列进行 `softmax` 操作时使用。通过 `valid_lens` 进行掩码，确保了只在有效部分上计算 `softmax`，避免了无效值的干扰。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how this function works, consider a minibatch of two examples of size 2x4, where their valid lengths are 2 and 3, respectively. \n",
    "\n",
    "As a result of the masked softmax operation, values beyond the valid lengths for each pair of vectors are all masked as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4459, 0.5541, 0.0000, 0.0000],\n",
       "         [0.5048, 0.4952, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3355, 0.3719, 0.2926, 0.0000],\n",
       "         [0.3038, 0.3705, 0.3257, 0.0000]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5011, 0.4989, 0.0000, 0.0000],\n",
       "         [0.5478, 0.4522, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2715, 0.2985, 0.4300, 0.0000],\n",
       "         [0.2788, 0.4322, 0.2891, 0.0000]],\n",
       "\n",
       "        [[0.3759, 0.6241, 0.0000, 0.0000],\n",
       "         [0.4576, 0.5424, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example\n",
    "masked_softmax(torch.rand(3, 2, 4), torch.tensor([2, 3, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2787, 0.3838, 0.3375, 0.0000]],\n",
       "\n",
       "        [[0.4754, 0.5246, 0.0000, 0.0000],\n",
       "         [0.1718, 0.1838, 0.3247, 0.3197]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2.2. Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，**Batch Matrix Multiplication** 是指对一批（batch）矩阵执行矩阵乘法操作，通常用于处理多个样本（批量输入）时的并行计算。它可以帮助加速在深度学习模型中计算注意力权重或神经网络的前向传播。\n",
    "\n",
    "PyTorch 和 NumPy 都支持批量矩阵乘法操作。以下是关于如何使用这两个库执行 **Batch Matrix Multiplication** 的介绍。\n",
    "\n",
    "### PyTorch 中的 Batch Matrix Multiplication\n",
    "\n",
    "在 PyTorch 中，`torch.bmm()` 用于执行批量矩阵乘法。`torch.matmul()` 也支持高维张量的批量矩阵乘法。\n",
    "\n",
    "- `torch.bmm(A, B)` 要求 `A` 和 `B` 的形状为 `(batch_size, m, n)` 和 `(batch_size, n, p)`，即每个批次都是一个 `(m, n)` 和 `(n, p)` 形状的矩阵。最终的输出形状是 `(batch_size, m, p)`。\n",
    "\n",
    "#### 示例\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 创建两组批次矩阵 A 和 B\n",
    "A = torch.randn(5, 3, 4)  # 5个批次，每个矩阵是3x4\n",
    "B = torch.randn(5, 4, 2)  # 5个批次，每个矩阵是4x2\n",
    "\n",
    "# 使用 torch.bmm() 进行批量矩阵乘法\n",
    "result = torch.bmm(A, B)\n",
    "print(result.shape)  # 输出形状为 (5, 3, 2)\n",
    "```\n",
    "\n",
    "### NumPy 中的 Batch Matrix Multiplication\n",
    "\n",
    "在 NumPy 中，`numpy.matmul()` 可以用于批量矩阵乘法。它支持高维数组，具体行为取决于输入的形状：\n",
    "\n",
    "- 如果 `A` 和 `B` 都是 3D 数组（例如 `(batch_size, m, n)` 和 `(batch_size, n, p)`），则 `numpy.matmul()` 会沿着第一个维度批量执行矩阵乘法。\n",
    "\n",
    "#### 示例\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 创建两组批次矩阵 A 和 B\n",
    "A = np.random.randn(5, 3, 4)  # 5个批次，每个矩阵是3x4\n",
    "B = np.random.randn(5, 4, 2)  # 5个批次，每个矩阵是4x2\n",
    "\n",
    "# 使用 numpy.matmul() 进行批量矩阵乘法\n",
    "result = np.matmul(A, B)\n",
    "print(result.shape)  # 输出形状为 (5, 3, 2)\n",
    "```\n",
    "\n",
    "### 多维张量支持\n",
    "\n",
    "`torch.matmul()` 和 `np.matmul()` 都支持多维张量。当输入为 3D 张量（或更高维度的张量）时，都会执行批量矩阵乘法。\n",
    "\n",
    "- **PyTorch**: `torch.matmul(A, B)` 会自动处理多维张量。对于高于 2D 的张量，它会沿着批次维度应用矩阵乘法。\n",
    "- **NumPy**: 同样，`np.matmul(A, B)` 也会自动沿着批次维度执行矩阵乘法。\n",
    "\n",
    "#### 高维张量示例（4D 张量）\n",
    "\n",
    "```python\n",
    "# PyTorch\n",
    "A = torch.randn(2, 5, 3, 4)  # 2x5批次，每个矩阵是3x4\n",
    "B = torch.randn(2, 5, 4, 2)  # 2x5批次，每个矩阵是4x2\n",
    "result = torch.matmul(A, B)\n",
    "print(result.shape)  # 输出形状为 (2, 5, 3, 2)\n",
    "\n",
    "# NumPy\n",
    "A = np.random.randn(2, 5, 3, 4)\n",
    "B = np.random.randn(2, 5, 4, 2)\n",
    "result = np.matmul(A, B)\n",
    "print(result.shape)  # 输出形状为 (2, 5, 3, 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q = [Q_1, Q_2,...,Q_n] $$\n",
    "$$ K = [K_1, K_2,...,K_n] $$\n",
    "$$ BMM(Q, k) = [Q_1K_1, Q_2K_2,...,Q_nK_n] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "K: tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n",
      "BMM:  tensor([[[4., 4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4., 4.]],\n",
      "\n",
      "        [[4., 4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4., 4.]]])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.ones((2, 3, 4))\n",
    "print(\"Q:\", Q)\n",
    "K = torch.ones((2, 4, 6))\n",
    "print(\"K:\", K)\n",
    "print(\"BMM: \", torch.bmm(Q, K))\n",
    "\n",
    "d2l.check_shape(torch.bmm(Q, K), (2, 3, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 4)\n",
    "b = torch.ones(4, 6)\n",
    "# [1, 1, 1, 1\n",
    "#  1, 1, 1, 1\n",
    "#  1, 1, 1, 1]\n",
    "\n",
    "# [1, 1, 1, 1, 1, 1\n",
    "#  1, 1, 1, 1, 1, 1\n",
    "#  1, 1, 1, 1, 1, 1\n",
    "#  1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# [1*1+1*1+1*1+1*1+1*1, 1*1+1*1+1*1+1*1+1*1, 1*1+1*1+1*1+1*1+1*1, 1*1+1*1+1*1+1*1+1*1, 1*1+1*1+1*1+1*1+1*1, 1*1+1*1+1*1+1*1+1*1, 1*1+1*1+1*1+1*1+1*1\n",
    "#  ...\n",
    "#  ...\n",
    "#  ...]\n",
    "print(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.3. Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):  #@save\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keys.transpose(1, 2)` 的作用是将 `keys` 张量的**第二维度**（`no. of key-value pairs`）和**第三维度**（`d`，特征维度）交换，以便与 `queries` 进行矩阵乘法。\n",
    "\n",
    "在 `DotProductAttention` 中：\n",
    "\n",
    "- `queries` 的形状是 `(batch_size, no. of queries, d)`。\n",
    "- `keys` 的形状是 `(batch_size, no. of key-value pairs, d)`。\n",
    "\n",
    "为了进行**点积注意力**计算，需要将 `queries` 和 `keys` 的最后一维（`d`）对齐进行点积。因此，需要将 `keys` 的形状从 `(batch_size, no. of key-value pairs, d)` 转换为 `(batch_size, d, no. of key-value pairs)`，即通过 `keys.transpose(1, 2)` 交换第二维和第三维。\n",
    "\n",
    "### 点积计算：\n",
    "\n",
    "- `queries` 的形状是 `(batch_size, no. of queries, d)`。\n",
    "- `keys.transpose(1, 2)` 变成 `(batch_size, d, no. of key-value pairs)`。\n",
    "\n",
    "这样就可以使用 **批量矩阵乘法** `torch.bmm(queries, keys.transpose(1, 2))`，计算出 `queries` 和 `keys` 之间的点积，其结果是一个形状为 `(batch_size, no. of queries, no. of key-value pairs)` 的张量。\n",
    "\n",
    "这个输出就是**每个 query 和每个 key 之间的相似度分数**，用于后续的 softmax 和加权求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries:  tensor([[[-0.1933, -0.6525]],\n",
      "\n",
      "        [[-1.2667,  0.0445]]])\n",
      "keys:  tensor([[[-1.4692,  0.0432],\n",
      "         [ 0.2185,  0.4542],\n",
      "         [ 1.7789, -1.6600],\n",
      "         [-0.4463, -0.0735],\n",
      "         [-0.6156, -0.9115],\n",
      "         [ 0.6116,  1.0802],\n",
      "         [-0.3488,  0.4225],\n",
      "         [ 2.1640,  0.2601],\n",
      "         [-3.2539,  0.3244],\n",
      "         [-1.9488, -2.7373]],\n",
      "\n",
      "        [[-0.0564,  0.0604],\n",
      "         [-0.3512,  1.4117],\n",
      "         [ 0.3267, -1.4301],\n",
      "         [-0.0409, -1.7266],\n",
      "         [-0.1149,  0.4027],\n",
      "         [ 0.6222, -0.8383],\n",
      "         [ 1.1585, -0.2635],\n",
      "         [ 2.7509, -1.3584],\n",
      "         [ 1.3409,  1.4852],\n",
      "         [ 1.0172, -0.3419]]])\n",
      "values:  tensor([[[ 0.4734,  0.0464,  1.0230,  0.3980],\n",
      "         [ 1.2151, -0.1061,  0.2331,  1.1884],\n",
      "         [-0.7626, -0.1376, -0.3411, -1.8837],\n",
      "         [ 0.2046,  1.0314, -0.9120,  1.2394],\n",
      "         [-1.0771,  1.0042,  0.7402, -0.3134],\n",
      "         [-0.1184, -0.2537,  0.9078,  0.4044],\n",
      "         [-1.1080, -0.8064,  0.4437,  0.0530],\n",
      "         [-0.1464,  0.6323, -0.3714,  1.4315],\n",
      "         [ 0.1026, -0.2382, -0.9837,  0.3330],\n",
      "         [-0.7611, -0.2642,  0.0579, -0.2390]],\n",
      "\n",
      "        [[-0.3894, -0.3681, -0.3704,  0.3831],\n",
      "         [-0.2752, -1.6868, -0.8033,  0.1524],\n",
      "         [-0.6536,  0.4047, -0.8567,  0.0176],\n",
      "         [-0.9643, -0.5741,  0.5494, -1.0741],\n",
      "         [-0.5869, -0.5924, -1.2367, -0.7242],\n",
      "         [ 0.1236, -1.7369, -1.0762, -0.3600],\n",
      "         [-1.7841, -0.1930,  0.7441, -0.3164],\n",
      "         [ 0.3348,  1.0110,  0.7116,  1.2367],\n",
      "         [-0.5305, -0.6334, -0.6167, -0.5527],\n",
      "         [-0.4849,  0.2657,  0.1106,  0.1359]]])\n"
     ]
    }
   ],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "print(\"queries: \", queries)\n",
    "keys = torch.normal(0, 1, (2, 10, 2))\n",
    "print(\"keys: \", keys)\n",
    "values = torch.normal(0, 1, (2, 10, 4))\n",
    "print(\"values: \", values)\n",
    "\n",
    "valid_lens = torch.tensor([2, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = DotProductAttention(dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DotProductAttention(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(attention.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7674, -0.0141,  0.7098,  0.7113]],\n",
       "\n",
       "        [[-0.4790, -0.8039, -0.6142, -0.2447]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(queries, keys, values, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"187.07675pt\" height=\"106.747003pt\" viewBox=\"0 0 187.07675 106.747003\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-10-13T15:18:17.161121</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 106.747003 \n",
       "L 187.07675 106.747003 \n",
       "L 187.07675 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 34.240625 63.248097 \n",
       "L 145.840625 63.248097 \n",
       "L 145.840625 40.928097 \n",
       "L 34.240625 40.928097 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p4bf4275b58)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAJsAAAAfCAYAAADwQL9CAAAAvUlEQVR4nO3asQkCQRRF0ZmNrUADCzDSAgRz+7ALS7AR2zCxBcHEyFRMhbWHGXiyeE7++BNcNtr6OZ/G0mi831qnpZRS6m7fPn4+um6X96t5WjfbrtPDctW1n6rh1w/gf4iNGLERIzZixEaM2IgRGzFiI0ZsxIiNGLERIzZixEaM2IgRGzH1UGbN/7Md14uu4/PLtWvPtPiyESM2YsRGjNiIERsxYiNGbMSIjRixESM2YsRGjNiIERsxYiPmC3fiDwOemUErAAAAAElFTkSuQmCC\" id=\"image22bd25d785\" transform=\"scale(1 -1) translate(0 -22.32)\" x=\"34.240625\" y=\"-40.928097\" width=\"111.6\" height=\"22.32\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m2cc6f531a0\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cc6f531a0\" x=\"39.820625\" y=\"63.248097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(36.639375 77.846534) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2cc6f531a0\" x=\"95.620625\" y=\"63.248097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(92.439375 77.846534) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- Keys -->\n",
       "     <g transform=\"translate(78.371094 91.524659) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 2694 \n",
       "L 3353 4666 \n",
       "L 4166 4666 \n",
       "L 1850 2491 \n",
       "L 4331 0 \n",
       "L 3500 0 \n",
       "L 1259 2247 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4b\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"60.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"122.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"181.279297\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path id=\"m698eadd05e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m698eadd05e\" x=\"34.240625\" y=\"46.508097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(20.878125 50.307315) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m698eadd05e\" x=\"34.240625\" y=\"57.668097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(20.878125 61.467315) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- Queries -->\n",
       "     <g transform=\"translate(14.798437 71.395128) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-51\" d=\"M 2522 4238 \n",
       "Q 1834 4238 1429 3725 \n",
       "Q 1025 3213 1025 2328 \n",
       "Q 1025 1447 1429 934 \n",
       "Q 1834 422 2522 422 \n",
       "Q 3209 422 3611 934 \n",
       "Q 4013 1447 4013 2328 \n",
       "Q 4013 3213 3611 3725 \n",
       "Q 3209 4238 2522 4238 \n",
       "z\n",
       "M 3406 84 \n",
       "L 4238 -825 \n",
       "L 3475 -825 \n",
       "L 2784 -78 \n",
       "Q 2681 -84 2626 -87 \n",
       "Q 2572 -91 2522 -91 \n",
       "Q 1538 -91 948 567 \n",
       "Q 359 1225 359 2328 \n",
       "Q 359 3434 948 4092 \n",
       "Q 1538 4750 2522 4750 \n",
       "Q 3503 4750 4090 4092 \n",
       "Q 4678 3434 4678 2328 \n",
       "Q 4678 1516 4351 937 \n",
       "Q 4025 359 3406 84 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"78.710938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"142.089844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"203.613281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"244.726562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"272.509766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"334.033203\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 34.240625 63.248097 \n",
       "L 34.240625 40.928097 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 145.840625 63.248097 \n",
       "L 145.840625 40.928097 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 34.240625 63.248097 \n",
       "L 145.840625 63.248097 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 34.240625 40.928097 \n",
       "L 145.840625 40.928097 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 152.815625 93.668097 \n",
       "L 156.973625 93.668097 \n",
       "L 156.973625 10.508097 \n",
       "L 152.815625 10.508097 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAAYAAAB0CAYAAACmJkOCAAAAyElEQVR4nM2WSwrEMAxDXcj9zzqbblp/egG/gEII02WEJMt2Qq+6f2XNN6yyO7dhGQQQo1qLFY9ij30AB5Rz6B7uIqNQKkiKAUwuAyRV8e6SmjTxgAcy5An+qTkD+vrQNVgw15sI5qWXi909kLwCpTCgDPgB83dfuXpVFDAD3vakR18H3KEqWJKJVCBAyTEgSukBw2WP9thsJP0zOAEoFcjoz22EUQ5kyFVhwH5BZx4MkBQ2Ee7mhAEjX2E88sxRCluyYg4MquoD0XoQUGMmzEcAAAAASUVORK5CYII=\" id=\"image5f5eaa7d94\" transform=\"scale(1 -1) translate(0 -83.52)\" x=\"152.64\" y=\"-10.08\" width=\"4.32\" height=\"83.52\"/>\n",
       "   <g id=\"matplotlib.axis_3\"/>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <defs>\n",
       "       <path id=\"md9b1e7f055\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md9b1e7f055\" x=\"156.973625\" y=\"93.668097\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(163.973625 97.467315) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md9b1e7f055\" x=\"156.973625\" y=\"66.111804\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(163.973625 69.911023) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md9b1e7f055\" x=\"156.973625\" y=\"38.555511\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(163.973625 42.35473) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md9b1e7f055\" x=\"156.973625\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(163.973625 14.798437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"LineCollection_1\"/>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 152.815625 93.668097 \n",
       "L 154.894625 93.668097 \n",
       "L 156.973625 93.668097 \n",
       "L 156.973625 10.508097 \n",
       "L 154.894625 10.508097 \n",
       "L 152.815625 10.508097 \n",
       "L 152.815625 93.668097 \n",
       "z\n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p4bf4275b58\">\n",
       "   <rect x=\"34.240625\" y=\"40.928097\" width=\"111.6\" height=\"22.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.show_heatmaps(attention.attention_weights.reshapee((1, 1, 2, 10)), xlabel='Keys', ylabel='Queris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.4. Additive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "似乎d2l中的attention相加更加复杂一些，普通的相加就是score*weight相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):  #@save\n",
    "    \"\"\"Additive attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
    "        self.w_v = nn.LazyLinear(1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of queries: (batch_size, no. of\n",
    "        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n",
    "        # key-value pairs, num_hiddens). Sum them up with broadcasting\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of self.w_v, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n",
    "        # no. of queries, no. of key-value pairs)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of values: (batch_size, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-d2l",
   "language": "python",
   "name": "ipykernel-d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
