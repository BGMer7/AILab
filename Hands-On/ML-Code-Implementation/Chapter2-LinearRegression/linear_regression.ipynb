{"cells":[{"cell_type":"code","id":"cd047f4c-725d-4103-9e63-8fafd1a176b6","source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","id":"d495845c-f08f-4f83-ae1f-688b2066a88b","source":["### 初始化模型参数\n","def initialize_params(dims):\n","    \"\"\"\n","    输入：\n","    dims: 训练数据的变量维度\n","    输出：\n","    w: 初始化权重系数\n","    b: 初始化偏置参数\n","    \"\"\"\n","    # 初始化权重系数为零向量\n","    w = np.zeros((dims, 1))\n","    \n","    # 初始化偏置参数为零\n","    b = 0\n","    \n","    return w, b"]},{"cell_type":"markdown","id":"50133b15-f089-429b-9147-a594e330caac","source":["1. 均方损失函数是机器学习常用的损失函数\n","$$\n","MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n","$$\n","\n","2. 假设模型是线性模型，即：\n","$$\n","\\hat{y}=Xw\n","$$\n","其中：\n","    - $x$是输入数据矩阵，shape是(n, d)\n","    - $w$是权重系数向量，shape是(d, 1)\n","    - $y$是预测值向量，shape是(n, 1)\n","\n","3. 关键问题在于求梯度\n","我们需要计算损失函数对权重系数$w$的梯度，以便进行梯度下降。\n","MSE是对权重系数向量求导，即：\n","$\\nabla_w \\text{MSE} = \\frac{\\partial \\text{MSE}}{\\partial w}$，其中$\\nabla$代表对每个变量以此求导。\n","将$\\hat{y}=Xw$代入$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$。此时对于每个$w_j$求导得到\n","$$\n","\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^{n} 2 (\\hat{y}_i - y_i) (X_{ij})\n","$$\n","向量化之后，化简为：\n","$$\n","\\nabla_w \\text{MSE} = \\frac{1}{n} X^T (\\hat{y} - y)\n","$$\n"]},{"cell_type":"code","id":"52f4084a-184b-46e0-875c-1da74eb5a4ad","source":["def linear_loss(X, y, w, b):\n","    \"\"\"\n","    输入：\n","    X: 输入变量矩阵\n","    y: 输出标签向量\n","    w: 变量参数权重矩阵\n","    b: 偏置\n","    输出：\n","    y_hat: 线性回归模型预测值\n","    loss: 均方损失\n","    dw: 权重系数一阶偏导\n","    db: 偏置一阶偏导\n","    \"\"\"\n","    \n","    # 训练样本量\n","    num_train = X.shape[0]\n","    \n","    # 训练特征数\n","    num_feature = X.shape[1]\n","    \n","    # 线性回归预测值\n","    y_hat = np.dot(X, w) + b\n","    \n","    # 计算预测值与实际标签之间的均方损失\n","    loss = np.sum((y_hat - y) ** 2) / num_train\n","    \n","    # 基于均方损失对权重系数的一阶梯度（化简过程如上）\n","    dw = np.dot(X.T, (y_hat - y)) / num_train\n","    \n","    # 基于均方损失对偏置的一阶梯度\n","    db = np.sum((y_hat - y)) / num_train\n","    \n","    return y_hat, loss, dw, db"]},{"cell_type":"code","id":"edd72e3e-c8e9-4c49-b634-faea5f701d79","source":["### 定义线性回归模型训练过程\n","def linear_train(X, y, learning_rate=0.01, epochs=10000):\n","    \"\"\"\n","    输入：\n","    X：输入变量矩阵\n","    y：输出标签向量\n","    learning_rate：学习率\n","    epochs：训练迭代次数\n","    输出：\n","    loss_his：每次迭代的均方损失\n","    params：优化后的参数字典\n","    grads：优化后的参数梯度字典\n","    \"\"\"\n","    # 记录训练损失的空列表\n","    loss_his = []\n","    \n","    # 初始化模型参数\n","    w, b = initialize_params(X.shape[1])\n","    \n","    # 迭代训练\n","    for i in range(1, epochs):\n","        # 计算当前迭代的预测值、损失和梯度\n","        y_hat, loss, dw, db = linear_loss(X, y, w, b)\n","        \n","        # 基于梯度下降的参数更新\n","        w += -learning_rate * dw\n","        b += -learning_rate * db\n","        \n","        # 记录当前迭代的损失\n","        loss_his.append(loss)\n","        # 每1000次迭代打印当前损失信息\n","        if i % 10000 == 0:\n","            print('epoch %d loss %f' % (i, loss))\n","            \n","        # 将当前迭代步优化后的参数保存到字典\n","        params = {\n","            'w': w,\n","            'b': b\n","        }\n","        # 将当前迭代步的梯度保存到字典\n","        grads = {\n","            'dw': dw,\n","            'db': db\n","        }\n","    return loss_his, params, grads"]},{"cell_type":"code","id":"3c5bab39-ec73-4ddc-b586-219c454b83b6","source":["from sklearn.datasets import load_diabetes\n","diabetes = load_diabetes()\n","data = diabetes.data\n","target = diabetes.target \n","print(\"data size:   \", data.shape)\n","print(\"target size: \", target.shape)\n","print(data[:5])\n","print(target[:5])"]},{"cell_type":"code","id":"1eaa1dc5-72ef-4a1f-95d9-53280bee5c35","source":["# 导入sklearn diabetes数据接口\n","from sklearn.datasets import load_diabetes\n","# 导入sklearn打乱数据函数\n","from sklearn.utils import shuffle\n","\n","# 获取diabetes数据集\n","diabetes = load_diabetes()\n","# 获取输入和标签\n","data, target = diabetes.data, diabetes.target \n","# 打乱数据集\n","\n","X, y = shuffle(data, target, random_state=13)\n","# 按照8/2划分训练集和测试集\n","offset = int(X.shape[0] * 0.8)\n","# 训练集\n","X_train, y_train = X[:offset], y[:offset]\n","# 测试集\n","X_test, y_test = X[offset:], y[offset:]\n","# 将训练集改为列向量的形式\n","y_train = y_train.reshape((-1,1))\n","# 将验证集改为列向量的形式\n","y_test = y_test.reshape((-1,1))\n","# 打印训练集和测试集维度\n","print(\"X_train's shape: \", X_train.shape)\n","print(\"X_test's shape: \", X_test.shape)\n","print(\"y_train's shape: \", y_train.shape)\n","print(\"y_test's shape: \", y_test.shape)"]},{"cell_type":"markdown","source":["learning rate 的选取不能太大也不能太小，太大会导致来回震荡，无法下降，太小会导致迭代批次太多，拟合速度很慢\n","如果 loss function 没有在减小而是在增大，这是 learning rate 太大的典型表现\n","尝试 learning rate 可以指数增长，寻找最合适的参数"],"id":"890323ab0729a84b"},{"cell_type":"code","source":["# 线性回归模型训练\n","loss_his, params, grads = linear_train(X_train, y_train, 0.01, 200000)\n","# 打印训练后得到模型参数\n","print(params)"],"id":"909fed6f-fb2d-410f-bdea-002fca9e6c9a"},{"cell_type":"code","id":"bb3d480c-326b-4505-8071-dbcf7de8de9e","source":["### 定义线性回归预测函数\n","def predict(X, params):\n","    \"\"\"\n","    输入：\n","    X：测试数据集\n","    params：模型训练参数\n","    输出：\n","    y_pred：模型预测结果\n","    \"\"\"\n","    # 获取模型参数\n","    w = params['w']\n","    b = params['b']\n","    # 预测\n","    y_pred = np.dot(X, w) + b\n","    return y_pred\n","# 基于测试集的预测\n","y_pred = predict(X_test, params)\n","# 打印前五个预测值\n","y_pred[:5]"]},{"cell_type":"code","id":"19d1a75d-50c7-4e94-8f20-76761ddbad9d","source":["print(y_test[:5])"]},{"cell_type":"code","id":"cfe82c03-d7aa-427c-b5d5-512e3d904258","source":["### 定义R2系数函数\n","def r2_score(y_test, y_pred):\n","    \"\"\"\n","    输入：\n","    y_test：测试集标签值\n","    y_pred：测试集预测值\n","    输出：\n","    r2：R2系数\n","    \"\"\"\n","    # 测试标签均值\n","    y_avg = np.mean(y_test)\n","    # 总离差平方和\n","    ss_tot = np.sum((y_test - y_avg)**2)\n","    # 残差平方和\n","    ss_res = np.sum((y_test - y_pred)**2)\n","    # R2计算\n","    r2 = 1 - (ss_res/ss_tot)\n","    return r2"]},{"cell_type":"code","id":"798552af-c945-46ab-ad31-9ebb5dd2bac5","source":["print(r2_score(y_test, y_pred))"]},{"cell_type":"code","id":"417554aa-e8e5-48e8-b65f-306dcb7f20f6","source":["import matplotlib.pyplot as plt\n","f = X_test.dot(params['w']) + params['b']\n","\n","plt.scatter(range(X_test.shape[0]), y_test)\n","plt.plot(f, color = 'darkorange')\n","plt.xlabel('X_test')\n","plt.ylabel('y_test')\n","plt.show();"]},{"cell_type":"code","id":"a0e5cdb8-4e17-4f3d-a222-7df3c4ed5021","source":["plt.plot(loss_his, color='blue')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.show()"]},{"cell_type":"code","id":"23d8658d-2eef-4584-91b0-7101bed5d20d","source":["### sklearn版本为1.0.2\n","# 导入线性回归模块\n","from sklearn import linear_model\n","from sklearn.metrics import mean_squared_error, r2_score\n","# 创建模型实例\n","regr = linear_model.LinearRegression()\n","# 模型拟合\n","regr.fit(X_train, y_train)\n","# 模型预测\n","y_pred = regr.predict(X_test)\n","# 打印模型均方误差\n","print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n","# 打印R2\n","print('R2 score: %.2f' % r2_score(y_test, y_pred))"]},{"cell_type":"code","id":"5e43b619-a0cc-4bf0-8d20-36b626e1c4e9","source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
