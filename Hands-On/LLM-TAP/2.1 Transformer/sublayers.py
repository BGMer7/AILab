import torch.nn as nn
import torch
import math
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout=0.1):
        super().__init__()
        
        self.h = heads
        self.d_k = d_model // heads
        self.d_model = d_model
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)
        
    def attention(self, q, k, v, d_k, mask=None, dropout=None):
        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)
        
        if mask is not None:
            mask = mask.unsqueeze(1)
            scores = scores.masked_fill(mask == 0, -1e9)
            
        scores = F.softmax(scores, dim=-1)
        
        if dropout is not None:
            scores = dropout(scores)

        output = torch.matmul(scores, v)
        return output
    
    def forward(self, q, k, v, mask=None):
        bs = q.size(0)

        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)

        # çŸ©é˜µè½¬ç½®
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)
        
        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)
        
        # æ‹¼æ¥å¤šå¤´æ³¨æ„åŠ›è¾“å‡º
        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)
        
        output = self.out(concat)

        return output
    
    
def test_multihead_attention():
    """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æ¨¡å—"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æ¨¡å—")
    print("="*50)
    
    # è®¾ç½®å‚æ•°
    batch_size = 2
    seq_length = 5
    d_model = 512
    heads = 8
    
    print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
    print(f"   - batch_size: {batch_size}")
    print(f"   - seq_length: {seq_length}")
    print(f"   - d_model: {d_model}")
    print(f"   - heads: {heads}")
    print(f"   - d_k (æ¯ä¸ªå¤´çš„ç»´åº¦): {d_model // heads}")
    
    # åˆ›å»ºæ¨¡å‹
    mha = MultiHeadAttention(heads=heads, d_model=d_model)
    print(f"\nâœ… åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—æˆåŠŸ")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    # æ¨¡æ‹Ÿå¥å­: "æˆ‘ çˆ± å­¦ä¹  æœºå™¨ å­¦ä¹ "
    input_data = torch.randn(batch_size, seq_length, d_model)
    print(f"\nğŸ“¥ è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        # è‡ªæ³¨æ„åŠ›ï¼šQ=K=V
        output = mha(input_data, input_data, input_data)
        print(f"ğŸ“¤ è¾“å‡ºæ•°æ®å½¢çŠ¶: {output.shape}")
        
        # éªŒè¯å½¢çŠ¶æ˜¯å¦æ­£ç¡®
        assert output.shape == input_data.shape, "è¾“å‡ºå½¢çŠ¶åº”è¯¥ä¸è¾“å…¥ç›¸åŒ"
        print("âœ… å½¢çŠ¶éªŒè¯é€šè¿‡")
    
    # æ‰“å°å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in mha.parameters())
    print(f"\nğŸ“ˆ æ¨¡å‹å‚æ•°:")
    print(f"   - æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   - Qçº¿æ€§å±‚: {d_model * d_model:,}")
    print(f"   - Kçº¿æ€§å±‚: {d_model * d_model:,}")
    print(f"   - Vçº¿æ€§å±‚: {d_model * d_model:,}")
    print(f"   - è¾“å‡ºå±‚: {d_model * d_model:,}")
    
    return mha, input_data, output

def visualize_attention_effect():
    """å¯è§†åŒ–æ³¨æ„åŠ›æ•ˆæœ"""
    print("\nğŸ¯ æµ‹è¯•æ³¨æ„åŠ›æ•ˆæœ")
    print("="*50)
    
    # ä½¿ç”¨å°ä¸€ç‚¹çš„å‚æ•°ä¾¿äºè§‚å¯Ÿ
    batch_size = 1
    seq_length = 4
    d_model = 64
    heads = 4
    
    mha = MultiHeadAttention(heads=heads, d_model=d_model)
    
    # åˆ›å»ºæœ‰æ„ä¹‰çš„æµ‹è¯•æ•°æ®
    # æ¨¡æ‹Ÿ4ä¸ªè¯çš„å¥å­ï¼Œæ¯ä¸ªè¯æœ‰ä¸åŒçš„ç‰¹å¾æ¨¡å¼
    test_input = torch.zeros(batch_size, seq_length, d_model)
    
    # ç¬¬1ä¸ªè¯ï¼šå‰16ç»´ä¸º1
    test_input[0, 0, :16] = 1.0
    # ç¬¬2ä¸ªè¯ï¼šä¸­é—´16ç»´ä¸º1  
    test_input[0, 1, 16:32] = 1.0
    # ç¬¬3ä¸ªè¯ï¼šå16ç»´ä¸º1
    test_input[0, 2, 32:48] = 1.0
    # ç¬¬4ä¸ªè¯ï¼šæ··åˆç‰¹å¾
    test_input[0, 3, [0, 16, 32, 48]] = 1.0
    
    print("ğŸ“Š è¾“å…¥ç‰¹å¾æ¨¡å¼:")
    for i in range(seq_length):
        non_zero = torch.nonzero(test_input[0, i]).flatten()
        print(f"   è¯{i+1}: éé›¶ä½ç½® {non_zero[:5].tolist()}{'...' if len(non_zero) > 5 else ''}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = mha(test_input, test_input, test_input)
        
        print(f"\nğŸ“¤ è¾“å‡ºç»Ÿè®¡:")
        print(f"   - è¾“å…¥èŒƒå›´: [{test_input.min():.3f}, {test_input.max():.3f}]")
        print(f"   - è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
        print(f"   - è¾“å‡ºå‡å€¼: {output.mean():.3f}")
        print(f"   - è¾“å‡ºæ ‡å‡†å·®: {output.std():.3f}")
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„è¾“å‡ºå˜åŒ–
        print(f"\nğŸ”„ æ³¨æ„åŠ›æ•ˆæœåˆ†æ:")
        for i in range(seq_length):
            input_norm = torch.norm(test_input[0, i])
            output_norm = torch.norm(output[0, i])
            print(f"   è¯{i+1}: è¾“å…¥èŒƒæ•°={input_norm:.3f} -> è¾“å‡ºèŒƒæ•°={output_norm:.3f}")

def test_with_mask():
    """æµ‹è¯•æ©ç åŠŸèƒ½"""
    print("\nğŸ­ æµ‹è¯•æ©ç åŠŸèƒ½")
    print("="*50)
    
    batch_size = 1
    seq_length = 5
    d_model = 1024
    heads = 4
    
    mha = MultiHeadAttention(heads=heads, d_model=d_model)
    input_data = torch.randn(batch_size, seq_length, d_model)
    
    # åˆ›å»ºæ©ç ï¼šå‡è®¾æœ€åä¸¤ä¸ªä½ç½®æ˜¯padding
    mask = torch.ones(batch_size, seq_length)
    mask[0, 3:] = 0  # æ©ç›–æœ€åä¸¤ä¸ªä½ç½®
    
    print(f"ğŸ“‹ æ©ç æ¨¡å¼: {mask[0].tolist()}")
    print("   (1è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼Œ0è¡¨ç¤ºpaddingä½ç½®)")
    
    with torch.no_grad():
        # ä¸ä½¿ç”¨æ©ç 
        output_no_mask = mha(input_data, input_data, input_data)
        
        # ä½¿ç”¨æ©ç 
        output_with_mask = mha(input_data, input_data, input_data, mask=mask)
        
        print(f"\nğŸ“Š æ©ç æ•ˆæœå¯¹æ¯”:")
        for i in range(seq_length):
            no_mask_norm = torch.norm(output_no_mask[0, i])
            with_mask_norm = torch.norm(output_with_mask[0, i])
            mask_status = "æœ‰æ•ˆ" if mask[0, i] == 1 else "æ©ç›–"
            print(f"   ä½ç½®{i+1}({mask_status}): æ— æ©ç ={no_mask_norm:.3f}, æœ‰æ©ç ={with_mask_norm:.3f}")

def compare_different_heads():
    """æ¯”è¾ƒä¸åŒå¤´æ•°çš„æ•ˆæœ"""
    print("\nğŸ”¢ æ¯”è¾ƒä¸åŒå¤´æ•°çš„æ•ˆæœ")
    print("="*50)
    
    d_model = 256
    seq_length = 6
    batch_size = 1
    
    head_configs = [1, 2, 4, 8]
    input_data = torch.randn(batch_size, seq_length, d_model)
    
    results = {}
    for heads in head_configs:
        mha = MultiHeadAttention(heads=heads, d_model=d_model)
        with torch.no_grad():
            output = mha(input_data, input_data, input_data)
            results[heads] = {
                'output_std': output.std().item(),
                'output_mean': output.mean().item(),
                'params': sum(p.numel() for p in mha.parameters())
            }
    
    print(f"ğŸ“Š ä¸åŒå¤´æ•°å¯¹æ¯”:")
    print(f"{'å¤´æ•°':<6} {'è¾“å‡ºæ ‡å‡†å·®':<12} {'è¾“å‡ºå‡å€¼':<12} {'å‚æ•°é‡':<12}")
    print("-" * 50)
    for heads in head_configs:
        r = results[heads]
        print(f"{heads:<6} {r['output_std']:<12.4f} {r['output_mean']:<12.4f} {r['params']:<12,}")

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    print("ğŸ‰ å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ç»¼åˆæµ‹è¯•")
    print("="*70)
    
    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    mha, input_data, output = test_multihead_attention()
    
    # æ³¨æ„åŠ›æ•ˆæœå¯è§†åŒ–
    visualize_attention_effect()
    
    # æ©ç åŠŸèƒ½æµ‹è¯•
    test_with_mask()
    
    # ä¸åŒå¤´æ•°å¯¹æ¯”
    compare_different_heads()
    
    print("\nğŸŠ æµ‹è¯•å®Œæˆï¼å¤šå¤´æ³¨æ„åŠ›æ¨¡å—è¿è¡Œæ­£å¸¸")
    print("="*70)

 