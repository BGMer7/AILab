{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer: \n",
    "\n",
    "https://www.kaggle.com/code/kmldas/cifar10-resnet-90-accuracy-less-than-5-min\n",
    "\n",
    "https://www.kaggle.com/code/ayushnitb/cifar10-custom-resnet-cnn-pytorch-97-acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Reorganization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampleSubmission.csv', 'test', 'train', 'trainLabels.csv', 'valid']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# manully download the dataset\n",
    "data_dir = '../../dataset/CIFAR-10'\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "data_train_dir = data_dir + \"/train\"\n",
    "data_test_dir = data_dir + \"/test\"\n",
    "data_valid_dir = data_dir + \"/valid\"\n",
    "\n",
    "# 创建 valid 文件夹，如果不存在\n",
    "os.makedirs(data_valid_dir, exist_ok=True)\n",
    "\n",
    "classes = os.listdir(data_train_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 读取trainLabels.csv文件\\nlabels_csv_path = os.path.join(data_dir, \\'trainLabels.csv\\')\\nlabels_df = pd.read_csv(labels_csv_path)\\n\\n# 确保标签文件夹存在\\nfor label in labels_df[\\'label\\'].unique():\\n    label_folder = os.path.join(data_train_dir, label)\\n    if not os.path.exists(label_folder):\\n        os.makedirs(label_folder)\\n\\n# 移动图片到对应的标签文件夹中\\nfor idx, row in labels_df.iterrows():\\n    img_id = row[\\'id\\']\\n    label = row[\\'label\\']\\n    \\n    # 源图片路径，假设图片后缀是 .png\\n    src_img_path = os.path.join(data_train_dir, f\\'{img_id}.png\\')\\n    \\n    # 目标图片路径\\n    dst_img_path = os.path.join(data_train_dir, label, f\\'{img_id}.png\\')\\n    \\n    # 移动图片到对应的标签文件夹\\n    if os.path.exists(src_img_path):\\n        shutil.move(src_img_path, dst_img_path)\\n\\nprint(\"图片移动完成！\")\\n\\n# 遍历 train 目录下的每个类别文件夹\\nfor class_name in os.listdir(data_train_dir):\\n    class_train_dir = os.path.join(data_train_dir, class_name)\\n    \\n    # 确保是目录\\n    if os.path.isdir(class_train_dir):\\n        # 创建对应的 valid 类别文件夹\\n        class_valid_dir = os.path.join(data_valid_dir, class_name)\\n        os.makedirs(class_valid_dir, exist_ok=True)\\n        \\n        # 获取当前类别文件夹中的所有图片文件\\n        images = [img for img in os.listdir(class_train_dir) if img.endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\'))]\\n        \\n        # 随机选择 20% 的图片进行移动\\n        num_images = len(images)\\n        num_valid = int(0.2 * num_images)\\n        valid_images = random.sample(images, num_valid)\\n        \\n        # 将选定的图片移动到 valid 类别文件夹\\n        for img in valid_images:\\n            src_img_path = os.path.join(class_train_dir, img)\\n            dst_img_path = os.path.join(class_valid_dir, img)\\n            shutil.move(src_img_path, dst_img_path)\\n        \\n        print(f\"Moved {num_valid} images from {class_name} to valid/{class_name}\")\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 读取trainLabels.csv文件\n",
    "labels_csv_path = os.path.join(data_dir, 'trainLabels.csv')\n",
    "labels_df = pd.read_csv(labels_csv_path)\n",
    "\n",
    "# 确保标签文件夹存在\n",
    "for label in labels_df['label'].unique():\n",
    "    label_folder = os.path.join(data_train_dir, label)\n",
    "    if not os.path.exists(label_folder):\n",
    "        os.makedirs(label_folder)\n",
    "\n",
    "# 移动图片到对应的标签文件夹中\n",
    "for idx, row in labels_df.iterrows():\n",
    "    img_id = row['id']\n",
    "    label = row['label']\n",
    "    \n",
    "    # 源图片路径，假设图片后缀是 .png\n",
    "    src_img_path = os.path.join(data_train_dir, f'{img_id}.png')\n",
    "    \n",
    "    # 目标图片路径\n",
    "    dst_img_path = os.path.join(data_train_dir, label, f'{img_id}.png')\n",
    "    \n",
    "    # 移动图片到对应的标签文件夹\n",
    "    if os.path.exists(src_img_path):\n",
    "        shutil.move(src_img_path, dst_img_path)\n",
    "\n",
    "print(\"图片移动完成！\")\n",
    "\n",
    "# 遍历 train 目录下的每个类别文件夹\n",
    "for class_name in os.listdir(data_train_dir):\n",
    "    class_train_dir = os.path.join(data_train_dir, class_name)\n",
    "    \n",
    "    # 确保是目录\n",
    "    if os.path.isdir(class_train_dir):\n",
    "        # 创建对应的 valid 类别文件夹\n",
    "        class_valid_dir = os.path.join(data_valid_dir, class_name)\n",
    "        os.makedirs(class_valid_dir, exist_ok=True)\n",
    "        \n",
    "        # 获取当前类别文件夹中的所有图片文件\n",
    "        images = [img for img in os.listdir(class_train_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # 随机选择 20% 的图片进行移动\n",
    "        num_images = len(images)\n",
    "        num_valid = int(0.2 * num_images)\n",
    "        valid_images = random.sample(images, num_valid)\n",
    "        \n",
    "        # 将选定的图片移动到 valid 类别文件夹\n",
    "        for img in valid_images:\n",
    "            src_img_path = os.path.join(class_train_dir, img)\n",
    "            dst_img_path = os.path.join(class_valid_dir, img)\n",
    "            shutil.move(src_img_path, dst_img_path)\n",
    "        \n",
    "        print(f\"Moved {num_valid} images from {class_name} to valid/{class_name}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use test set for validation\n",
    "我们将简单地使用测试集作为验证集，而不是从训练集中留出一小部分（例如10%）的数据进行验证。这只是为训练提供了更多的数据。一般来说，一旦你使用固定的验证集选择了最好的模型架构和超参数，在整个数据集上重新训练相同的模型是一个好主意，只是为了给它一个小小的性能提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel-wise data normalization\n",
    "通道数据归一化：我们将通过减去平均值并除以每个通道的标准偏差来归一化图像张量。因此，每个通道的数据均值为0，标准差为1。规范化数据可以防止来自任何一个通道的值在训练时不成比例地影响损失和梯度，只需具有比其他通道更高或更宽的值范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized data augmentations\n",
    "随机数据增强：我们将在从训练数据集加载图像时应用随机选择的转换。具体来说，我们将每张图像填充4个像素，然后随机裁剪大小为32 x 32像素，然后以50%的概率水平翻转图像。由于每次加载特定图像时都会随机动态地应用变换，因此模型在每个训练历元中看到的图像略有不同，这使得它可以更好地进行泛化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CIFAR-10数据集中，数据预处理通常包括归一化和数据增强（Data Augmentation）两部分。你提供的代码中，通过`torchvision.transforms`（简称`tt`）实现了这些操作。下面是对代码的详细解释：\n",
    "\n",
    "### 1. **归一化（Normalization）**\n",
    "   - CIFAR-10 的图像是 32x32 大小的彩色图像，像素值范围在 `[0, 1]` 或 `[0, 255]` 之间。\n",
    "   - 归一化是将图像的像素值通过减去均值并除以标准差，使其符合标准正态分布。\n",
    "   \n",
    "   ```python\n",
    "   stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "   ```\n",
    "   - 这里的 `stats` 变量包含了每个通道（RGB）的均值和标准差：\n",
    "     - **均值**： `(0.4914, 0.4822, 0.4465)` 对应于 RGB 通道的均值。\n",
    "     - **标准差**： `(0.2023, 0.1994, 0.2010)` 对应于 RGB 通道的标准差。\n",
    "   - 这些值是通过对整个 CIFAR-10 数据集统计得到的，用于将图像像素值归一化。\n",
    "\n",
    "### 2. **数据增强（Data Augmentation）**\n",
    "   数据增强用于提高模型的泛化能力，通过随机变换图像来生成不同的训练样本。这对模型训练有帮助，尤其是在数据集较小时。\n",
    "\n",
    "   ```python\n",
    "   train_tfms = tt.Compose([\n",
    "       tt.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
    "       tt.RandomHorizontalFlip(), \n",
    "       tt.ToTensor(), \n",
    "       tt.Normalize(*stats, inplace=True)\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "   - **`tt.RandomCrop(32, padding=4, padding_mode='reflect')`**：随机裁剪图像为 32x32 的大小，并在每个方向上增加 4 像素的填充。`padding_mode='reflect'` 表示使用反射填充边缘区域。\n",
    "   - **`tt.RandomHorizontalFlip()`**：随机水平翻转图像（以 50% 的概率）。这种水平翻转在很多场景中可以保留图像的主要特征，特别适用于物体不需要有固定方向的任务。\n",
    "   - **`tt.ToTensor()`**：将图像从 PIL 格式（或 numpy 数组）转换为 PyTorch 张量，并将像素值归一化到 `[0, 1]`。\n",
    "   - **`tt.Normalize(*stats, inplace=True)`**：使用之前定义的均值和标准差对图像进行归一化，使每个像素的值服从标准正态分布。这一步是在转换为张量之后进行的。\n",
    "\n",
    "### 3. **验证集预处理**\n",
    "   验证集的预处理一般比训练集简单，因为不需要数据增强。验证集只需进行归一化即可：\n",
    "\n",
    "   ```python\n",
    "   valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
    "   ```\n",
    "   - 这里没有使用数据增强，只是将图像转换为张量并归一化。\n",
    "\n",
    "### 总结\n",
    "- **训练集**：使用了数据增强（随机裁剪和水平翻转）和归一化。\n",
    "- **验证集**：只进行了归一化处理。\n",
    "   \n",
    "这些预处理步骤帮助模型更好地学习 CIFAR-10 数据集的特征，并且通过数据增强，增加了数据的多样性，减少过拟合的可能性。\n",
    "\n",
    "见experiments/Transforms.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform (normalization and data augmentation)\n",
    "# CIFAR 数据集的均值\n",
    "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transforms_train = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode=\"reflect\"),\n",
    "                               tt.RandomHorizontalFlip(),\n",
    "                               tt.ToTensor(),\n",
    "                               tt.Normalize(*stats, inplace=True)])\n",
    "\n",
    "transforms_valid = tt.Compose([tt.Normalize(*stats),\n",
    "                             tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch datasets\n",
    "\n",
    "train_ds = ImageFolder(data_train_dir, transforms_train)\n",
    "valid_ds = ImageFolder(data_valid_dir, transforms_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method to show images\n",
    "def show_batch(dl, n=1):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
